# -*- coding: utf-8 -*-
"""basic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xh6sP2-tbZuSieVuvsP5RuiX9ixuMyz_

# Prereq
"""

# !pip install tensortrade ray[tune,rllib] symfit

import tensorflow as tf
from tensorflow.python.client import device_lib
def get_available_gpus():
    local_device_protos = device_lib.list_local_devices()
    return [x.name for x in local_device_protos if x.device_type == 'GPU']

num_gpus = len(get_available_gpus())
print(num_gpus)

"""# Setup

### Imports
"""

from gym.spaces import Discrete

from tensortrade.env.default.actions import TensorTradeActionScheme

from tensortrade.env.generic import ActionScheme, TradingEnv
from tensortrade.core import Clock
from tensortrade.oms.instruments import ExchangePair
from tensortrade.oms.wallets import Portfolio
from tensortrade.oms.instruments import Instrument
from tensortrade.oms.orders import (
    Order,
    proportion_order,
    TradeSide,
    TradeType
)
from tensortrade.data.cdd import CryptoDataDownload
from tensortrade.feed.core import Stream, DataFeed
from tensortrade.oms.exchanges import Exchange
from tensortrade.oms.services.execution.simulated import execute_order
from tensortrade.oms.instruments import USD, BTC, ETH
from tensortrade.oms.wallets import Wallet, Portfolio
from tensortrade.agents import DQNAgent

import pandas as pd

import tensortrade.env.default as default
import ray
import numpy as np
import pandas as pd
from ray import tune
from ray.tune.registry import register_env

"""### Util"""

from plotly.offline import init_notebook_mode
init_notebook_mode(connected = True)

def get_available_gpus():
    local_device_protos = device_lib.list_local_devices()
    return [x.name for x in local_device_protos if x.device_type == 'GPU']
def get_available_cpus():
    local_device_protos = device_lib.list_local_devices()
    return [x.name for x in local_device_protos if x.device_type == 'CPU']
num_gpus = len(get_available_gpus())
num_cpus = len(get_available_cpus())

"""### Data"""

# Fetch Data
cdd = CryptoDataDownload()
data = cdd.fetch("Bitfinex", "USD", "BTC", "1h")
def CPLogRSIMacdFeed():
    
    def rsi(price: Stream[float], period: float) -> Stream[float]:
        r = price.diff()
        upside = r.clamp_min(0).abs()
        downside = r.clamp_max(0).abs()
        rs = upside.ewm(alpha=1 / period).mean() / downside.ewm(alpha=1 / period).mean()
        return 100*(1 - (1 + rs) ** -1)


    def macd(price: Stream[float], fast: float, slow: float, signal: float) -> Stream[float]:
        fm = price.ewm(span=fast, adjust=False).mean()
        sm = price.ewm(span=slow, adjust=False).mean()
        md = fm - sm
        signal = md - md.ewm(span=signal, adjust=False).mean()
        return signal


    features = []
    for c in data.columns[1:]:
        s = Stream.source(list(data[c]), dtype="float").rename(data[c].name)
        features += [s]

    cp = Stream.select(features, lambda s: s.name == "close")

    features = [
        # Remove auto correlation
        cp.log().diff().rename("lr"),
        rsi(cp, period=20).rename("rsi"),
        macd(cp, fast=10, slow=50, signal=5).rename("macd")
    ]

    feed = DataFeed(features)
    feed.compile()


    return feed

"""### Portfolio"""

from tensortrade.oms.instruments import USD, BTC, ETH
portfolios = []
def getPortfolio(exchange, usd=50000, btc=1):
    portfolio = Portfolio(USD, [
        Wallet(exchange, usd * USD),
        Wallet(exchange, btc * BTC)
    ])

    portfolios.append(portfolio)

    return portfolio

"""# Rendering"""

# Position Change Chart
import matplotlib.pyplot as plt

from tensortrade.env.generic import Renderer


class PositionChangeChart(Renderer):
    def __init__(self, color: str = "orange"):
        self.color = "orange"

    def render(self, env, **kwargs):
        history = pd.DataFrame(env.observer.renderer_history)

        actions = list(history.action)
        p = list(history.price)

        buy = {}
        sell = {}

        for i in range(len(actions) - 1):
            a1 = actions[i]
            a2 = actions[i + 1]

            if a1 != a2:
                if a1 == 0 and a2 == 1:
                    buy[i] = p[i]
                else:
                    sell[i] = p[i]

        buy = pd.Series(buy)
        sell = pd.Series(sell)

        fig, axs = plt.subplots(1, 2, figsize=(15, 5))

        fig.suptitle("Performance")

        axs[0].plot(np.arange(len(p)), p, label="price", color=self.color)
        axs[0].scatter(buy.index, buy.values, marker="^", color="green")
        axs[0].scatter(sell.index, sell.values, marker="^", color="red")
        axs[0].set_title("Trading Chart")

        d = env.action_scheme.portfolio.performance

        # lists = sorted(d.items()) # sorted by key, return+ a list of tuples
        keys = d[0].keys()
        x = range(len(d))
        c = {}

        for i in range(len(d)):
            for key in keys:
                if key != 'net_worth' and key != 'base_symbol':
                    if not key in c:
                        c[key] = []
                    c[key].append(d[i][key])

        for key in keys:
            if key != 'net_worth' and key != 'base_symbol':
                axs[1].plot(x, c[key], label=key)
        
        axs[1].set_title("Net Worth")

        plt.show()

"""# Define Model

### Reward Schemes
"""

# Reward Schemes
from tensortrade.env.default.rewards import TensorTradeRewardScheme
from tensortrade.env.generic import RewardScheme
# from tensortrade.rewards import RewardStrategy
# from tensortrade.trades import Trade

class PBR(TensorTradeRewardScheme):
    """A reward scheme for position-based returns.

    * Let :math:`p_t` denote the price at time t.
    * Let :math:`x_t` denote the position at time t.
    * Let :math:`R_t` denote the reward at time t.

    Then the reward is defined as,
    :math:`R_{t} = (p_{t} - p_{t-1}) \cdot x_{t}`.

    Parameters
    ----------
    price : `Stream`
        The price stream to use for computing rewards.
    """

    registered_name = "pbr"

    def __init__(self, price: 'Stream') -> None:
        super().__init__()
        self.position = -1

        r = Stream.sensor(price, lambda p: p.value, dtype="float").diff()
        position = Stream.sensor(self, lambda rs: rs.position, dtype="float")

        reward = (position * r).fillna(0).rename("reward")

        self.feed = DataFeed([reward])
        self.feed.compile()

    def on_action(self, action: int) -> None:
        self.position = -1 if action == 0 else 1

    def get_reward(self, portfolio: 'Portfolio') -> float:
        return self.feed.next()["reward"]

    def reset(self) -> None:
        """Resets the `position` and `feed` of the reward scheme."""
        self.position = -1
        self.feed.reset()


# class DirectProfitStrategy(RewardScheme):
#     '''This reward = how much money that the strategy earns'''
#     def reset(self):
#         """Necessary to reset the open amount and the last price"""
#         self._open_amount= 0
#         self._last_price = 0

#     def get_reward(self, current_step: int, trade: Trade) -> float:
#         last_price = self._last_price
#         price = trade.price
#         last_amount = self._open_amount

#         #reset values
#         if trade.is_hold:
#             pass
#         elif trade.is_buy:
#             self._open_amount += trade.amount
#         elif trade.is_sell:
#             self._open_amount -= trade.amount

#         last_price = self._last_price

#         self._last_price = trade.price        
        
#         if trade.is_hold:
#             return last_amount * (price - last_price)
#         else:
#             return last_amount * (price - last_price) - trade.amount * trade.price * 0.0003

"""### Action Scheme"""

# BuySellHold
class BuySellHold(TensorTradeActionScheme):

    registered_name = "bsh"

    def __init__(self, cash: 'Wallet', asset: 'Wallet'):
        super().__init__()
        self.cash = cash
        self.asset = asset

        self.listeners = []
        self.action = 0

    @property
    def action_space(self):
        return Discrete(2)

    def attach(self, listener):
        self.listeners += [listener]
        return self

    def get_orders(self, action: int, portfolio: 'Portfolio'):
        order = None

        if abs(action - self.action) > 0:
            src = self.cash if self.action == 0 else self.asset
            tgt = self.asset if self.action == 0 else self.cash
            order = proportion_order(portfolio, src, tgt, 1.0)
            self.action = action

        for listener in self.listeners:
            listener.on_action(action)

        return [order]

    def reset(self):
        super().reset()
        self.action = 0

"""# Trading Environment"""

def create_env(config):
    p = Stream.source(list(data["close"]), dtype="float").rename("USD-BTC")

    # Define exchange
    bitfinex = Exchange("bitfinex", service=execute_order)(
        p
    )

    portfolio = getPortfolio(bitfinex, 50000, 1)

    feed = CPLogRSIMacdFeed()

    for i in range(5):
        print(feed.next())

    reward_scheme = PBR(price=p)

    action_scheme = BuySellHold(
        cash=portfolio.wallets[0],
        asset=portfolio.wallets[1]
    ).attach(reward_scheme)

    renderer_feed = DataFeed([
        Stream.source(list(data["date"])).rename("date"),
        Stream.source(list(data["open"]), dtype="float").rename("open"),
        Stream.source(list(data["high"]), dtype="float").rename("high"),
        Stream.source(list(data["low"]), dtype="float").rename("low"),
        Stream.source(list(data["close"]), dtype="float").rename("close"),
        Stream.source(list(data["volume"]), dtype="float").rename("volume")
    ])

    environment = default.create(
        feed=feed,
        portfolio=portfolio,
        action_scheme=action_scheme,
        reward_scheme=reward_scheme,
        renderer_feed=renderer_feed,
        # renderer=default.renderers.PlotlyTradingChart(),
        renderer= PositionChangeChart(),
        window_size=config["window_size"],
        max_allowed_loss=0.6
    )
    return environment

register_env("TradingEnv", create_env)

"""# Training"""

analysis = tune.run(
    "PPO",
    stop={
      "episode_reward_mean": 500
    },
    config={
        "env": "TradingEnv",
        "env_config": {
            "window_size": 25
        },
        "log_level": "DEBUG",
        "framework": "torch",
        "ignore_worker_failures": True,
        "num_workers": 1,
        "num_gpus": 0,
        "clip_rewards": True,
        "lr": 8e-6,
        "lr_schedule": [
            [0, 1e-1],
            [int(1e2), 1e-2],
            [int(1e3), 1e-3],
            [int(1e4), 1e-4],
            [int(1e5), 1e-5],
            [int(1e6), 1e-6],
            [int(1e7), 1e-7]
        ],
        "gamma": 0,
        "observation_filter": "MeanStdFilter",
        "lambda": 0.72,
        "vf_loss_coeff": 0.5,
        "entropy_coeff": 0.01
    },
    checkpoint_at_end=True,
    mode="max"
)

# !cat /root/ray_results/PPO/PPO_TradingEnv_b2427_00000_0_2021-04-25_05-54-28/error.txt

"""# Evaluation"""

def create_eval_env(config):
    p = Stream.source(list(data["close"]), dtype="float").rename("USD-BTC")

    # Define exchange
    bitfinex = Exchange("bitfinex", service=execute_order)(
        p
    )
    portfolio = getPortfolio(bitfinex, 50000, 1)


    feed = CPLogRSIMacdFeed()

    for i in range(5):
        print(feed.next())

    reward_scheme = PBR(price=p)

    action_scheme = BuySellHold(
        cash=portfolio.wallets[0],
        asset=portfolio.wallets[1]
    ).attach(reward_scheme)

    renderer_feed = DataFeed([
        Stream.source(list(data["date"])).rename("date"),
        Stream.source(list(data["open"]), dtype="float").rename("open"),
        Stream.source(list(data["high"]), dtype="float").rename("high"),
        Stream.source(list(data["low"]), dtype="float").rename("low"),
        Stream.source(list(data["close"]), dtype="float").rename("close"),
        Stream.source(list(data["volume"]), dtype="float").rename("volume")
    ])

    environment = default.create(
        feed=feed,
        portfolio=portfolio,
        action_scheme=action_scheme,
        reward_scheme=reward_scheme,
        renderer_feed=renderer_feed,
        # renderer=default.renderers.PlotlyTradingChart(),
        renderer = PositionChangeChart(),
        window_size=config["window_size"],
        max_allowed_loss=0.6
    )
    return environment

import ray.rllib.agents.ppo as ppo

# Get checkpoint
checkpoints = analysis.get_trial_checkpoints_paths(
    trial=analysis.get_best_trial("episode_reward_mean"),
    metric="episode_reward_mean"
)
checkpoint_path = checkpoints[0][0]

# Restore agent
agent = ppo.PPOTrainer(
    env="TradingEnv",
    config={
        "env_config": {
            "window_size": 25
        },
        "framework": "torch",
        "log_level": "DEBUG",
        "ignore_worker_failures": True,
        "num_workers": 1,
        "num_gpus": num_gpus,
        "clip_rewards": True,
        "lr": 8e-6,
        "lr_schedule": [
            [0, 1e-1],
            [int(1e2), 1e-2],
            [int(1e3), 1e-3],
            [int(1e4), 1e-4],
            [int(1e5), 1e-5],
            [int(1e6), 1e-6],
            [int(1e7), 1e-7]
        ],
        "gamma": 0,
        "observation_filter": "MeanStdFilter",
        "lambda": 0.72,
        "vf_loss_coeff": 0.5,
        "entropy_coeff": 0.01
    }
)

agent.restore(checkpoint_path)

# Instantiate the environment
env = create_eval_env({
    "window_size": 25,
})


# Run until episode ends
episode_reward = 0
done = False
obs = env.reset()
i = 0
while not done:
    action = agent.compute_action(obs)
    print(action)
    obs, reward, done, info = env.step(action)
    episode_reward += reward

history = pd.DataFrame(env.observer.renderer_history)
print(history)