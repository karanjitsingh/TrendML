{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "basic.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "68qfUWV3eipZ"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNDlUx7mIF5v"
      },
      "source": [
        "# Prereq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXhxE_9hNmP3",
        "outputId": "399c96cb-b8c2-4dd9-9871-df083086544a"
      },
      "source": [
        "!pip install tensortrade ray[tune,rllib] symfit"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensortrade in /usr/local/lib/python3.7/dist-packages (1.0.1b0)\n",
            "Requirement already satisfied: ray[rllib,tune] in /usr/local/lib/python3.7/dist-packages (1.3.0)\n",
            "Requirement already satisfied: symfit in /usr/local/lib/python3.7/dist-packages (0.5.3)\n",
            "Requirement already satisfied: gym>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from tensortrade) (0.17.3)\n",
            "Requirement already satisfied: plotly>=4.5.0 in /usr/local/lib/python3.7/dist-packages (from tensortrade) (4.14.3)\n",
            "Requirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.7/dist-packages (from tensortrade) (1.1.5)\n",
            "Requirement already satisfied: ipython>=7.12.0 in /usr/local/lib/python3.7/dist-packages (from tensortrade) (7.22.0)\n",
            "Requirement already satisfied: pyyaml>=5.1.2 in /usr/local/lib/python3.7/dist-packages (from tensortrade) (5.4.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from tensortrade) (1.19.5)\n",
            "Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tensortrade) (2.4.1)\n",
            "Requirement already satisfied: stochastic>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensortrade) (0.6.0)\n",
            "Requirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.7/dist-packages (from tensortrade) (3.2.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray[rllib,tune]) (7.1.2)\n",
            "Requirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray[rllib,tune]) (1.32.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from ray[rllib,tune]) (0.4.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray[rllib,tune]) (3.0.12)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray[rllib,tune]) (1.0.2)\n",
            "Requirement already satisfied: aiohttp-cors in /usr/local/lib/python3.7/dist-packages (from ray[rllib,tune]) (0.7.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from ray[rllib,tune]) (3.7.4.post0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray[rllib,tune]) (2.6.0)\n",
            "Requirement already satisfied: py-spy>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ray[rllib,tune]) (0.3.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray[rllib,tune]) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray[rllib,tune]) (3.15.8)\n",
            "Requirement already satisfied: aioredis in /usr/local/lib/python3.7/dist-packages (from ray[rllib,tune]) (1.3.1)\n",
            "Requirement already satisfied: opencensus in /usr/local/lib/python3.7/dist-packages (from ray[rllib,tune]) (0.7.12)\n",
            "Requirement already satisfied: gpustat in /usr/local/lib/python3.7/dist-packages (from ray[rllib,tune]) (0.6.0)\n",
            "Requirement already satisfied: redis>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ray[rllib,tune]) (3.5.3)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from ray[rllib,tune]) (0.10.1)\n",
            "Requirement already satisfied: atari-py; extra == \"rllib\" in /usr/local/lib/python3.7/dist-packages (from ray[rllib,tune]) (0.2.6)\n",
            "Requirement already satisfied: tensorboardX; extra == \"rllib\" in /usr/local/lib/python3.7/dist-packages (from ray[rllib,tune]) (2.2)\n",
            "Requirement already satisfied: dm-tree; extra == \"rllib\" in /usr/local/lib/python3.7/dist-packages (from ray[rllib,tune]) (0.1.6)\n",
            "Requirement already satisfied: scipy; extra == \"rllib\" in /usr/local/lib/python3.7/dist-packages (from ray[rllib,tune]) (1.4.1)\n",
            "Requirement already satisfied: lz4; extra == \"rllib\" in /usr/local/lib/python3.7/dist-packages (from ray[rllib,tune]) (3.1.3)\n",
            "Requirement already satisfied: opencv-python-headless<=4.3.0.36; extra == \"rllib\" in /usr/local/lib/python3.7/dist-packages (from ray[rllib,tune]) (4.3.0.36)\n",
            "Requirement already satisfied: tabulate; extra == \"rllib\" in /usr/local/lib/python3.7/dist-packages (from ray[rllib,tune]) (0.8.9)\n",
            "Requirement already satisfied: sympy>=1.2 in /usr/local/lib/python3.7/dist-packages (from symfit) (1.7.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from symfit) (1.15.0)\n",
            "Requirement already satisfied: toposort in /usr/local/lib/python3.7/dist-packages (from symfit) (1.6)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.14.0->tensortrade) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.14.0->tensortrade) (1.3.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly>=4.5.0->tensortrade) (1.3.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->tensortrade) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->tensortrade) (2018.9)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.12.0->tensortrade) (5.0.5)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.12.0->tensortrade) (0.7.5)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.12.0->tensortrade) (0.18.0)\n",
            "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=7.12.0->tensortrade) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.12.0->tensortrade) (3.0.18)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.12.0->tensortrade) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.12.0->tensortrade) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.12.0->tensortrade) (56.0.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=7.12.0->tensortrade) (4.4.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tensortrade) (2.4.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tensortrade) (1.6.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tensortrade) (0.36.2)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tensortrade) (0.12.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tensortrade) (1.1.2)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tensortrade) (3.7.4.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tensortrade) (1.12.1)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tensortrade) (2.4.1)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tensortrade) (2.10.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tensortrade) (1.12)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tensortrade) (0.2.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tensortrade) (1.1.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tensortrade) (0.3.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tensortrade) (3.3.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->tensortrade) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->tensortrade) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->tensortrade) (2.4.7)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->ray[rllib,tune]) (1.6.3)\n",
            "Requirement already satisfied: chardet<5.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->ray[rllib,tune]) (3.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->ray[rllib,tune]) (20.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->ray[rllib,tune]) (5.1.0)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->ray[rllib,tune]) (3.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ray[rllib,tune]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray[rllib,tune]) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray[rllib,tune]) (1.24.3)\n",
            "Requirement already satisfied: hiredis in /usr/local/lib/python3.7/dist-packages (from aioredis->ray[rllib,tune]) (2.0.0)\n",
            "Requirement already satisfied: opencensus-context==0.1.2 in /usr/local/lib/python3.7/dist-packages (from opencensus->ray[rllib,tune]) (0.1.2)\n",
            "Requirement already satisfied: google-api-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from opencensus->ray[rllib,tune]) (1.26.3)\n",
            "Requirement already satisfied: blessings>=1.6 in /usr/local/lib/python3.7/dist-packages (from gpustat->ray[rllib,tune]) (1.7)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from gpustat->ray[rllib,tune]) (5.4.8)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.7/dist-packages (from gpustat->ray[rllib,tune]) (7.352.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy>=1.2->symfit) (1.2.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.14.0->tensortrade) (0.16.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython>=7.12.0->tensortrade) (0.2.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.12.0->tensortrade) (0.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython>=7.12.0->tensortrade) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.12.0->tensortrade) (0.2.5)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tensortrade) (0.4.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tensortrade) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tensortrade) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tensortrade) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tensortrade) (1.28.1)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray[rllib,tune]) (20.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray[rllib,tune]) (1.53.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1.0->tensortrade) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1.0->tensortrade) (3.10.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->tensortrade) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->tensortrade) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->tensortrade) (4.7.2)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1.0->tensortrade) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1.0->tensortrade) (3.4.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->tensortrade) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnrBC0eqOkAh",
        "outputId": "362393a1-3f13-4e23-b76a-fce842cf85a3"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.client import device_lib\n",
        "def get_available_gpus():\n",
        "    local_device_protos = device_lib.list_local_devices()\n",
        "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
        "\n",
        "num_gpus = len(get_available_gpus())\n",
        "print(num_gpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCSaJZO3WXoM"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMsqlxr-H3HM"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sALx-rZYRAq7"
      },
      "source": [
        "from gym.spaces import Discrete\n",
        "\n",
        "from tensortrade.env.default.actions import TensorTradeActionScheme\n",
        "\n",
        "from tensortrade.env.generic import ActionScheme, TradingEnv\n",
        "from tensortrade.core import Clock\n",
        "from tensortrade.oms.instruments import ExchangePair\n",
        "from tensortrade.oms.wallets import Portfolio\n",
        "from tensortrade.oms.instruments import Instrument\n",
        "from tensortrade.oms.orders import (\n",
        "    Order,\n",
        "    proportion_order,\n",
        "    TradeSide,\n",
        "    TradeType\n",
        ")\n",
        "from tensortrade.data.cdd import CryptoDataDownload\n",
        "from tensortrade.feed.core import Stream, DataFeed\n",
        "from tensortrade.oms.exchanges import Exchange\n",
        "from tensortrade.oms.services.execution.simulated import execute_order\n",
        "from tensortrade.oms.instruments import USD, BTC, ETH\n",
        "from tensortrade.oms.wallets import Wallet, Portfolio\n",
        "from tensortrade.agents import DQNAgent\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import tensortrade.env.default as default\n",
        "\n",
        "from ray import tune\n",
        "from ray.tune.registry import register_env\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aU9Z7DjSUN9"
      },
      "source": [
        "### Util"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "J76NI8Q1SWPQ",
        "outputId": "6688ebcd-4e10-4889-ba4e-a08c1e06b34f"
      },
      "source": [
        "from plotly.offline import init_notebook_mode\n",
        "init_notebook_mode(connected = True)\n",
        "\n",
        "def get_available_gpus():\n",
        "    local_device_protos = device_lib.list_local_devices()\n",
        "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
        "def get_available_cpus():\n",
        "    local_device_protos = device_lib.list_local_devices()\n",
        "    return [x.name for x in local_device_protos if x.device_type == 'CPU']\n",
        "num_gpus = len(get_available_gpus())\n",
        "num_cpus = len(get_available_cpus())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrfU8Re2W9DL"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LlhcNGnW-5U"
      },
      "source": [
        "# Fetch Data\n",
        "cdd = CryptoDataDownload()\n",
        "data = cdd.fetch(\"Bitfinex\", \"USD\", \"BTC\", \"1h\")\n",
        "def CPLogRSIMacdFeed():\n",
        "    \n",
        "    def rsi(price: Stream[float], period: float) -> Stream[float]:\n",
        "        r = price.diff()\n",
        "        upside = r.clamp_min(0).abs()\n",
        "        downside = r.clamp_max(0).abs()\n",
        "        rs = upside.ewm(alpha=1 / period).mean() / downside.ewm(alpha=1 / period).mean()\n",
        "        return 100*(1 - (1 + rs) ** -1)\n",
        "\n",
        "\n",
        "    def macd(price: Stream[float], fast: float, slow: float, signal: float) -> Stream[float]:\n",
        "        fm = price.ewm(span=fast, adjust=False).mean()\n",
        "        sm = price.ewm(span=slow, adjust=False).mean()\n",
        "        md = fm - sm\n",
        "        signal = md - md.ewm(span=signal, adjust=False).mean()\n",
        "        return signal\n",
        "\n",
        "\n",
        "    features = []\n",
        "    for c in data.columns[1:]:\n",
        "        s = Stream.source(list(data[c]), dtype=\"float\").rename(data[c].name)\n",
        "        features += [s]\n",
        "\n",
        "    cp = Stream.select(features, lambda s: s.name == \"close\")\n",
        "\n",
        "    features = [\n",
        "        # Remove auto correlation\n",
        "        cp.log().diff().rename(\"lr\"),\n",
        "        rsi(cp, period=20).rename(\"rsi\"),\n",
        "        macd(cp, fast=10, slow=50, signal=5).rename(\"macd\")\n",
        "    ]\n",
        "\n",
        "    feed = DataFeed(features)\n",
        "    feed.compile()\n",
        "\n",
        "\n",
        "    return feed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbpXi01kWsVf"
      },
      "source": [
        "### Portfolio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtM1BTG_Sp76"
      },
      "source": [
        "from tensortrade.oms.instruments import USD, BTC, ETH\n",
        "portfolios = []\n",
        "def getPortfolio(exchange, usd=50000, btc=1):\n",
        "    portfolio = Portfolio(USD, [\n",
        "        Wallet(exchange, usd * USD),\n",
        "        Wallet(exchange, btc * BTC)\n",
        "    ])\n",
        "\n",
        "    portfolios.append(portfolio)\n",
        "\n",
        "    return portfolio\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yO0_2U6Rli2"
      },
      "source": [
        "# Rendering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QlijV7MRsuB"
      },
      "source": [
        "# Position Change Chart\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensortrade.env.generic import Renderer\n",
        "\n",
        "\n",
        "class PositionChangeChart(Renderer):\n",
        "    def __init__(self, color: str = \"orange\"):\n",
        "        self.color = \"orange\"\n",
        "\n",
        "    def render(self, env, **kwargs):\n",
        "        history = pd.DataFrame(env.observer.renderer_history)\n",
        "\n",
        "        actions = list(history.action)\n",
        "        p = list(history.price)\n",
        "\n",
        "        buy = {}\n",
        "        sell = {}\n",
        "\n",
        "        for i in range(len(actions) - 1):\n",
        "            a1 = actions[i]\n",
        "            a2 = actions[i + 1]\n",
        "\n",
        "            if a1 != a2:\n",
        "                if a1 == 0 and a2 == 1:\n",
        "                    buy[i] = p[i]\n",
        "                else:\n",
        "                    sell[i] = p[i]\n",
        "\n",
        "        buy = pd.Series(buy)\n",
        "        sell = pd.Series(sell)\n",
        "\n",
        "        fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "        fig.suptitle(\"Performance\")\n",
        "\n",
        "        axs[0].plot(np.arange(len(p)), p, label=\"price\", color=self.color)\n",
        "        axs[0].scatter(buy.index, buy.values, marker=\"^\", color=\"green\")\n",
        "        axs[0].scatter(sell.index, sell.values, marker=\"^\", color=\"red\")\n",
        "        axs[0].set_title(\"Trading Chart\")\n",
        "\n",
        "        d = env.action_scheme.portfolio.performance\n",
        "\n",
        "        # lists = sorted(d.items()) # sorted by key, return+ a list of tuples\n",
        "        keys = d[0].keys()\n",
        "        x = range(len(d))\n",
        "        c = {}\n",
        "\n",
        "        for i in range(len(d)):\n",
        "            for key in keys:\n",
        "                if key != 'net_worth' and key != 'base_symbol':\n",
        "                    if not key in c:\n",
        "                        c[key] = []\n",
        "                    c[key].append(d[i][key])\n",
        "\n",
        "        for key in keys:\n",
        "            if key != 'net_worth' and key != 'base_symbol':\n",
        "                axs[1].plot(x, c[key], label=key)\n",
        "        \n",
        "        axs[1].set_title(\"Net Worth\")\n",
        "\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfQ9ARPdYlqK"
      },
      "source": [
        "# Define Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XHOVEuNeZsD"
      },
      "source": [
        "### Reward Schemes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCflOIQ2Ytct"
      },
      "source": [
        "# Reward Schemes\n",
        "from tensortrade.env.default.rewards import TensorTradeRewardScheme\n",
        "from tensortrade.env.generic import RewardScheme\n",
        "# from tensortrade.rewards import RewardStrategy\n",
        "# from tensortrade.trades import Trade\n",
        "\n",
        "class PBR(TensorTradeRewardScheme):\n",
        "    \"\"\"A reward scheme for position-based returns.\n",
        "\n",
        "    * Let :math:`p_t` denote the price at time t.\n",
        "    * Let :math:`x_t` denote the position at time t.\n",
        "    * Let :math:`R_t` denote the reward at time t.\n",
        "\n",
        "    Then the reward is defined as,\n",
        "    :math:`R_{t} = (p_{t} - p_{t-1}) \\cdot x_{t}`.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    price : `Stream`\n",
        "        The price stream to use for computing rewards.\n",
        "    \"\"\"\n",
        "\n",
        "    registered_name = \"pbr\"\n",
        "\n",
        "    def __init__(self, price: 'Stream') -> None:\n",
        "        super().__init__()\n",
        "        self.position = -1\n",
        "\n",
        "        r = Stream.sensor(price, lambda p: p.value, dtype=\"float\").diff()\n",
        "        position = Stream.sensor(self, lambda rs: rs.position, dtype=\"float\")\n",
        "\n",
        "        reward = (position * r).fillna(0).rename(\"reward\")\n",
        "\n",
        "        self.feed = DataFeed([reward])\n",
        "        self.feed.compile()\n",
        "\n",
        "    def on_action(self, action: int) -> None:\n",
        "        self.position = -1 if action == 0 else 1\n",
        "\n",
        "    def get_reward(self, portfolio: 'Portfolio') -> float:\n",
        "        return self.feed.next()[\"reward\"]\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        \"\"\"Resets the `position` and `feed` of the reward scheme.\"\"\"\n",
        "        self.position = -1\n",
        "        self.feed.reset()\n",
        "\n",
        "\n",
        "# class DirectProfitStrategy(RewardScheme):\n",
        "#     '''This reward = how much money that the strategy earns'''\n",
        "#     def reset(self):\n",
        "#         \"\"\"Necessary to reset the open amount and the last price\"\"\"\n",
        "#         self._open_amount= 0\n",
        "#         self._last_price = 0\n",
        "\n",
        "#     def get_reward(self, current_step: int, trade: Trade) -> float:\n",
        "#         last_price = self._last_price\n",
        "#         price = trade.price\n",
        "#         last_amount = self._open_amount\n",
        "\n",
        "#         #reset values\n",
        "#         if trade.is_hold:\n",
        "#             pass\n",
        "#         elif trade.is_buy:\n",
        "#             self._open_amount += trade.amount\n",
        "#         elif trade.is_sell:\n",
        "#             self._open_amount -= trade.amount\n",
        "\n",
        "#         last_price = self._last_price\n",
        "\n",
        "#         self._last_price = trade.price        \n",
        "        \n",
        "#         if trade.is_hold:\n",
        "#             return last_amount * (price - last_price)\n",
        "#         else:\n",
        "#             return last_amount * (price - last_price) - trade.amount * trade.price * 0.0003"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68qfUWV3eipZ"
      },
      "source": [
        "### Action Scheme"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8yJNXEiehe9"
      },
      "source": [
        "# BuySellHold\n",
        "class BuySellHold(TensorTradeActionScheme):\n",
        "\n",
        "    registered_name = \"bsh\"\n",
        "\n",
        "    def __init__(self, cash: 'Wallet', asset: 'Wallet'):\n",
        "        super().__init__()\n",
        "        self.cash = cash\n",
        "        self.asset = asset\n",
        "\n",
        "        self.listeners = []\n",
        "        self.action = 0\n",
        "\n",
        "    @property\n",
        "    def action_space(self):\n",
        "        return Discrete(2)\n",
        "\n",
        "    def attach(self, listener):\n",
        "        self.listeners += [listener]\n",
        "        return self\n",
        "\n",
        "    def get_orders(self, action: int, portfolio: 'Portfolio'):\n",
        "        order = None\n",
        "\n",
        "        if abs(action - self.action) > 0:\n",
        "            src = self.cash if self.action == 0 else self.asset\n",
        "            tgt = self.asset if self.action == 0 else self.cash\n",
        "            order = proportion_order(portfolio, src, tgt, 1.0)\n",
        "            self.action = action\n",
        "\n",
        "        for listener in self.listeners:\n",
        "            listener.on_action(action)\n",
        "\n",
        "        return [order]\n",
        "\n",
        "    def reset(self):\n",
        "        super().reset()\n",
        "        self.action = 0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiAfxNQhfEME"
      },
      "source": [
        "# Trading Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHm2q6DrfSa9"
      },
      "source": [
        "from tensortrade.env.default.rewards import SimpleProfit\n",
        "from tensortrade.env.default.actions import BSH\n",
        "\n",
        "def create_env(config):\n",
        "    p = Stream.source(list(data[\"close\"]), dtype=\"float\").rename(\"USD-BTC\")\n",
        "\n",
        "    # Define exchange\n",
        "    bitfinex = Exchange(\"bitfinex\", service=execute_order)(\n",
        "        p\n",
        "    )\n",
        "\n",
        "    portfolio = getPortfolio(bitfinex, 50000, 1)\n",
        "\n",
        "    feed = CPLogRSIMacdFeed()\n",
        "\n",
        "    for i in range(5):\n",
        "        print(feed.next())\n",
        "\n",
        "    reward_scheme = PBR(price=p)\n",
        "\n",
        "    action_scheme = BuySellHold(\n",
        "        cash=portfolio.wallets[0],\n",
        "        asset=portfolio.wallets[1]\n",
        "    ).attach(reward_scheme)\n",
        "\n",
        "    renderer_feed = DataFeed([\n",
        "        Stream.source(list(data[\"date\"])).rename(\"date\"),\n",
        "        Stream.source(list(data[\"open\"]), dtype=\"float\").rename(\"open\"),\n",
        "        Stream.source(list(data[\"high\"]), dtype=\"float\").rename(\"high\"),\n",
        "        Stream.source(list(data[\"low\"]), dtype=\"float\").rename(\"low\"),\n",
        "        Stream.source(list(data[\"close\"]), dtype=\"float\").rename(\"close\"),\n",
        "        Stream.source(list(data[\"volume\"]), dtype=\"float\").rename(\"volume\")\n",
        "    ])\n",
        "\n",
        "    environment = default.create(\n",
        "        feed=feed,\n",
        "        portfolio=portfolio,\n",
        "        action_scheme=BSH(\n",
        "            cash=portfolio.wallets[0],\n",
        "            asset=portfolio.wallets[1]\n",
        "        ),\n",
        "        reward_scheme=SimpleProfit(),\n",
        "        renderer_feed=renderer_feed,\n",
        "        # renderer=default.renderers.PlotlyTradingChart(),\n",
        "        renderer=PositionChangeChart(),\n",
        "        window_size=config[\"window_size\"],\n",
        "        max_allowed_loss=0.6\n",
        "    )\n",
        "    return environment\n",
        "\n",
        "register_env(\"TradingEnv\", create_env)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wov4WsnIjAcM"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tPRbshUMjELM",
        "outputId": "afb2f506-7eb5-4393-8907-d327a234b1e2"
      },
      "source": [
        "analysis = tune.run(\n",
        "    \"PPO\",\n",
        "    stop={\n",
        "      \"training_iteration\": 100\n",
        "    },\n",
        "    config={\n",
        "        \"env\": \"TradingEnv\",\n",
        "        \"env_config\": {\n",
        "            \"window_size\": 25\n",
        "        },\n",
        "        \"log_level\": \"DEBUG\",\n",
        "        \"framework\": \"torch\",\n",
        "        \"ignore_worker_failures\": True,\n",
        "        \"num_workers\": 1,\n",
        "        \"num_gpus\": 0,\n",
        "        \"clip_rewards\": True,\n",
        "        \"lr\": 8e-6,\n",
        "        \"lr_schedule\": [\n",
        "            [0, 1e-1],\n",
        "            [int(1e2), 1e-2],\n",
        "            [int(1e3), 1e-3],\n",
        "            [int(1e4), 1e-4],\n",
        "            [int(1e5), 1e-5],\n",
        "            [int(1e6), 1e-6],\n",
        "            [int(1e7), 1e-7]\n",
        "        ],\n",
        "        \"gamma\": 0,\n",
        "        \"observation_filter\": \"MeanStdFilter\",\n",
        "        \"lambda\": 0.72,\n",
        "        \"vf_loss_coeff\": 0.5,\n",
        "        \"entropy_coeff\": 0.01\n",
        "    },\n",
        "    checkpoint_at_end=True,\n",
        "    mode=\"max\"\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.3/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc  </th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>PENDING </td><td>     </td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m non-resource variables are not supported in the long term\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m non-resource variables are not supported in the long term\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:45:13,100\tDEBUG rollout_worker.py:1122 -- Creating policy for default_policy\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:45:13,101\tDEBUG catalog.py:632 -- Created preprocessor <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f2d310b4c90>: Box(-inf, inf, (25, 3), float32) -> (25, 3)\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:45:13,154\tDEBUG rollout_worker.py:1122 -- Creating policy for default_policy\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:45:13,155\tDEBUG catalog.py:632 -- Created preprocessor <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fe13b78dad0>: Box(-inf, inf, (25, 3), float32) -> (25, 3)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:45:13,116\tINFO torch_policy.py:112 -- TorchPolicy running on CPU.\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:45:13,148\tDEBUG rollout_worker.py:533 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:45:13,148\tDEBUG rollout_worker.py:680 -- Created rollout worker with env <ray.rllib.env.base_env._VectorEnvToBaseEnv object at 0x7f2d30d35fd0> (<TradingEnv instance>), policies {'default_policy': <ray.rllib.policy.policy_template.PPOTorchPolicy object at 0x7f2d33b61e50>}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:45:13,179\tINFO torch_policy.py:112 -- TorchPolicy running on CPU.\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:45:13,207\tINFO rollout_worker.py:1161 -- Built policy map: {'default_policy': <ray.rllib.policy.policy_template.PPOTorchPolicy object at 0x7fe1391f8650>}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:45:13,207\tINFO rollout_worker.py:1162 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fe13b78dad0>}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:45:13,207\tDEBUG rollout_worker.py:533 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:45:13,208\tINFO rollout_worker.py:563 -- Built filter map: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=0, mean_mean=0.0, mean_std=0.0), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:45:13,208\tDEBUG rollout_worker.py:680 -- Created rollout worker with env None (None), policies {'default_policy': <ray.rllib.policy.policy_template.PPOTorchPolicy object at 0x7fe1391f8650>}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:45:13,211\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:45:13,220\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:45:13,220\tDEBUG sampler.py:537 -- No episode horizon specified, assuming inf.\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:45:13,222\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=0.0, max=0.0, mean=0.0)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:45:13,222\tINFO sampler.py:589 -- Info return from env: {0: {'agent0': None}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:45:13,223\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=0.0, max=0.0, mean=0.0)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:45:13,223\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=0.0, max=0.0, mean=0.0)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:45:13,224\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': None,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': None,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': None},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:45:13,226\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-0.693, max=-0.693, mean=-0.693),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.5, max=0.5, mean=0.5),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m {'lr': nan, 'rsi': nan, 'macd': 0.0}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m {'lr': 0.00045761355334761333, 'rsi': 100.0, 'macd': 0.3802733214494462}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m {'lr': -0.0022099203461092287, 'rsi': 16.45021645021696, 'macd': -1.2850831900864756}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m {'lr': -0.001789832853125617, 'rsi': 9.61512851245181, 'macd': -3.5410157838872425}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m {'lr': 0.00872379407148216, 'rsi': 71.2075575333076, 'macd': 2.810321575123835}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:45:13,704\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-0.011, max=0.016, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-0.702, max=-0.683, mean=-0.694),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((200,), dtype=float32, min=0.496, max=0.505, mean=0.5),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.5),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-1.013, max=1.007, mean=-0.384),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=1097499381.0, max=1097499381.0, mean=1097499381.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 1, 'net_worth': 58743.0}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-5.29, max=4.949, mean=0.17),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((200, 25, 3), dtype=float32, min=-5.29, max=4.949, mean=0.172),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.385),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.385),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-0.009, max=0.013, mean=-0.001)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:45:13,706\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-0.011, max=0.016, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-0.702, max=-0.683, mean=-0.694),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.496, max=0.505, mean=0.5),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.5),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-1.013, max=1.007, mean=-0.384),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=1097499381.0, max=1097499381.0, mean=1097499381.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 1, 'net_worth': 58743.0}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-5.29, max=4.949, mean=0.17),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-5.29, max=4.949, mean=0.172),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.385),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.385),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-0.009, max=0.013, mean=-0.001)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:45:22,948\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:45:22,952\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-0.013, max=0.011, mean=-0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((128,), dtype=float32, min=-0.699, max=-0.686, mean=-0.693),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((128,), dtype=float32, min=0.497, max=0.503, mean=0.5),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.547),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((128,), dtype=float32, min=-0.791, max=1.78, mean=-0.193),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((128,), dtype=int64, min=199378586.0, max=1820536437.0, mean=1202717640.688),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((128,), dtype=object, head={'step': 5, 'net_worth': 58439.494904832005}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((128, 25, 3), dtype=float32, min=-4.268, max=4.306, mean=-0.03),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((128, 25, 3), dtype=float32, min=-4.265, max=4.383, mean=-0.027),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.539),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((128,), dtype=int64, min=0.0, max=33.0, mean=16.328),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.539),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((128,), dtype=float32, min=-0.006, max=0.007, mean=0.0)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:45:22,964\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 0.2,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 8e-06,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.6931424736976624,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': -6.973376320829061e-10,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': 0.19292323291301727,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': 0.5809093713760376,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=-0.001, max=-0.001, mean=-0.001),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.7898350954055786},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 4000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-45-30\n",
            "  done: false\n",
            "  episode_len_mean: 275.92857142857144\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.5074792722103028\n",
            "  episode_reward_mean: -0.5116379591160387\n",
            "  episode_reward_min: -0.526563729993532\n",
            "  episodes_this_iter: 14\n",
            "  episodes_total: 14\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.2\n",
            "          cur_lr: 8.0e-06\n",
            "          entropy: 0.6804733481258154\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.012773005350027233\n",
            "          policy_loss: -0.03113506973022595\n",
            "          total_loss: 0.32169100595638156\n",
            "          vf_explained_var: 0.027326490730047226\n",
            "          vf_loss: 0.7141524199396372\n",
            "    num_agent_steps_sampled: 4000\n",
            "    num_steps_sampled: 4000\n",
            "    num_steps_trained: 4000\n",
            "  iterations_since_restore: 1\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 29.18076923076923\n",
            "    ram_util_percent: 15.149999999999999\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04850730571827867\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.906805252021803\n",
            "    mean_inference_ms: 1.2915693381761675\n",
            "    mean_raw_obs_processing_ms: 0.15470213486057197\n",
            "  time_since_restore: 17.56873846054077\n",
            "  time_this_iter_s: 17.56873846054077\n",
            "  time_total_s: 17.56873846054077\n",
            "  timers:\n",
            "    learn_throughput: 510.893\n",
            "    learn_time_ms: 7829.423\n",
            "    sample_throughput: 411.058\n",
            "    sample_time_ms: 9730.999\n",
            "    update_time_ms: 1.856\n",
            "  timestamp: 1619351130\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 4000\n",
            "  training_iteration: 1\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:45:30,787\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=4015, mean_mean=13.55875526756683, mean_std=10.535694717648523), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.9/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         17.5687</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-0.511638</td><td style=\"text-align: right;\">           -0.507479</td><td style=\"text-align: right;\">           -0.526564</td><td style=\"text-align: right;\">           275.929</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 8000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-45-48\n",
            "  done: false\n",
            "  episode_len_mean: 285.5\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.5067132428447974\n",
            "  episode_reward_mean: -0.5113396681235785\n",
            "  episode_reward_min: -0.526563729993532\n",
            "  episodes_this_iter: 14\n",
            "  episodes_total: 28\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.2\n",
            "          cur_lr: 0.0007000000000000001\n",
            "          entropy: 0.6143910735845566\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.05595968244597316\n",
            "          policy_loss: -0.15788354276446626\n",
            "          total_loss: 0.04525322307017632\n",
            "          vf_explained_var: 0.3696795701980591\n",
            "          vf_loss: 0.39617747347801924\n",
            "    num_agent_steps_sampled: 8000\n",
            "    num_steps_sampled: 8000\n",
            "    num_steps_trained: 8000\n",
            "  iterations_since_restore: 2\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.791666666666668\n",
            "    ram_util_percent: 15.199999999999998\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.048503706858012356\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.9076155416981909\n",
            "    mean_inference_ms: 1.291382713052643\n",
            "    mean_raw_obs_processing_ms: 0.15465634734547667\n",
            "  time_since_restore: 34.97558546066284\n",
            "  time_this_iter_s: 17.40684700012207\n",
            "  time_total_s: 34.97558546066284\n",
            "  timers:\n",
            "    learn_throughput: 515.886\n",
            "    learn_time_ms: 7753.644\n",
            "    sample_throughput: 411.265\n",
            "    sample_time_ms: 9726.091\n",
            "    update_time_ms: 1.723\n",
            "  timestamp: 1619351148\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 8000\n",
            "  training_iteration: 2\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:45:48,231\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=8029, mean_mean=13.656625636267684, mean_std=10.448150139803257), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.9/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         34.9756</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">-0.51134</td><td style=\"text-align: right;\">           -0.506713</td><td style=\"text-align: right;\">           -0.526564</td><td style=\"text-align: right;\">             285.5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 12000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-46-06\n",
            "  done: false\n",
            "  episode_len_mean: 329.3333333333333\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.505291791327272\n",
            "  episode_reward_mean: -0.5105754281342777\n",
            "  episode_reward_min: -0.526563729993532\n",
            "  episodes_this_iter: 8\n",
            "  episodes_total: 36\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.30000000000000004\n",
            "          cur_lr: 0.00030000000000000003\n",
            "          entropy: 0.5044625252485275\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.05649646371603012\n",
            "          policy_loss: -0.12002527405275032\n",
            "          total_loss: 0.09052051565959118\n",
            "          vf_explained_var: 0.4260028600692749\n",
            "          vf_loss: 0.3972829384729266\n",
            "    num_agent_steps_sampled: 12000\n",
            "    num_steps_sampled: 12000\n",
            "    num_steps_trained: 12000\n",
            "  iterations_since_restore: 3\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.76923076923077\n",
            "    ram_util_percent: 15.199999999999998\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04846620347550744\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.911785506996105\n",
            "    mean_inference_ms: 1.2911596045345313\n",
            "    mean_raw_obs_processing_ms: 0.1545951472557502\n",
            "  time_since_restore: 52.77548885345459\n",
            "  time_this_iter_s: 17.799903392791748\n",
            "  time_total_s: 52.77548885345459\n",
            "  timers:\n",
            "    learn_throughput: 513.244\n",
            "    learn_time_ms: 7793.557\n",
            "    sample_throughput: 408.566\n",
            "    sample_time_ms: 9790.343\n",
            "    update_time_ms: 1.716\n",
            "  timestamp: 1619351166\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 12000\n",
            "  training_iteration: 3\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:46:06,067\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=12037, mean_mean=14.169791364776493, mean_std=10.318636288221066), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.9/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         52.7755</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">-0.510575</td><td style=\"text-align: right;\">           -0.505292</td><td style=\"text-align: right;\">           -0.526564</td><td style=\"text-align: right;\">           329.333</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:46:13,705\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': { 'net_worth': 54237.556961928,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                             'step': 163},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=-1.531, max=1.414, mean=-0.038),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': 1,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': -0.009937932766476587,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:46:13,708\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-0.629, max=0.633, mean=0.002),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-1.511, max=-1.511, mean=-1.511),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.221, max=0.221, mean=0.221),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.694, max=-0.694, mean=-0.694)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:46:13,709\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=-25.292, max=64.545, mean=13.531)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:46:13,709\tINFO sampler.py:589 -- Info return from env: {0: {'agent0': {'net_worth': 54074.84, 'step': 164}}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:46:13,710\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=-25.292, max=64.545, mean=13.531)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:46:13,710\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=-1.544, max=1.409, mean=-0.061)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:46:14,055\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-1.67, max=1.679, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-3.012, max=-0.035, mean=-0.5),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((200,), dtype=float32, min=0.049, max=0.966, mean=0.68),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.515),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-1.758, max=1.696, mean=0.052),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=1478112756.0, max=1478112756.0, mean=1478112756.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 99, 'net_worth': 55867.521115193995}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-3.718, max=3.037, mean=0.052),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((200, 25, 3), dtype=float32, min=-3.718, max=3.037, mean=0.056),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.235),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=114.0, max=114.0, mean=114.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.235),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-1.1, max=0.951, mean=-0.287)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:46:14,057\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-1.67, max=1.679, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-3.012, max=-0.035, mean=-0.5),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.049, max=0.966, mean=0.68),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.515),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-1.758, max=1.696, mean=0.052),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=1478112756.0, max=1478112756.0, mean=1478112756.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 99, 'net_worth': 55867.521115193995}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-3.718, max=3.037, mean=0.052),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-3.718, max=3.037, mean=0.056),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.235),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=114.0, max=114.0, mean=114.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.235),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-1.1, max=0.951, mean=-0.287)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:46:14,061\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:46:22,972\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-1.735, max=1.736, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((128,), dtype=float32, min=-2.098, max=-0.031, mean=-0.516),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((128,), dtype=float32, min=0.123, max=0.97, mean=0.659),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.531),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((128,), dtype=float32, min=-2.182, max=2.226, mean=-0.07),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((128,), dtype=int64, min=211763147.0, max=1478112756.0, mean=797920252.5),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((128,), dtype=object, head={'step': 664, 'net_worth': 40552.71}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((128, 25, 3), dtype=float32, min=-10.839, max=4.883, mean=0.048),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((128, 25, 3), dtype=float32, min=-10.797, max=4.876, mean=0.048),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.266),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((128,), dtype=int64, min=96.0, max=120.0, mean=108.078),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.266),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((128,), dtype=float32, min=-1.124, max=0.832, mean=-0.27)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:46:22,983\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 0.45000000000000007,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 9.800000000000001e-05,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.44033417105674744,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': 0.03042416088283062,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': -0.03002825193107128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': 0.17397592961788177,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.466, max=0.466, mean=0.466),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.3894333243370056},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:46:23,193\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 16000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-46-24\n",
            "  done: false\n",
            "  episode_len_mean: 380.07142857142856\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.5045670160033054\n",
            "  episode_reward_mean: -0.5112640400632257\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 6\n",
            "  episodes_total: 42\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.45000000000000007\n",
            "          cur_lr: 9.800000000000001e-05\n",
            "          entropy: 0.44482414331287146\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.029723985644523054\n",
            "          policy_loss: -0.09434056770987809\n",
            "          total_loss: 0.08952181438507978\n",
            "          vf_explained_var: 0.5152446031570435\n",
            "          vf_loss: 0.3498696628957987\n",
            "    num_agent_steps_sampled: 16000\n",
            "    num_steps_sampled: 16000\n",
            "    num_steps_trained: 16000\n",
            "  iterations_since_restore: 4\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 29.15\n",
            "    ram_util_percent: 15.199999999999998\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04847391298298688\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.9206427232760045\n",
            "    mean_inference_ms: 1.2903786561975719\n",
            "    mean_raw_obs_processing_ms: 0.15478039514918482\n",
            "  time_since_restore: 71.20922064781189\n",
            "  time_this_iter_s: 18.4337317943573\n",
            "  time_total_s: 71.20922064781189\n",
            "  timers:\n",
            "    learn_throughput: 511.096\n",
            "    learn_time_ms: 7826.312\n",
            "    sample_throughput: 401.278\n",
            "    sample_time_ms: 9968.148\n",
            "    update_time_ms: 1.707\n",
            "  timestamp: 1619351184\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 16000\n",
            "  training_iteration: 4\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:46:24,538\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=16043, mean_mean=14.407192040287589, mean_std=10.17430156706191), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.9/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         71.2092</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">-0.511264</td><td style=\"text-align: right;\">           -0.504567</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           380.071</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 20000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-46-43\n",
            "  done: false\n",
            "  episode_len_mean: 417.7391304347826\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.5035977114295217\n",
            "  episode_reward_mean: -0.5107730862644606\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 4\n",
            "  episodes_total: 46\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.675\n",
            "          cur_lr: 9.400000000000001e-05\n",
            "          entropy: 0.4064576104283333\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.019368047709576786\n",
            "          policy_loss: -0.0835642981401179\n",
            "          total_loss: 0.08152775705093518\n",
            "          vf_explained_var: 0.5601264238357544\n",
            "          vf_loss: 0.3121664081700146\n",
            "    num_agent_steps_sampled: 20000\n",
            "    num_steps_sampled: 20000\n",
            "    num_steps_trained: 20000\n",
            "  iterations_since_restore: 5\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 29.011111111111106\n",
            "    ram_util_percent: 15.27037037037037\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04848110057397901\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.9271183025606841\n",
            "    mean_inference_ms: 1.2906484688657514\n",
            "    mean_raw_obs_processing_ms: 0.15487641726007742\n",
            "  time_since_restore: 89.73388743400574\n",
            "  time_this_iter_s: 18.524666786193848\n",
            "  time_total_s: 89.73388743400574\n",
            "  timers:\n",
            "    learn_throughput: 509.255\n",
            "    learn_time_ms: 7854.619\n",
            "    sample_throughput: 396.657\n",
            "    sample_time_ms: 10084.282\n",
            "    update_time_ms: 1.705\n",
            "  timestamp: 1619351203\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 20000\n",
            "  training_iteration: 5\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.9/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         89.7339</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">-0.510773</td><td style=\"text-align: right;\">           -0.503598</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           417.739</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:46:43,101\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=20047, mean_mean=14.571464911351631, mean_std=10.052935665662485), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 24000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-47-02\n",
            "  done: false\n",
            "  episode_len_mean: 470.27450980392155\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.5018533004236672\n",
            "  episode_reward_mean: -0.5099953761724857\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 5\n",
            "  episodes_total: 51\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.675\n",
            "          cur_lr: 9.0e-05\n",
            "          entropy: 0.3756561949849129\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.015828532603336498\n",
            "          policy_loss: -0.0707627217634581\n",
            "          total_loss: 0.07667460915399715\n",
            "          vf_explained_var: 0.6105383634567261\n",
            "          vf_loss: 0.2810192690230906\n",
            "    num_agent_steps_sampled: 24000\n",
            "    num_steps_sampled: 24000\n",
            "    num_steps_trained: 24000\n",
            "  iterations_since_restore: 6\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 29.203703703703702\n",
            "    ram_util_percent: 15.300000000000002\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04851437774429147\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.9367293655941884\n",
            "    mean_inference_ms: 1.2909639335368979\n",
            "    mean_raw_obs_processing_ms: 0.15508263758991297\n",
            "  time_since_restore: 108.68980813026428\n",
            "  time_this_iter_s: 18.955920696258545\n",
            "  time_total_s: 108.68980813026428\n",
            "  timers:\n",
            "    learn_throughput: 506.586\n",
            "    learn_time_ms: 7895.987\n",
            "    sample_throughput: 391.733\n",
            "    sample_time_ms: 10211.026\n",
            "    update_time_ms: 1.7\n",
            "  timestamp: 1619351222\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 24000\n",
            "  training_iteration: 6\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:47:02,094\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=24052, mean_mean=14.687437765643883, mean_std=10.001834760952605), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.9/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">          108.69</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">-0.509995</td><td style=\"text-align: right;\">           -0.501853</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           470.275</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 28000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-47-21\n",
            "  done: false\n",
            "  episode_len_mean: 502.5740740740741\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.5018533004236672\n",
            "  episode_reward_mean: -0.509690464068943\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 3\n",
            "  episodes_total: 54\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.675\n",
            "          cur_lr: 8.6e-05\n",
            "          entropy: 0.34511471912264824\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.01562951973755844\n",
            "          policy_loss: -0.06787052136496641\n",
            "          total_loss: 0.05889565183315426\n",
            "          vf_explained_var: 0.6589968204498291\n",
            "          vf_loss: 0.23933478887192905\n",
            "    num_agent_steps_sampled: 28000\n",
            "    num_steps_sampled: 28000\n",
            "    num_steps_trained: 28000\n",
            "  iterations_since_restore: 7\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 29.15555555555556\n",
            "    ram_util_percent: 15.300000000000002\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04853943925619954\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.9433554098419578\n",
            "    mean_inference_ms: 1.2910766537097458\n",
            "    mean_raw_obs_processing_ms: 0.15521164400776297\n",
            "  time_since_restore: 127.84131979942322\n",
            "  time_this_iter_s: 19.151511669158936\n",
            "  time_total_s: 127.84131979942322\n",
            "  timers:\n",
            "    learn_throughput: 504.909\n",
            "    learn_time_ms: 7922.213\n",
            "    sample_throughput: 387.113\n",
            "    sample_time_ms: 10332.909\n",
            "    update_time_ms: 1.698\n",
            "  timestamp: 1619351241\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 28000\n",
            "  training_iteration: 7\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:47:21,284\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=28055, mean_mean=14.787342568412287, mean_std=9.94105045426571), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.9/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         127.841</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">-0.50969</td><td style=\"text-align: right;\">           -0.501853</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           502.574</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:47:21,319\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': { 'net_worth': 40807.832287373996,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                             'step': 861},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=-1.491, max=1.704, mean=0.234),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': 1,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': 0.0005451076587625892,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:47:21,322\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-0.921, max=0.918, mean=-0.002),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-0.148, max=-0.148, mean=-0.148),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.863, max=0.863, mean=0.863),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.876, max=-0.876, mean=-0.876)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:47:21,326\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=-22.128, max=68.42, mean=17.269)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:47:21,326\tINFO sampler.py:589 -- Info return from env: {0: {'agent0': {'net_worth': 40739.899517744, 'step': 862}}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:47:21,326\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=-22.128, max=68.42, mean=17.269)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:47:21,326\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=-1.493, max=1.695, mean=0.189)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:47:21,587\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((83, 2), dtype=float32, min=-3.026, max=3.021, mean=-0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((83,), dtype=float32, min=-3.053, max=-0.002, mean=-0.459),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((83,), dtype=float32, min=0.047, max=0.998, mean=0.722),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((83,), dtype=int64, min=0.0, max=1.0, mean=0.639),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((83,), dtype=float32, min=-1.75, max=1.681, mean=-0.084),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((83,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((83,), dtype=bool, min=0.0, max=1.0, mean=0.012),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((83,), dtype=int64, min=1087142304.0, max=1087142304.0, mean=1087142304.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((83,), dtype=object, head={'step': 862, 'net_worth': 40739.899517744}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((83, 25, 3), dtype=float32, min=-5.433, max=2.784, mean=-0.173),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((83, 25, 3), dtype=float32, min=-5.433, max=2.784, mean=-0.165),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((83,), dtype=float32, min=-1.0, max=1.0, mean=-0.386),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((83,), dtype=int64, min=194.0, max=194.0, mean=194.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((83,), dtype=float32, min=-1.0, max=1.0, mean=-0.386),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((83,), dtype=float32, min=-1.119, max=0.918, mean=-0.301)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:47:21,885\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-3.026, max=3.021, mean=-0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-3.736, max=-0.002, mean=-0.353),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.024, max=0.998, mean=0.787),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.54),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-1.75, max=1.681, mean=-0.048),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.005),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=504878274.0, max=1087142304.0, mean=746517846.45),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 862, 'net_worth': 40739.899517744}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-5.433, max=4.544, mean=-0.164),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-5.433, max=4.544, mean=-0.168),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.2),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=194.0, max=195.0, mean=194.585),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.2),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-1.125, max=1.243, mean=-0.152)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:47:21,889\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:47:32,591\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:47:32,594\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-2.48, max=2.48, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((128,), dtype=float32, min=-3.523, max=-0.007, mean=-0.394),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((128,), dtype=float32, min=0.03, max=0.993, mean=0.755),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.516),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((128,), dtype=float32, min=-2.453, max=3.249, mean=-0.033),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((128,), dtype=int64, min=252984601.0, max=1744529139.0, mean=644735278.305),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((128,), dtype=object, head={'step': 492, 'net_worth': 58532.7127596}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((128, 25, 3), dtype=float32, min=-5.389, max=11.26, mean=0.077),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((128, 25, 3), dtype=float32, min=-5.38, max=11.25, mean=0.077),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.312),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((128,), dtype=int64, min=194.0, max=217.0, mean=205.688),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.312),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((128,), dtype=float32, min=-1.197, max=1.069, mean=-0.315)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:47:32,604\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 0.675,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 8.2e-05,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.37102439999580383,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': 2.003287535856657e-09,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': 0.032817158848047256,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': 0.2140052318572998,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.446, max=0.446, mean=0.446),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.36979663372039795},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 32000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-47-40\n",
            "  done: false\n",
            "  episode_len_mean: 543.8965517241379\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.49939614227815377\n",
            "  episode_reward_mean: -0.5091882757317687\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 4\n",
            "  episodes_total: 58\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.675\n",
            "          cur_lr: 8.2e-05\n",
            "          entropy: 0.31171534955501556\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.01334575074724853\n",
            "          policy_loss: -0.05767484303214587\n",
            "          total_loss: 0.07440634843078442\n",
            "          vf_explained_var: 0.6349403262138367\n",
            "          vf_loss: 0.2523799273185432\n",
            "    num_agent_steps_sampled: 32000\n",
            "    num_steps_sampled: 32000\n",
            "    num_steps_trained: 32000\n",
            "  iterations_since_restore: 8\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 29.289285714285715\n",
            "    ram_util_percent: 15.300000000000002\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04858014855013688\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.9523781419394619\n",
            "    mean_inference_ms: 1.2919082361956404\n",
            "    mean_raw_obs_processing_ms: 0.15542039030634794\n",
            "  time_since_restore: 147.27351427078247\n",
            "  time_this_iter_s: 19.432194471359253\n",
            "  time_total_s: 147.27351427078247\n",
            "  timers:\n",
            "    learn_throughput: 503.089\n",
            "    learn_time_ms: 7950.885\n",
            "    sample_throughput: 382.763\n",
            "    sample_time_ms: 10450.336\n",
            "    update_time_ms: 1.709\n",
            "  timestamp: 1619351260\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 32000\n",
            "  training_iteration: 8\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:47:40,753\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=32059, mean_mean=14.867937792010459, mean_std=9.920764515067043), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.9/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         147.274</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">-0.509188</td><td style=\"text-align: right;\">           -0.499396</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           543.897</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 36000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-47-59\n",
            "  done: false\n",
            "  episode_len_mean: 581.5409836065573\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4968824346877172\n",
            "  episode_reward_mean: -0.5086559809259502\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 3\n",
            "  episodes_total: 61\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.675\n",
            "          cur_lr: 7.8e-05\n",
            "          entropy: 0.2994602797552943\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.012326803291216493\n",
            "          policy_loss: -0.05697069870075211\n",
            "          total_loss: 0.06229692918714136\n",
            "          vf_explained_var: 0.6637037396430969\n",
            "          vf_loss: 0.22788327746093273\n",
            "    num_agent_steps_sampled: 36000\n",
            "    num_steps_sampled: 36000\n",
            "    num_steps_trained: 36000\n",
            "  iterations_since_restore: 9\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 29.103703703703705\n",
            "    ram_util_percent: 15.300000000000002\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04861049247432845\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.9592147116969758\n",
            "    mean_inference_ms: 1.2923878062648744\n",
            "    mean_raw_obs_processing_ms: 0.15557543396243748\n",
            "  time_since_restore: 166.4289424419403\n",
            "  time_this_iter_s: 19.155428171157837\n",
            "  time_total_s: 166.4289424419403\n",
            "  timers:\n",
            "    learn_throughput: 501.913\n",
            "    learn_time_ms: 7969.503\n",
            "    sample_throughput: 380.429\n",
            "    sample_time_ms: 10514.457\n",
            "    update_time_ms: 1.715\n",
            "  timestamp: 1619351279\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 36000\n",
            "  training_iteration: 9\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:47:59,946\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=36062, mean_mean=14.973842884344265, mean_std=9.89168508978961), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.9/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         166.429</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">-0.508656</td><td style=\"text-align: right;\">           -0.496882</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           581.541</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 40000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-48-19\n",
            "  done: false\n",
            "  episode_len_mean: 620.015625\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4960695295206272\n",
            "  episode_reward_mean: -0.5081178288005107\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 3\n",
            "  episodes_total: 64\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.675\n",
            "          cur_lr: 7.400000000000001e-05\n",
            "          entropy: 0.29189626779407263\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.012876297114416957\n",
            "          policy_loss: -0.05711251172760967\n",
            "          total_loss: 0.05670653784181923\n",
            "          vf_explained_var: 0.6738899946212769\n",
            "          vf_loss: 0.21609301795251667\n",
            "    num_agent_steps_sampled: 40000\n",
            "    num_steps_sampled: 40000\n",
            "    num_steps_trained: 40000\n",
            "  iterations_since_restore: 10\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.864285714285717\n",
            "    ram_util_percent: 15.335714285714284\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04863723085698787\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.966608668740967\n",
            "    mean_inference_ms: 1.2926980413885552\n",
            "    mean_raw_obs_processing_ms: 0.15572829729894277\n",
            "  time_since_restore: 185.73877787590027\n",
            "  time_this_iter_s: 19.30983543395996\n",
            "  time_total_s: 185.73877787590027\n",
            "  timers:\n",
            "    learn_throughput: 502.602\n",
            "    learn_time_ms: 7958.588\n",
            "    sample_throughput: 377.105\n",
            "    sample_time_ms: 10607.127\n",
            "    update_time_ms: 1.709\n",
            "  timestamp: 1619351299\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 40000\n",
            "  training_iteration: 10\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:48:19,297\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=40065, mean_mean=15.036460658358727, mean_std=9.862012278376255), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.9/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         185.739</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">-0.508118</td><td style=\"text-align: right;\">            -0.49607</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           620.016</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:48:21,892\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=-18.203, max=55.008, mean=13.799)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:48:21,892\tINFO sampler.py:589 -- Info return from env: {0: {'agent0': {'net_worth': 53118.12, 'step': 1245}}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:48:21,892\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=-18.203, max=55.008, mean=13.799)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:48:21,892\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=-2.026, max=1.08, mean=-0.075)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:48:21,893\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': { 'net_worth': 53118.12,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                             'step': 1245},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=-2.026, max=1.08, mean=-0.075),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': -0.002999984298466263,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:48:21,895\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-2.389, max=2.39, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-0.008, max=-0.008, mean=-0.008),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.992, max=0.992, mean=0.992),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.691, max=-0.691, mean=-0.691)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:48:22,132\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-4.06, max=4.059, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-3.81, max=-0.0, mean=-0.341),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((200,), dtype=float32, min=0.022, max=1.0, mean=0.785),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.62),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-1.957, max=1.471, mean=-0.043),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=1650978243.0, max=1650978243.0, mean=1650978243.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 1120, 'net_worth': 56171.95}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-2.952, max=8.755, mean=0.286),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((200, 25, 3), dtype=float32, min=-2.952, max=8.755, mean=0.29),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.24),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=268.0, max=268.0, mean=268.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.24),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-1.173, max=1.135, mean=-0.197)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:48:22,134\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-4.06, max=4.059, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-3.81, max=-0.0, mean=-0.341),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.022, max=1.0, mean=0.785),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.62),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-1.957, max=1.471, mean=-0.043),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=1650978243.0, max=1650978243.0, mean=1650978243.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 1120, 'net_worth': 56171.95}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-2.952, max=8.755, mean=0.286),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-2.952, max=8.755, mean=0.29),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.24),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=268.0, max=268.0, mean=268.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.24),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-1.173, max=1.135, mean=-0.197)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:48:22,138\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:48:32,612\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-4.663, max=4.65, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((128,), dtype=float32, min=-1.631, max=-0.0, mean=-0.24),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((128,), dtype=float32, min=0.196, max=1.0, mean=0.832),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.477),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((128,), dtype=float32, min=-3.19, max=2.785, mean=0.081),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((128,), dtype=int64, min=1419228155.0, max=1841275345.0, mean=1600459098.672),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((128,), dtype=object, head={'step': 1651, 'net_worth': 40237.511117701004}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((128, 25, 3), dtype=float32, min=-10.286, max=9.719, mean=0.078),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((128, 25, 3), dtype=float32, min=-10.283, max=9.716, mean=0.075),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.094),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((128,), dtype=int64, min=264.0, max=285.0, mean=275.227),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.094),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((128,), dtype=float32, min=-1.245, max=1.142, mean=-0.137)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:48:32,621\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 0.675,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 7.000000000000001e-05,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.28308072686195374,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': 0.007141883485019207,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': -0.12080886960029602,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': 0.014698092825710773,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.598, max=0.598, mean=0.598),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.26703396439552307},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:48:32,825\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 44000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-48-38\n",
            "  done: false\n",
            "  episode_len_mean: 653.8333333333334\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.491159662427611\n",
            "  episode_reward_mean: -0.5076275885392494\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 2\n",
            "  episodes_total: 66\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.675\n",
            "          cur_lr: 7.000000000000001e-05\n",
            "          entropy: 0.28479395154863596\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.011175914740306325\n",
            "          policy_loss: -0.05573280592216179\n",
            "          total_loss: 0.062191157980123535\n",
            "          vf_explained_var: 0.6566793322563171\n",
            "          vf_loss: 0.2264563273638487\n",
            "    num_agent_steps_sampled: 44000\n",
            "    num_steps_sampled: 44000\n",
            "    num_steps_trained: 44000\n",
            "  iterations_since_restore: 11\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 29.15555555555555\n",
            "    ram_util_percent: 15.399999999999999\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.048652886226909275\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.9717622803758809\n",
            "    mean_inference_ms: 1.292825244848181\n",
            "    mean_raw_obs_processing_ms: 0.15582484342031092\n",
            "  time_since_restore: 204.9957389831543\n",
            "  time_this_iter_s: 19.25696110725403\n",
            "  time_total_s: 204.9957389831543\n",
            "  timers:\n",
            "    learn_throughput: 502.374\n",
            "    learn_time_ms: 7962.193\n",
            "    sample_throughput: 371.319\n",
            "    sample_time_ms: 10772.408\n",
            "    update_time_ms: 1.694\n",
            "  timestamp: 1619351318\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 44000\n",
            "  training_iteration: 11\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:48:38,589\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=44067, mean_mean=15.153745889912717, mean_std=9.858873034979778), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.9/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         204.996</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">-0.507628</td><td style=\"text-align: right;\">            -0.49116</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           653.833</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 48000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-48-58\n",
            "  done: false\n",
            "  episode_len_mean: 690.8970588235294\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.48980370056032796\n",
            "  episode_reward_mean: -0.5071361799190084\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 2\n",
            "  episodes_total: 68\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.675\n",
            "          cur_lr: 6.6e-05\n",
            "          entropy: 0.27343291230499744\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.010093902790686116\n",
            "          policy_loss: -0.06007760172360577\n",
            "          total_loss: 0.06213174725417048\n",
            "          vf_explained_var: 0.6412526369094849\n",
            "          vf_loss: 0.23626058688387275\n",
            "    num_agent_steps_sampled: 48000\n",
            "    num_steps_sampled: 48000\n",
            "    num_steps_trained: 48000\n",
            "  iterations_since_restore: 12\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.99310344827586\n",
            "    ram_util_percent: 15.399999999999997\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.048665758394034433\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.9774249827583854\n",
            "    mean_inference_ms: 1.2928517242298727\n",
            "    mean_raw_obs_processing_ms: 0.15591792140175098\n",
            "  time_since_restore: 224.7678554058075\n",
            "  time_this_iter_s: 19.7721164226532\n",
            "  time_total_s: 224.7678554058075\n",
            "  timers:\n",
            "    learn_throughput: 500.886\n",
            "    learn_time_ms: 7985.856\n",
            "    sample_throughput: 364.124\n",
            "    sample_time_ms: 10985.27\n",
            "    update_time_ms: 1.718\n",
            "  timestamp: 1619351338\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 48000\n",
            "  training_iteration: 12\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:48:58,398\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=48069, mean_mean=15.25049018963728, mean_std=9.866079937224828), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.9/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         224.768</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">-0.507136</td><td style=\"text-align: right;\">           -0.489804</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           690.897</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 52000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-49-17\n",
            "  done: false\n",
            "  episode_len_mean: 727.4285714285714\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.48980370056032796\n",
            "  episode_reward_mean: -0.5066849166022459\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 2\n",
            "  episodes_total: 70\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.675\n",
            "          cur_lr: 6.2e-05\n",
            "          entropy: 0.2575107654556632\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.010274527463479899\n",
            "          policy_loss: -0.0607601098890882\n",
            "          total_loss: 0.0471873406204395\n",
            "          vf_explained_var: 0.6802568435668945\n",
            "          vf_loss: 0.20717450324445963\n",
            "    num_agent_steps_sampled: 52000\n",
            "    num_steps_sampled: 52000\n",
            "    num_steps_trained: 52000\n",
            "  iterations_since_restore: 13\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.770370370370372\n",
            "    ram_util_percent: 15.399999999999999\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04867693380846436\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.98329645654137\n",
            "    mean_inference_ms: 1.292813539104813\n",
            "    mean_raw_obs_processing_ms: 0.15600817519900398\n",
            "  time_since_restore: 244.1956160068512\n",
            "  time_this_iter_s: 19.4277606010437\n",
            "  time_total_s: 244.1956160068512\n",
            "  timers:\n",
            "    learn_throughput: 501.402\n",
            "    learn_time_ms: 7977.629\n",
            "    sample_throughput: 358.544\n",
            "    sample_time_ms: 11156.247\n",
            "    update_time_ms: 1.721\n",
            "  timestamp: 1619351357\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 52000\n",
            "  training_iteration: 13\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:49:17,863\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=52071, mean_mean=15.323236879121144, mean_std=9.865142586414786), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.9/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         244.196</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">-0.506685</td><td style=\"text-align: right;\">           -0.489804</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           727.429</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:49:22,140\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=-20.838, max=61.71, mean=15.379)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:49:22,141\tINFO sampler.py:589 -- Info return from env: {0: {'agent0': {'net_worth': 67217.977599642, 'step': 429}}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:49:22,141\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=-20.838, max=61.71, mean=15.379)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:49:22,141\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=-4.12, max=1.568, mean=0.033)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:49:22,142\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': { 'net_worth': 67217.977599642,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                             'step': 429},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=-4.12, max=1.568, mean=0.033),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': 1,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': -1.3347036290634051e-05,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:49:22,143\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-1.866, max=1.868, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-0.024, max=-0.024, mean=-0.024),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.977, max=0.977, mean=0.977),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.236, max=-0.236, mean=-0.236)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:49:22,624\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-4.931, max=4.925, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-2.608, max=-0.0, mean=-0.228),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((200,), dtype=float32, min=0.074, max=1.0, mean=0.845),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.545),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-1.721, max=1.648, mean=-0.018),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=1518603643.0, max=1518603643.0, mean=1518603643.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 411, 'net_worth': 66894.668014304}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.144, max=3.531, mean=0.152),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.144, max=3.531, mean=0.153),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.13),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=338.0, max=338.0, mean=338.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.13),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-1.186, max=1.157, mean=-0.112)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:49:22,627\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-4.931, max=4.925, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-2.608, max=-0.0, mean=-0.228),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.074, max=1.0, mean=0.845),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.545),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-1.721, max=1.648, mean=-0.018),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=1518603643.0, max=1518603643.0, mean=1518603643.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 411, 'net_worth': 66894.668014304}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.144, max=3.531, mean=0.152),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.144, max=3.531, mean=0.153),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.13),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=338.0, max=338.0, mean=338.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.13),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-1.186, max=1.157, mean=-0.112)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:49:22,631\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:49:32,829\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-4.139, max=4.14, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((128,), dtype=float32, min=-4.734, max=-0.0, mean=-0.231),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((128,), dtype=float32, min=0.009, max=1.0, mean=0.868),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.43),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((128,), dtype=float32, min=-3.375, max=3.159, mean=0.01),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((128,), dtype=int64, min=1518603643.0, max=1808112915.0, mean=1624291347.344),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((128,), dtype=object, head={'step': 1729, 'net_worth': 50591.355367284}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((128, 25, 3), dtype=float32, min=-10.17, max=11.005, mean=0.034),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((128, 25, 3), dtype=float32, min=-10.167, max=10.996, mean=0.037),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.148),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((128,), dtype=int64, min=330.0, max=351.0, mean=340.07),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.148),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((128,), dtype=float32, min=-1.226, max=1.213, mean=-0.164)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:49:32,838\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 0.675,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 5.8e-05,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.21580161154270172,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': 0.005888457875698805,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': -0.0454656258225441,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': 0.05018968880176544,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.685, max=0.685, mean=0.685),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.187677264213562},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:49:32,861\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 56000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-49-37\n",
            "  done: false\n",
            "  episode_len_mean: 763.7083333333334\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.48980370056032796\n",
            "  episode_reward_mean: -0.5063531647610827\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 2\n",
            "  episodes_total: 72\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.675\n",
            "          cur_lr: 5.8e-05\n",
            "          entropy: 0.24691766779869795\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.010284604708431289\n",
            "          policy_loss: -0.048661945504136384\n",
            "          total_loss: 0.04410131709300913\n",
            "          vf_explained_var: 0.7168476581573486\n",
            "          vf_loss: 0.17658066446892917\n",
            "    num_agent_steps_sampled: 56000\n",
            "    num_steps_sampled: 56000\n",
            "    num_steps_trained: 56000\n",
            "  iterations_since_restore: 14\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 29.106896551724137\n",
            "    ram_util_percent: 15.399999999999997\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.048687522760127645\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.9892866503325553\n",
            "    mean_inference_ms: 1.2929366115863556\n",
            "    mean_raw_obs_processing_ms: 0.15609703073976064\n",
            "  time_since_restore: 264.0635850429535\n",
            "  time_this_iter_s: 19.867969036102295\n",
            "  time_total_s: 264.0635850429535\n",
            "  timers:\n",
            "    learn_throughput: 502.248\n",
            "    learn_time_ms: 7964.199\n",
            "    sample_throughput: 353.574\n",
            "    sample_time_ms: 11313.034\n",
            "    update_time_ms: 1.721\n",
            "  timestamp: 1619351377\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 56000\n",
            "  training_iteration: 14\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:49:37,768\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=56073, mean_mean=15.38136026334643, mean_std=9.875824396157123), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.9/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         264.064</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">-0.506353</td><td style=\"text-align: right;\">           -0.489804</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           763.708</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 60000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-49-57\n",
            "  done: false\n",
            "  episode_len_mean: 801.5675675675676\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.48980370056032796\n",
            "  episode_reward_mean: -0.5059484307584905\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 2\n",
            "  episodes_total: 74\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.675\n",
            "          cur_lr: 5.4000000000000005e-05\n",
            "          entropy: 0.24370845686644316\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.008428699526120909\n",
            "          policy_loss: -0.052640860609244555\n",
            "          total_loss: 0.04890955751761794\n",
            "          vf_explained_var: 0.6760878562927246\n",
            "          vf_loss: 0.19659625412896276\n",
            "    num_agent_steps_sampled: 60000\n",
            "    num_steps_sampled: 60000\n",
            "    num_steps_trained: 60000\n",
            "  iterations_since_restore: 15\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 30.0\n",
            "    ram_util_percent: 15.399999999999997\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.048696352745956316\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.9953798955667776\n",
            "    mean_inference_ms: 1.293193299374871\n",
            "    mean_raw_obs_processing_ms: 0.15618208002290765\n",
            "  time_since_restore: 284.1200330257416\n",
            "  time_this_iter_s: 20.056447982788086\n",
            "  time_total_s: 284.1200330257416\n",
            "  timers:\n",
            "    learn_throughput: 502.915\n",
            "    learn_time_ms: 7953.637\n",
            "    sample_throughput: 348.53\n",
            "    sample_time_ms: 11476.766\n",
            "    update_time_ms: 1.717\n",
            "  timestamp: 1619351397\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 60000\n",
            "  training_iteration: 15\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:49:57,861\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=60075, mean_mean=15.430285060063925, mean_std=9.885364774569666), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.9/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">          284.12</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">-0.505948</td><td style=\"text-align: right;\">           -0.489804</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           801.568</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 64000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-50-17\n",
            "  done: false\n",
            "  episode_len_mean: 836.8026315789474\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4893788831150232\n",
            "  episode_reward_mean: -0.5055460345669668\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 2\n",
            "  episodes_total: 76\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.675\n",
            "          cur_lr: 5.0e-05\n",
            "          entropy: 0.22923704935237765\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.008586400974309072\n",
            "          policy_loss: -0.05441197889740579\n",
            "          total_loss: 0.030027333239559084\n",
            "          vf_explained_var: 0.7349461913108826\n",
            "          vf_loss: 0.16187172709032893\n",
            "    num_agent_steps_sampled: 64000\n",
            "    num_steps_sampled: 64000\n",
            "    num_steps_trained: 64000\n",
            "  iterations_since_restore: 16\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.889285714285712\n",
            "    ram_util_percent: 15.435714285714285\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.048704200643171575\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.0015654696351866\n",
            "    mean_inference_ms: 1.2933828464322474\n",
            "    mean_raw_obs_processing_ms: 0.1562650088903695\n",
            "  time_since_restore: 303.76632380485535\n",
            "  time_this_iter_s: 19.64629077911377\n",
            "  time_total_s: 303.76632380485535\n",
            "  timers:\n",
            "    learn_throughput: 505.161\n",
            "    learn_time_ms: 7918.272\n",
            "    sample_throughput: 345.388\n",
            "    sample_time_ms: 11581.184\n",
            "    update_time_ms: 1.741\n",
            "  timestamp: 1619351417\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 64000\n",
            "  training_iteration: 16\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.9/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         303.766</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">-0.505546</td><td style=\"text-align: right;\">           -0.489379</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           836.803</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:50:17,547\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=64077, mean_mean=15.472486381042557, mean_std=9.896305007466914), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:50:22,633\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=-0.01, max=59.505, mean=22.926)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:50:22,633\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=-1.454, max=4.162, mean=0.521)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:50:22,634\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': { 'net_worth': 43860.18,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                             'step': 2084},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=-1.454, max=4.162, mean=0.521),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:50:22,636\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-1.572, max=1.561, mean=-0.006),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-0.043, max=-0.043, mean=-0.043),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.958, max=0.958, mean=0.958),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.048, max=0.048, mean=0.048)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:50:22,639\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=-0.01, max=59.505, mean=22.968)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:50:22,639\tINFO sampler.py:589 -- Info return from env: {0: {'agent0': {'net_worth': 43860.18, 'step': 2085}}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:50:23,076\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-6.184, max=6.185, mean=-0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-2.772, max=-0.0, mean=-0.279),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((200,), dtype=float32, min=0.063, max=1.0, mean=0.812),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.37),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-2.094, max=2.214, mean=0.044),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=605506647.0, max=605506647.0, mean=605506647.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 2004, 'net_worth': 45227.39}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-5.511, max=8.306, mean=-0.091),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((200, 25, 3), dtype=float32, min=-5.511, max=8.306, mean=-0.09),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.155),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=404.0, max=404.0, mean=404.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.155),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-1.249, max=1.094, mean=-0.199)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:50:23,078\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-6.184, max=6.185, mean=-0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-2.772, max=-0.0, mean=-0.279),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.063, max=1.0, mean=0.812),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.37),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-2.094, max=2.214, mean=0.044),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=605506647.0, max=605506647.0, mean=605506647.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 2004, 'net_worth': 45227.39}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-5.511, max=8.306, mean=-0.091),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-5.511, max=8.306, mean=-0.09),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.155),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=404.0, max=404.0, mean=404.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.155),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-1.249, max=1.094, mean=-0.199)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:50:23,082\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:50:32,864\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-5.547, max=5.55, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((128,), dtype=float32, min=-4.431, max=-0.0, mean=-0.321),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((128,), dtype=float32, min=0.012, max=1.0, mean=0.843),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.438),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((128,), dtype=float32, min=-3.453, max=3.348, mean=-0.031),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((128,), dtype=int64, min=605506647.0, max=1507081018.0, mean=1098555131.141),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((128,), dtype=object, head={'step': 2244, 'net_worth': 39645.8}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((128, 25, 3), dtype=float32, min=-10.159, max=9.633, mean=0.065),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((128, 25, 3), dtype=float32, min=-10.146, max=9.617, mean=0.057),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.008),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((128,), dtype=int64, min=396.0, max=416.0, mean=406.086),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.008),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((128,), dtype=float32, min=-1.141, max=1.18, mean=0.001)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:50:32,873\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 0.675,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 4.6e-05,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.22276781499385834,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': 0.004977403674274683,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': -0.040893618017435074,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': 0.09545537829399109,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.539, max=0.539, mean=0.539),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.27043381333351135},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:50:32,899\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 68000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-50-37\n",
            "  done: false\n",
            "  episode_len_mean: 855.7272727272727\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.48637465170688965\n",
            "  episode_reward_mean: -0.5052970555687839\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 77\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.675\n",
            "          cur_lr: 4.6e-05\n",
            "          entropy: 0.22972443234175444\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.008105018670903519\n",
            "          policy_loss: -0.05165012722136453\n",
            "          total_loss: 0.03945668725646101\n",
            "          vf_explained_var: 0.7111524343490601\n",
            "          vf_loss: 0.17586634447798133\n",
            "    num_agent_steps_sampled: 68000\n",
            "    num_steps_sampled: 68000\n",
            "    num_steps_trained: 68000\n",
            "  iterations_since_restore: 17\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 29.089655172413792\n",
            "    ram_util_percent: 15.482758620689655\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04870752855134906\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.0047501520812274\n",
            "    mean_inference_ms: 1.2935223953647026\n",
            "    mean_raw_obs_processing_ms: 0.1563040938838773\n",
            "  time_since_restore: 324.12402296066284\n",
            "  time_this_iter_s: 20.357699155807495\n",
            "  time_total_s: 324.12402296066284\n",
            "  timers:\n",
            "    learn_throughput: 506.143\n",
            "    learn_time_ms: 7902.897\n",
            "    sample_throughput: 341.378\n",
            "    sample_time_ms: 11717.201\n",
            "    update_time_ms: 1.737\n",
            "  timestamp: 1619351437\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 68000\n",
            "  training_iteration: 17\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.9/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         324.124</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">-0.505297</td><td style=\"text-align: right;\">           -0.486375</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           855.727</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:50:37,942\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=68078, mean_mean=15.520138111593363, mean_std=9.897725139196615), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 72000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-50-57\n",
            "  done: false\n",
            "  episode_len_mean: 896.0632911392405\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.48637465170688965\n",
            "  episode_reward_mean: -0.5049067017049448\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 2\n",
            "  episodes_total: 79\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.675\n",
            "          cur_lr: 4.2e-05\n",
            "          entropy: 0.22794007789343596\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.006801243172958493\n",
            "          policy_loss: -0.052486348286038265\n",
            "          total_loss: 0.04124834017420653\n",
            "          vf_explained_var: 0.6982874870300293\n",
            "          vf_loss: 0.1828465056605637\n",
            "    num_agent_steps_sampled: 72000\n",
            "    num_steps_sampled: 72000\n",
            "    num_steps_trained: 72000\n",
            "  iterations_since_restore: 18\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.985714285714288\n",
            "    ram_util_percent: 15.5\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04871282873181275\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.0111956560218323\n",
            "    mean_inference_ms: 1.2937305035814226\n",
            "    mean_raw_obs_processing_ms: 0.15638060729590791\n",
            "  time_since_restore: 343.8426399230957\n",
            "  time_this_iter_s: 19.71861696243286\n",
            "  time_total_s: 343.8426399230957\n",
            "  timers:\n",
            "    learn_throughput: 508.193\n",
            "    learn_time_ms: 7871.03\n",
            "    sample_throughput: 339.625\n",
            "    sample_time_ms: 11777.709\n",
            "    update_time_ms: 1.712\n",
            "  timestamp: 1619351457\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 72000\n",
            "  training_iteration: 18\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:50:57,698\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=72080, mean_mean=15.559235490266458, mean_std=9.889338019423196), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.9/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         343.843</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">-0.504907</td><td style=\"text-align: right;\">           -0.486375</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           896.063</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 76000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-51-17\n",
            "  done: false\n",
            "  episode_len_mean: 936.1481481481482\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4831785552301767\n",
            "  episode_reward_mean: -0.5043902571741238\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 2\n",
            "  episodes_total: 81\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.675\n",
            "          cur_lr: 3.8e-05\n",
            "          entropy: 0.22606840124353766\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.006617269835260231\n",
            "          policy_loss: -0.04914488484791946\n",
            "          total_loss: 0.04538260347908363\n",
            "          vf_explained_var: 0.6900741457939148\n",
            "          vf_loss: 0.18464302411302924\n",
            "    num_agent_steps_sampled: 76000\n",
            "    num_steps_sampled: 76000\n",
            "    num_steps_trained: 76000\n",
            "  iterations_since_restore: 19\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.77931034482759\n",
            "    ram_util_percent: 15.5\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04871733332289663\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.0178205569465344\n",
            "    mean_inference_ms: 1.2938817636729245\n",
            "    mean_raw_obs_processing_ms: 0.15645836185126702\n",
            "  time_since_restore: 364.08159375190735\n",
            "  time_this_iter_s: 20.238953828811646\n",
            "  time_total_s: 364.08159375190735\n",
            "  timers:\n",
            "    learn_throughput: 511.303\n",
            "    learn_time_ms: 7823.144\n",
            "    sample_throughput: 335.175\n",
            "    sample_time_ms: 11934.054\n",
            "    update_time_ms: 1.713\n",
            "  timestamp: 1619351477\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 76000\n",
            "  training_iteration: 19\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.0/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         364.082</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">-0.50439</td><td style=\"text-align: right;\">           -0.483179</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           936.148</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:51:17,973\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=76082, mean_mean=15.606508645535422, mean_std=9.888560739942125), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:51:23,085\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=-0.318, max=54.229, mean=18.827)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:51:23,085\tINFO sampler.py:589 -- Info return from env: {0: {'agent0': {'net_worth': 52388.84, 'step': 1948}}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:51:23,085\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=-0.318, max=54.229, mean=18.827)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:51:23,085\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=-1.9, max=1.686, mean=0.223)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:51:23,086\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': { 'net_worth': 52388.84,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                             'step': 1948},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=-1.9, max=1.686, mean=0.223),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:51:23,088\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-2.594, max=2.596, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-0.006, max=-0.006, mean=-0.006),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.994, max=0.994, mean=0.994),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.342, max=-0.342, mean=-0.342)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:51:23,173\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-6.01, max=6.015, mean=-0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-4.544, max=-0.0, mean=-0.246),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((200,), dtype=float32, min=0.011, max=1.0, mean=0.855),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.45),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-1.513, max=1.49, mean=-0.006),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=123639414.0, max=123639414.0, mean=123639414.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 1773, 'net_worth': 59152.0}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.208, max=6.12, mean=-0.121),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.207, max=6.12, mean=-0.115),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.185),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=469.0, max=469.0, mean=469.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.185),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-1.187, max=1.037, mean=-0.179)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:51:23,175\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-6.01, max=6.015, mean=-0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-4.544, max=-0.0, mean=-0.246),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.011, max=1.0, mean=0.855),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.45),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-1.513, max=1.49, mean=-0.006),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=123639414.0, max=123639414.0, mean=123639414.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 1773, 'net_worth': 59152.0}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.208, max=6.12, mean=-0.121),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.207, max=6.12, mean=-0.115),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.185),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=469.0, max=469.0, mean=469.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.185),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-1.187, max=1.037, mean=-0.179)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:51:23,179\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:51:32,907\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-6.22, max=6.242, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((128,), dtype=float32, min=-2.929, max=-0.0, mean=-0.225),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((128,), dtype=float32, min=0.053, max=1.0, mean=0.867),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.484),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((128,), dtype=float32, min=-3.514, max=2.128, mean=-0.038),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((128,), dtype=int64, min=123639414.0, max=615955014.0, mean=331335057.75),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((128,), dtype=object, head={'step': 2025, 'net_worth': 49036.83}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((128, 25, 3), dtype=float32, min=-10.059, max=8.54, mean=0.103),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((128, 25, 3), dtype=float32, min=-10.054, max=8.538, mean=0.102),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.086),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((128,), dtype=int64, min=461.0, max=481.0, mean=471.102),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.086),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((128,), dtype=float32, min=-1.316, max=1.021, mean=-0.076)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:51:32,916\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 0.675,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 3.4e-05,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.21955645084381104,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': 0.004833429120481014,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': -0.002467891201376915,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': 0.10855470597743988,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.649, max=0.649, mean=0.649),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.21991121768951416},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:51:32,932\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 80000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-51-38\n",
            "  done: false\n",
            "  episode_len_mean: 957.2926829268292\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4789157411006265\n",
            "  episode_reward_mean: -0.5040795923439592\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 82\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.675\n",
            "          cur_lr: 3.4e-05\n",
            "          entropy: 0.21067296527326107\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.005386760320106987\n",
            "          policy_loss: -0.05240550794405863\n",
            "          total_loss: 0.04074493594816886\n",
            "          vf_explained_var: 0.6943425536155701\n",
            "          vf_loss: 0.18324222741648555\n",
            "    num_agent_steps_sampled: 80000\n",
            "    num_steps_sampled: 80000\n",
            "    num_steps_trained: 80000\n",
            "  iterations_since_restore: 20\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 29.103448275862068\n",
            "    ram_util_percent: 15.5\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.048719189409509825\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.021128415259796\n",
            "    mean_inference_ms: 1.2939980990919242\n",
            "    mean_raw_obs_processing_ms: 0.15649533302834653\n",
            "  time_since_restore: 384.3442494869232\n",
            "  time_this_iter_s: 20.26265573501587\n",
            "  time_total_s: 384.3442494869232\n",
            "  timers:\n",
            "    learn_throughput: 510.73\n",
            "    learn_time_ms: 7831.93\n",
            "    sample_throughput: 332.763\n",
            "    sample_time_ms: 12020.559\n",
            "    update_time_ms: 1.715\n",
            "  timestamp: 1619351498\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 80000\n",
            "  training_iteration: 20\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:51:38,273\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=80083, mean_mean=15.64236168237155, mean_std=9.868245733979592), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.0/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         384.344</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">-0.50408</td><td style=\"text-align: right;\">           -0.478916</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           957.293</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 84000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-51-59\n",
            "  done: false\n",
            "  episode_len_mean: 999.8452380952381\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4789157411006265\n",
            "  episode_reward_mean: -0.5036393465022979\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 2\n",
            "  episodes_total: 84\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.675\n",
            "          cur_lr: 2.9999999999999997e-05\n",
            "          entropy: 0.22383286152034998\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0059769386643893085\n",
            "          policy_loss: -0.05113799209357239\n",
            "          total_loss: 0.05509841677849181\n",
            "          vf_explained_var: 0.6571013927459717\n",
            "          vf_loss: 0.20888060564175248\n",
            "    num_agent_steps_sampled: 84000\n",
            "    num_steps_sampled: 84000\n",
            "    num_steps_trained: 84000\n",
            "  iterations_since_restore: 21\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.85666666666667\n",
            "    ram_util_percent: 15.5\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.048722354500885384\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.028046161670408\n",
            "    mean_inference_ms: 1.2941814913941005\n",
            "    mean_raw_obs_processing_ms: 0.15657128638390774\n",
            "  time_since_restore: 405.19246435165405\n",
            "  time_this_iter_s: 20.848214864730835\n",
            "  time_total_s: 405.19246435165405\n",
            "  timers:\n",
            "    learn_throughput: 511.426\n",
            "    learn_time_ms: 7821.262\n",
            "    sample_throughput: 328.128\n",
            "    sample_time_ms: 12190.383\n",
            "    update_time_ms: 1.704\n",
            "  timestamp: 1619351519\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 84000\n",
            "  training_iteration: 21\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:51:59,159\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=84085, mean_mean=15.687081167027369, mean_std=9.871775713537437), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.0/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         405.192</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">-0.503639</td><td style=\"text-align: right;\">           -0.478916</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           999.845</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 88000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-52-18\n",
            "  done: false\n",
            "  episode_len_mean: 1023.0705882352942\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4789157411006265\n",
            "  episode_reward_mean: -0.5034316319353712\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 85\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.675\n",
            "          cur_lr: 2.600000000000001e-05\n",
            "          entropy: 0.203949058894068\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.004512801198870875\n",
            "          policy_loss: -0.0390771494130604\n",
            "          total_loss: 0.054165510227903724\n",
            "          vf_explained_var: 0.6816138625144958\n",
            "          vf_loss: 0.18447203189134598\n",
            "    num_agent_steps_sampled: 88000\n",
            "    num_steps_sampled: 88000\n",
            "    num_steps_trained: 88000\n",
            "  iterations_since_restore: 22\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.842857142857135\n",
            "    ram_util_percent: 15.596428571428573\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.048723638517054794\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.0314839894178913\n",
            "    mean_inference_ms: 1.2942429674831093\n",
            "    mean_raw_obs_processing_ms: 0.1566069139666342\n",
            "  time_since_restore: 424.9291422367096\n",
            "  time_this_iter_s: 19.736677885055542\n",
            "  time_total_s: 424.9291422367096\n",
            "  timers:\n",
            "    learn_throughput: 512.258\n",
            "    learn_time_ms: 7808.56\n",
            "    sample_throughput: 327.883\n",
            "    sample_time_ms: 12199.466\n",
            "    update_time_ms: 1.699\n",
            "  timestamp: 1619351538\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 88000\n",
            "  training_iteration: 22\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:52:18,933\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=88086, mean_mean=15.709263882181233, mean_std=9.858908244851783), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.0/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         424.929</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">-0.503432</td><td style=\"text-align: right;\">           -0.478916</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           1023.07</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:52:23,182\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=-11.018, max=62.312, mean=18.097)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:52:23,183\tINFO sampler.py:589 -- Info return from env: {0: {'agent0': {'net_worth': 50745.59, 'step': 2338}}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:52:23,183\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=-11.018, max=62.312, mean=18.097)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:52:23,183\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=-1.033, max=3.965, mean=0.205)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:52:23,184\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': { 'net_worth': 50745.59,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                             'step': 2338},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=-1.033, max=3.965, mean=0.205),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:52:23,185\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-1.339, max=1.347, mean=0.004),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-0.066, max=-0.066, mean=-0.066),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.936, max=0.936, mean=0.936),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.024, max=-0.024, mean=-0.024)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:52:23,570\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-6.778, max=6.779, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-4.443, max=-0.0, mean=-0.306),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((200,), dtype=float32, min=0.012, max=1.0, mean=0.819),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.415),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-1.415, max=1.719, mean=-0.036),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=608090239.0, max=608090239.0, mean=608090239.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 2240, 'net_worth': 53706.9}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.72, max=8.396, mean=0.107),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.72, max=8.396, mean=0.105),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.195),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=531.0, max=531.0, mean=531.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.195),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-1.177, max=1.028, mean=-0.159)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:52:23,572\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-6.778, max=6.779, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-4.443, max=-0.0, mean=-0.306),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.012, max=1.0, mean=0.819),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.415),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-1.415, max=1.719, mean=-0.036),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=608090239.0, max=608090239.0, mean=608090239.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 2240, 'net_worth': 53706.9}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.72, max=8.396, mean=0.107),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.72, max=8.396, mean=0.105),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.195),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=531.0, max=531.0, mean=531.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.195),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-1.177, max=1.028, mean=-0.159)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:52:23,577\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:52:32,937\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-6.985, max=7.007, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((128,), dtype=float32, min=-3.523, max=-0.0, mean=-0.235),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((128,), dtype=float32, min=0.03, max=1.0, mean=0.867),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.477),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((128,), dtype=float32, min=-3.327, max=2.813, mean=-0.041),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((128,), dtype=int64, min=513388320.0, max=608090239.0, mean=559999420.758),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((128,), dtype=object, head={'step': 1754, 'net_worth': 64545.61996815901}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((128, 25, 3), dtype=float32, min=-10.125, max=10.877, mean=0.011),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((128, 25, 3), dtype=float32, min=-10.121, max=10.874, mean=0.008),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.156),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((128,), dtype=int64, min=525.0, max=545.0, mean=534.773),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.156),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((128,), dtype=float32, min=-1.055, max=1.057, mean=-0.146)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:52:32,946\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 0.3375,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 2.1999999999999993e-05,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.20585599541664124,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': 0.003460064996033907,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': 0.014873314648866653,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': 0.12795597314834595,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.64, max=0.64, mean=0.64),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.22794689238071442},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:52:33,076\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 92000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-52-39\n",
            "  done: false\n",
            "  episode_len_mean: 1044.4883720930231\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4789157411006265\n",
            "  episode_reward_mean: -0.5031732585697353\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 86\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.3375\n",
            "          cur_lr: 2.1999999999999993e-05\n",
            "          entropy: 0.2000189288519323\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.004564643953926861\n",
            "          policy_loss: -0.04493817620095797\n",
            "          total_loss: 0.04834129207301885\n",
            "          vf_explained_var: 0.6801133155822754\n",
            "          vf_loss: 0.18747818237170577\n",
            "    num_agent_steps_sampled: 92000\n",
            "    num_steps_sampled: 92000\n",
            "    num_steps_trained: 92000\n",
            "  iterations_since_restore: 23\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.896551724137932\n",
            "    ram_util_percent: 15.600000000000003\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04872442016293601\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.0350111400297506\n",
            "    mean_inference_ms: 1.2942804242453023\n",
            "    mean_raw_obs_processing_ms: 0.15664153839311376\n",
            "  time_since_restore: 445.2735786437988\n",
            "  time_this_iter_s: 20.344436407089233\n",
            "  time_total_s: 445.2735786437988\n",
            "  timers:\n",
            "    learn_throughput: 512.466\n",
            "    learn_time_ms: 7805.395\n",
            "    sample_throughput: 325.355\n",
            "    sample_time_ms: 12294.25\n",
            "    update_time_ms: 1.704\n",
            "  timestamp: 1619351559\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 92000\n",
            "  training_iteration: 23\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:52:39,314\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=92087, mean_mean=15.742594460543371, mean_std=9.859792247485808), (n=0, mean_mean=0.0, mean_std=0.0))}"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.0/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         445.274</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">-0.503173</td><td style=\"text-align: right;\">           -0.478916</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           1044.49</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 96000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-53-00\n",
            "  done: false\n",
            "  episode_len_mean: 1090.3977272727273\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4789157411006265\n",
            "  episode_reward_mean: -0.5027459085408374\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 2\n",
            "  episodes_total: 88\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.16875\n",
            "          cur_lr: 1.8000000000000004e-05\n",
            "          entropy: 0.21078061778098345\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.004915041427011602\n",
            "          policy_loss: -0.04384218840277754\n",
            "          total_loss: 0.07620420318562537\n",
            "          vf_explained_var: 0.5781955122947693\n",
            "          vf_loss: 0.24264957010746002\n",
            "    num_agent_steps_sampled: 96000\n",
            "    num_steps_sampled: 96000\n",
            "    num_steps_trained: 96000\n",
            "  iterations_since_restore: 24\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.75\n",
            "    ram_util_percent: 15.600000000000003\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04872503441840233\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.0423945109283972\n",
            "    mean_inference_ms: 1.2943071887991224\n",
            "    mean_raw_obs_processing_ms: 0.1567104829103803\n",
            "  time_since_restore: 466.62617230415344\n",
            "  time_this_iter_s: 21.352593660354614\n",
            "  time_total_s: 466.62617230415344\n",
            "  timers:\n",
            "    learn_throughput: 513.344\n",
            "    learn_time_ms: 7792.047\n",
            "    sample_throughput: 321.128\n",
            "    sample_time_ms: 12456.088\n",
            "    update_time_ms: 1.71\n",
            "  timestamp: 1619351580\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 96000\n",
            "  training_iteration: 24\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:53:00,704\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=96089, mean_mean=15.76925088980159, mean_std=9.84562321390897), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.0/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         466.626</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">-0.502746</td><td style=\"text-align: right;\">           -0.478916</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">            1090.4</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 100000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-53-20\n",
            "  done: false\n",
            "  episode_len_mean: 1114.8426966292134\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4789157411006265\n",
            "  episode_reward_mean: -0.5025211158677404\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 89\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.084375\n",
            "          cur_lr: 1.4000000000000001e-05\n",
            "          entropy: 0.19668928300961852\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0049870493458001874\n",
            "          policy_loss: -0.028869060857687145\n",
            "          total_loss: 0.0693097916955594\n",
            "          vf_explained_var: 0.6522059440612793\n",
            "          vf_loss: 0.19944992079399526\n",
            "    num_agent_steps_sampled: 100000\n",
            "    num_steps_sampled: 100000\n",
            "    num_steps_trained: 100000\n",
            "  iterations_since_restore: 25\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.751724137931042\n",
            "    ram_util_percent: 15.600000000000003\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04872483441551046\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.0460640354800672\n",
            "    mean_inference_ms: 1.2942973514351375\n",
            "    mean_raw_obs_processing_ms: 0.15674258447709175\n",
            "  time_since_restore: 486.5563762187958\n",
            "  time_this_iter_s: 19.930203914642334\n",
            "  time_total_s: 486.5563762187958\n",
            "  timers:\n",
            "    learn_throughput: 514.609\n",
            "    learn_time_ms: 7772.894\n",
            "    sample_throughput: 320.96\n",
            "    sample_time_ms: 12462.628\n",
            "    update_time_ms: 1.702\n",
            "  timestamp: 1619351600\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 100000\n",
            "  training_iteration: 25\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:53:20,671\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=100090, mean_mean=15.78967237129303, mean_std=9.826264010805598), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.0/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         486.556</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">-0.502521</td><td style=\"text-align: right;\">           -0.478916</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           1114.84</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:53:23,578\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=-37.284, max=58.116, mean=15.07)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:53:23,579\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=-2.281, max=1.874, mean=0.002)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:53:23,579\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': { 'net_worth': 76104.761774833,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                             'step': 1739},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=-2.281, max=1.874, mean=0.002),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': 1,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': 0.0006574541912702792,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:53:23,581\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-1.992, max=1.993, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-0.018, max=-0.018, mean=-0.018),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.982, max=0.982, mean=0.982),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.796, max=0.796, mean=0.796)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:53:23,583\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=-37.284, max=58.116, mean=15.498)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:53:23,584\tINFO sampler.py:589 -- Info return from env: { 0: { 'agent0': { 'net_worth': 76331.62506109799,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                    'step': 1740}}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:53:23,717\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-6.236, max=6.231, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-3.846, max=-0.0, mean=-0.2),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((200,), dtype=float32, min=0.021, max=1.0, mean=0.883),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.535),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-1.386, max=1.542, mean=0.03),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=250754523.0, max=250754523.0, mean=250754523.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 1580, 'net_worth': 76707.100108144}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-3.282, max=6.083, mean=0.223),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((200, 25, 3), dtype=float32, min=-3.282, max=6.079, mean=0.221),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.05),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=593.0, max=593.0, mean=593.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.05),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-1.156, max=1.107, mean=-0.08)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:53:23,719\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-6.236, max=6.231, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-3.846, max=-0.0, mean=-0.2),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.021, max=1.0, mean=0.883),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.535),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-1.386, max=1.542, mean=0.03),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=250754523.0, max=250754523.0, mean=250754523.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 1580, 'net_worth': 76707.100108144}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-3.282, max=6.083, mean=0.223),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-3.282, max=6.079, mean=0.221),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.05),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=593.0, max=593.0, mean=593.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.05),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-1.156, max=1.107, mean=-0.08)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:53:23,723\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:53:33,459\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:53:33,463\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-6.911, max=6.933, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((128,), dtype=float32, min=-2.895, max=-0.0, mean=-0.213),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((128,), dtype=float32, min=0.055, max=1.0, mean=0.868),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.469),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((128,), dtype=float32, min=-2.959, max=3.711, mean=-0.069),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((128,), dtype=int64, min=250754523.0, max=1661255042.0, mean=702555470.492),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((128,), dtype=object, head={'step': 1081, 'net_worth': 65242.04660212799}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((128, 25, 3), dtype=float32, min=-5.883, max=9.6, mean=0.023),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((128, 25, 3), dtype=float32, min=-5.882, max=9.644, mean=0.028),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.18),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((128,), dtype=int64, min=589.0, max=609.0, mean=598.492),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.18),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((128,), dtype=float32, min=-1.23, max=1.117, mean=-0.144)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:53:33,473\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 0.0421875,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 1e-05,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.22422349452972412,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': 3.819465188570348e-09,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': 0.06942944973707199,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': 0.2099706530570984,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.514, max=0.514, mean=0.514),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.28556686639785767},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 104000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-53-41\n",
            "  done: false\n",
            "  episode_len_mean: 1139.5666666666666\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4789157411006265\n",
            "  episode_reward_mean: -0.5022992643925235\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 90\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.0421875\n",
            "          cur_lr: 1.0e-05\n",
            "          entropy: 0.19231648254208267\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0036378462209540885\n",
            "          policy_loss: -0.027561685259570368\n",
            "          total_loss: 0.0830383735592477\n",
            "          vf_explained_var: 0.603783905506134\n",
            "          vf_loss: 0.22473950730636716\n",
            "    num_agent_steps_sampled: 104000\n",
            "    num_steps_sampled: 104000\n",
            "    num_steps_trained: 104000\n",
            "  iterations_since_restore: 26\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 29.093103448275862\n",
            "    ram_util_percent: 15.600000000000003\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.0487244785683223\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.0497928858008247\n",
            "    mean_inference_ms: 1.2942710742225843\n",
            "    mean_raw_obs_processing_ms: 0.15677385879649644\n",
            "  time_since_restore: 507.1705048084259\n",
            "  time_this_iter_s: 20.614128589630127\n",
            "  time_total_s: 507.1705048084259\n",
            "  timers:\n",
            "    learn_throughput: 513.913\n",
            "    learn_time_ms: 7783.413\n",
            "    sample_throughput: 318.753\n",
            "    sample_time_ms: 12548.896\n",
            "    update_time_ms: 1.689\n",
            "  timestamp: 1619351621\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 104000\n",
            "  training_iteration: 26\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:53:41,323\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=104091, mean_mean=15.814510267296962, mean_std=9.807192263581625), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.0/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         507.171</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\">-0.502299</td><td style=\"text-align: right;\">           -0.478916</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           1139.57</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 108000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-54-02\n",
            "  done: false\n",
            "  episode_len_mean: 1161.89010989011\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4789157411006265\n",
            "  episode_reward_mean: -0.5021293980572833\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 91\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.02109375\n",
            "          cur_lr: 9.960000000000001e-06\n",
            "          entropy: 0.19198721321299672\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0035468587375362404\n",
            "          policy_loss: -0.029882519593229517\n",
            "          total_loss: 0.07082318220636807\n",
            "          vf_explained_var: 0.6474192142486572\n",
            "          vf_loss: 0.20510151516646147\n",
            "    num_agent_steps_sampled: 108000\n",
            "    num_steps_sampled: 108000\n",
            "    num_steps_trained: 108000\n",
            "  iterations_since_restore: 27\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 29.05806451612903\n",
            "    ram_util_percent: 15.600000000000005\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04872413840764694\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.0535941442196919\n",
            "    mean_inference_ms: 1.294293271987941\n",
            "    mean_raw_obs_processing_ms: 0.15680544645138536\n",
            "  time_since_restore: 528.6626613140106\n",
            "  time_this_iter_s: 21.492156505584717\n",
            "  time_total_s: 528.6626613140106\n",
            "  timers:\n",
            "    learn_throughput: 514.407\n",
            "    learn_time_ms: 7775.946\n",
            "    sample_throughput: 315.712\n",
            "    sample_time_ms: 12669.763\n",
            "    update_time_ms: 1.692\n",
            "  timestamp: 1619351642\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 108000\n",
            "  training_iteration: 27\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:54:02,853\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=108092, mean_mean=15.84235375674951, mean_std=9.805418643187602), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.0/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         528.663</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">-0.502129</td><td style=\"text-align: right;\">           -0.478916</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           1161.89</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 112000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-54-24\n",
            "  done: false\n",
            "  episode_len_mean: 1185.8695652173913\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4789157411006265\n",
            "  episode_reward_mean: -0.5018983529513775\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 92\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.010546875\n",
            "          cur_lr: 9.920000000000002e-06\n",
            "          entropy: 0.1982078324072063\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.003183389806508785\n",
            "          policy_loss: -0.023755083879223093\n",
            "          total_loss: 0.09656226745573804\n",
            "          vf_explained_var: 0.5638187527656555\n",
            "          vf_loss: 0.24453170923516154\n",
            "    num_agent_steps_sampled: 112000\n",
            "    num_steps_sampled: 112000\n",
            "    num_steps_trained: 112000\n",
            "  iterations_since_restore: 28\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.90666666666667\n",
            "    ram_util_percent: 15.600000000000003\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04872359316863187\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.05749311760405\n",
            "    mean_inference_ms: 1.2942973096153905\n",
            "    mean_raw_obs_processing_ms: 0.15683653524033425\n",
            "  time_since_restore: 549.8479495048523\n",
            "  time_this_iter_s: 21.185288190841675\n",
            "  time_total_s: 549.8479495048523\n",
            "  timers:\n",
            "    learn_throughput: 514.664\n",
            "    learn_time_ms: 7772.061\n",
            "    sample_throughput: 312.004\n",
            "    sample_time_ms: 12820.334\n",
            "    update_time_ms: 1.713\n",
            "  timestamp: 1619351664\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 112000\n",
            "  training_iteration: 28\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.0/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         549.848</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">-0.501898</td><td style=\"text-align: right;\">           -0.478916</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           1185.87</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:54:24,078\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=112093, mean_mean=15.866546127720081, mean_std=9.785806145691238), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:54:24,110\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': { 'net_worth': 49441.29,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                             'step': 2900},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=-0.681, max=1.915, mean=0.249),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:54:24,113\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-1.076, max=1.084, mean=0.004),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-0.109, max=-0.109, mean=-0.109),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.897, max=0.897, mean=0.897),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.146, max=0.146, mean=0.146)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:54:24,120\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=-2.151, max=65.54, mean=19.044)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:54:24,120\tINFO sampler.py:589 -- Info return from env: {0: {'agent0': {'net_worth': 49441.29, 'step': 2901}}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:54:24,121\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=-2.151, max=65.54, mean=19.044)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:54:24,121\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=-0.681, max=1.915, mean=0.277)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:54:24,954\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-5.797, max=5.815, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-1.949, max=-0.0, mean=-0.164),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((200,), dtype=float32, min=0.142, max=1.0, mean=0.881),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.445),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-1.298, max=1.602, mean=-0.019),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=1338595878.0, max=1338595878.0, mean=1338595878.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 2901, 'net_worth': 49441.29}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.29, max=6.451, mean=0.127),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.29, max=6.451, mean=0.125),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.185),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=652.0, max=652.0, mean=652.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.185),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-1.423, max=0.865, mean=-0.166)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:54:24,956\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-5.797, max=5.815, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-1.949, max=-0.0, mean=-0.164),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.142, max=1.0, mean=0.881),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.445),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-1.298, max=1.602, mean=-0.019),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=1338595878.0, max=1338595878.0, mean=1338595878.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 2901, 'net_worth': 49441.29}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.29, max=6.451, mean=0.127),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.29, max=6.451, mean=0.125),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.185),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=652.0, max=652.0, mean=652.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.185),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-1.423, max=0.865, mean=-0.166)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:54:24,961\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:54:37,769\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:54:37,772\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-8.226, max=8.232, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((128,), dtype=float32, min=-2.754, max=0.0, mean=-0.22),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((128,), dtype=float32, min=0.064, max=1.0, mean=0.861),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.336),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((128,), dtype=float32, min=-2.625, max=3.018, mean=0.157),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((128,), dtype=int64, min=1338595878.0, max=1977859651.0, mean=1892957431.148),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((128,), dtype=object, head={'step': 1722, 'net_worth': 78931.03}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((128, 25, 3), dtype=float32, min=-10.131, max=9.623, mean=0.04),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((128, 25, 3), dtype=float32, min=-10.129, max=9.62, mean=0.037),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.055),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((128,), dtype=int64, min=652.0, max=672.0, mean=662.18),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.055),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((128,), dtype=float32, min=-1.106, max=1.187, mean=-0.151)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:54:37,783\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 0.0052734375,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 9.88e-06,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.22462409734725952,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': 1.376321723256524e-09,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': -0.15694363415241241,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': -0.026534486562013626,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.493, max=0.493, mean=0.493),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.26531076431274414},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 116000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-54-45\n",
            "  done: false\n",
            "  episode_len_mean: 1209.6666666666667\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4789157411006265\n",
            "  episode_reward_mean: -0.5017334848253339\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 93\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.0052734375\n",
            "          cur_lr: 9.88e-06\n",
            "          entropy: 0.2006522798910737\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0036146331767668016\n",
            "          policy_loss: -0.017177159345010296\n",
            "          total_loss: 0.10750088276108727\n",
            "          vf_explained_var: 0.5513818264007568\n",
            "          vf_loss: 0.2533310065045953\n",
            "    num_agent_steps_sampled: 116000\n",
            "    num_steps_sampled: 116000\n",
            "    num_steps_trained: 116000\n",
            "  iterations_since_restore: 29\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.983870967741936\n",
            "    ram_util_percent: 15.69354838709677\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.0487228220162364\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.0614980650770764\n",
            "    mean_inference_ms: 1.2942864531529228\n",
            "    mean_raw_obs_processing_ms: 0.15686713491727655\n",
            "  time_since_restore: 571.3670575618744\n",
            "  time_this_iter_s: 21.519108057022095\n",
            "  time_total_s: 571.3670575618744\n",
            "  timers:\n",
            "    learn_throughput: 513.258\n",
            "    learn_time_ms: 7793.351\n",
            "    sample_throughput: 309.427\n",
            "    sample_time_ms: 12927.135\n",
            "    update_time_ms: 1.699\n",
            "  timestamp: 1619351685\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 116000\n",
            "  training_iteration: 29\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:54:45,631\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=116094, mean_mean=15.888383189057155, mean_std=9.756508585895046), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.0/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         571.367</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">-0.501733</td><td style=\"text-align: right;\">           -0.478916</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           1209.67</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 120000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-55-06\n",
            "  done: false\n",
            "  episode_len_mean: 1260.7473684210527\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4722132475200209\n",
            "  episode_reward_mean: -0.5011994756157775\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 2\n",
            "  episodes_total: 95\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.00263671875\n",
            "          cur_lr: 9.84e-06\n",
            "          entropy: 0.18749815877526999\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.004838957160245627\n",
            "          policy_loss: -0.029085156711516902\n",
            "          total_loss: 0.09272021459764801\n",
            "          vf_explained_var: 0.5548586845397949\n",
            "          vf_loss: 0.24733519228175282\n",
            "    num_agent_steps_sampled: 120000\n",
            "    num_steps_sampled: 120000\n",
            "    num_steps_trained: 120000\n",
            "  iterations_since_restore: 30\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.953333333333337\n",
            "    ram_util_percent: 15.699999999999996\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04872127529356274\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.0695385957671606\n",
            "    mean_inference_ms: 1.2942456008737415\n",
            "    mean_raw_obs_processing_ms: 0.15692907763419656\n",
            "  time_since_restore: 592.627858877182\n",
            "  time_this_iter_s: 21.260801315307617\n",
            "  time_total_s: 592.627858877182\n",
            "  timers:\n",
            "    learn_throughput: 513.616\n",
            "    learn_time_ms: 7787.918\n",
            "    sample_throughput: 306.929\n",
            "    sample_time_ms: 13032.336\n",
            "    update_time_ms: 1.705\n",
            "  timestamp: 1619351706\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 120000\n",
            "  training_iteration: 30\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:55:06,930\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=120096, mean_mean=15.898965978297564, mean_std=9.742018331977487), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.0/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         592.628</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">-0.501199</td><td style=\"text-align: right;\">           -0.472213</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           1260.75</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 124000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-55-27\n",
            "  done: false\n",
            "  episode_len_mean: 1282.4166666666667\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4722132475200209\n",
            "  episode_reward_mean: -0.5009924705745367\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 96\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.001318359375\n",
            "          cur_lr: 9.800000000000001e-06\n",
            "          entropy: 0.18436060892418027\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.003030801242857706\n",
            "          policy_loss: -0.030207696894649416\n",
            "          total_loss: 0.07075805435306393\n",
            "          vf_explained_var: 0.6397581100463867\n",
            "          vf_loss: 0.20561071624979377\n",
            "    num_agent_steps_sampled: 124000\n",
            "    num_steps_sampled: 124000\n",
            "    num_steps_trained: 124000\n",
            "  iterations_since_restore: 31\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.913333333333334\n",
            "    ram_util_percent: 15.699999999999996\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04872045951177551\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.0735013844402044\n",
            "    mean_inference_ms: 1.2942639498123782\n",
            "    mean_raw_obs_processing_ms: 0.15695844365640838\n",
            "  time_since_restore: 613.5560865402222\n",
            "  time_this_iter_s: 20.92822766304016\n",
            "  time_total_s: 613.5560865402222\n",
            "  timers:\n",
            "    learn_throughput: 513.487\n",
            "    learn_time_ms: 7789.878\n",
            "    sample_throughput: 306.789\n",
            "    sample_time_ms: 13038.265\n",
            "    update_time_ms: 1.73\n",
            "  timestamp: 1619351727\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 124000\n",
            "  training_iteration: 31\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:55:27,894\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=124097, mean_mean=15.915852195921419, mean_std=9.727017902789427), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.0/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         613.556</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">-0.500992</td><td style=\"text-align: right;\">           -0.472213</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           1282.42</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:55:27,931\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': { 'net_worth': 63368.74,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                             'step': 888},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=-0.94, max=2.636, mean=0.346),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:55:27,934\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-3.74, max=3.758, mean=0.009),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-0.001, max=-0.001, mean=-0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.999, max=0.999, mean=0.999),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.134, max=0.134, mean=0.134)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:55:27,937\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=-11.4, max=61.907, mean=20.024)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:55:27,937\tINFO sampler.py:589 -- Info return from env: {0: {'agent0': {'net_worth': 63368.74, 'step': 889}}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:55:27,938\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=-11.4, max=61.907, mean=20.024)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:55:27,938\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=-0.737, max=2.637, mean=0.352)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:55:28,519\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-6.837, max=6.837, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-2.308, max=-0.0, mean=-0.131),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((200,), dtype=float32, min=0.099, max=1.0, mean=0.908),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.435),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-1.529, max=1.327, mean=0.016),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=875419210.0, max=875419210.0, mean=875419210.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 889, 'net_worth': 63368.74}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-5.255, max=10.925, mean=-0.121),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((200, 25, 3), dtype=float32, min=-5.255, max=10.925, mean=-0.117),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.08),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=716.0, max=716.0, mean=716.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.08),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-1.131, max=1.118, mean=-0.096)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:55:28,521\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-6.837, max=6.837, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-2.308, max=-0.0, mean=-0.131),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.099, max=1.0, mean=0.908),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.435),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-1.529, max=1.327, mean=0.016),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=875419210.0, max=875419210.0, mean=875419210.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 889, 'net_worth': 63368.74}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-5.255, max=10.925, mean=-0.121),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-5.255, max=10.925, mean=-0.117),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.08),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=716.0, max=716.0, mean=716.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.08),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-1.131, max=1.118, mean=-0.096)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:55:28,526\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:55:41,094\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:55:41,098\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-7.556, max=7.579, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((128,), dtype=float32, min=-3.126, max=0.0, mean=-0.199),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((128,), dtype=float32, min=0.044, max=1.0, mean=0.886),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.508),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((128,), dtype=float32, min=-4.104, max=2.866, mean=-0.011),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((128,), dtype=int64, min=875419210.0, max=1135576905.0, mean=985173237.578),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((128,), dtype=object, head={'step': 282, 'net_worth': 65608.7}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((128, 25, 3), dtype=float32, min=-5.798, max=9.634, mean=0.065),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((128, 25, 3), dtype=float32, min=-5.797, max=9.632, mean=0.063),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((128,), dtype=int64, min=716.0, max=736.0, mean=726.602),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((128,), dtype=float32, min=-1.198, max=1.073, mean=0.004)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:55:41,108\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 0.0006591796875,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 9.760000000000001e-06,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.1620047390460968,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': 9.932941580359511e-09,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': 0.0110801812261343,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': 0.11539442092180252,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.669, max=0.669, mean=0.669),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.211868554353714},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 128000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-55-48\n",
            "  done: false\n",
            "  episode_len_mean: 1304.5257731958764\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4722132475200209\n",
            "  episode_reward_mean: -0.5007893704010533\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 97\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.0006591796875\n",
            "          cur_lr: 9.760000000000001e-06\n",
            "          entropy: 0.182933593634516\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0029739141136815306\n",
            "          policy_loss: -0.026164911221712828\n",
            "          total_loss: 0.0800682025437709\n",
            "          vf_explained_var: 0.6145104765892029\n",
            "          vf_loss: 0.21612098067998886\n",
            "    num_agent_steps_sampled: 128000\n",
            "    num_steps_sampled: 128000\n",
            "    num_steps_trained: 128000\n",
            "  iterations_since_restore: 32\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 29.006666666666668\n",
            "    ram_util_percent: 15.699999999999996\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04871984329059845\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.077483962159103\n",
            "    mean_inference_ms: 1.2942785047604037\n",
            "    mean_raw_obs_processing_ms: 0.15698837524443118\n",
            "  time_since_restore: 634.479437828064\n",
            "  time_this_iter_s: 20.923351287841797\n",
            "  time_total_s: 634.479437828064\n",
            "  timers:\n",
            "    learn_throughput: 513.72\n",
            "    learn_time_ms: 7786.338\n",
            "    sample_throughput: 303.939\n",
            "    sample_time_ms: 13160.552\n",
            "    update_time_ms: 1.729\n",
            "  timestamp: 1619351748\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 128000\n",
            "  training_iteration: 32\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.0/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         634.479</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\">-0.500789</td><td style=\"text-align: right;\">           -0.472213</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           1304.53</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:55:48,856\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=128098, mean_mean=15.930219909884872, mean_std=9.712171293402111), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 132000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-56-09\n",
            "  done: false\n",
            "  episode_len_mean: 1326.7244897959183\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4722132475200209\n",
            "  episode_reward_mean: -0.50058840766752\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 98\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.00032958984375\n",
            "          cur_lr: 9.72e-06\n",
            "          entropy: 0.18604370905086398\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0030027106731722597\n",
            "          policy_loss: -0.021028932911576703\n",
            "          total_loss: 0.08119741990230978\n",
            "          vf_explained_var: 0.6461504101753235\n",
            "          vf_loss: 0.20817159628495574\n",
            "    num_agent_steps_sampled: 132000\n",
            "    num_steps_sampled: 132000\n",
            "    num_steps_trained: 132000\n",
            "  iterations_since_restore: 33\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.846666666666668\n",
            "    ram_util_percent: 15.699999999999996\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04871922142054871\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.0814882917155861\n",
            "    mean_inference_ms: 1.2942818429293925\n",
            "    mean_raw_obs_processing_ms: 0.15701771428375724\n",
            "  time_since_restore: 655.5892641544342\n",
            "  time_this_iter_s: 21.10982632637024\n",
            "  time_total_s: 655.5892641544342\n",
            "  timers:\n",
            "    learn_throughput: 512.506\n",
            "    learn_time_ms: 7804.791\n",
            "    sample_throughput: 302.602\n",
            "    sample_time_ms: 13218.689\n",
            "    update_time_ms: 1.717\n",
            "  timestamp: 1619351769\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 132000\n",
            "  training_iteration: 33\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:56:10,002\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=132099, mean_mean=15.950273622713071, mean_std=9.704823873984651), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.0/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         655.589</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">-0.500588</td><td style=\"text-align: right;\">           -0.472213</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           1326.72</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 136000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-56-31\n",
            "  done: false\n",
            "  episode_len_mean: 1349.4040404040404\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4722132475200209\n",
            "  episode_reward_mean: -0.5004561254286461\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 99\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.000164794921875\n",
            "          cur_lr: 9.68e-06\n",
            "          entropy: 0.18720321683213115\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.002968612821860006\n",
            "          policy_loss: -0.030182630522176623\n",
            "          total_loss: 0.07685121480608359\n",
            "          vf_explained_var: 0.6171491146087646\n",
            "          vf_loss: 0.21781078819185495\n",
            "    num_agent_steps_sampled: 136000\n",
            "    num_steps_sampled: 136000\n",
            "    num_steps_trained: 136000\n",
            "  iterations_since_restore: 34\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.773333333333333\n",
            "    ram_util_percent: 15.699999999999996\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04871838416416525\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.0855218452344142\n",
            "    mean_inference_ms: 1.2942707601399241\n",
            "    mean_raw_obs_processing_ms: 0.15704628470372325\n",
            "  time_since_restore: 676.6458532810211\n",
            "  time_this_iter_s: 21.056589126586914\n",
            "  time_total_s: 676.6458532810211\n",
            "  timers:\n",
            "    learn_throughput: 511.798\n",
            "    learn_time_ms: 7815.582\n",
            "    sample_throughput: 303.53\n",
            "    sample_time_ms: 13178.263\n",
            "    update_time_ms: 1.711\n",
            "  timestamp: 1619351791\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 136000\n",
            "  training_iteration: 34\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:56:31,096\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=136100, mean_mean=15.964621599798807, mean_std=9.689786762774588), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.0/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         676.646</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\">-0.500456</td><td style=\"text-align: right;\">           -0.472213</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">            1349.4</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:56:31,133\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': { 'net_worth': 61247.842104024,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                             'step': 2409},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=-4.75, max=1.536, mean=-0.437),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': 1,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': -0.0029536060975482448,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:56:31,136\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-1.351, max=1.345, mean=-0.003),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-0.065, max=-0.065, mean=-0.065),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.937, max=0.937, mean=0.937),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.457, max=0.457, mean=0.457)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:56:31,141\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=-42.876, max=49.807, mean=10.735)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:56:31,142\tINFO sampler.py:589 -- Info return from env: { 0: { 'agent0': { 'net_worth': 61378.08788771101,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                    'step': 2410}}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:56:31,142\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=-42.876, max=49.807, mean=10.735)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:56:31,143\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=-2.648, max=1.536, mean=-0.333)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:56:31,908\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-7.021, max=7.036, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-4.485, max=-0.0, mean=-0.144),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((200,), dtype=float32, min=0.011, max=1.0, mean=0.905),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.585),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-1.323, max=1.267, mean=0.047),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=270966814.0, max=270966814.0, mean=270966814.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 2410, 'net_worth': 61378.08788771101}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-2.652, max=3.49, mean=0.244),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.75, max=3.49, mean=0.241),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.08),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=779.0, max=779.0, mean=779.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.08),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-0.982, max=0.936, mean=-0.127)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:56:31,911\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-7.021, max=7.036, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-4.485, max=-0.0, mean=-0.144),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.011, max=1.0, mean=0.905),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.585),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-1.323, max=1.267, mean=0.047),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=270966814.0, max=270966814.0, mean=270966814.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 2410, 'net_worth': 61378.08788771101}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-2.652, max=3.49, mean=0.244),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.75, max=3.49, mean=0.241),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.08),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=779.0, max=779.0, mean=779.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.08),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-0.982, max=0.936, mean=-0.127)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:56:31,915\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:56:44,764\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:56:44,767\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-9.478, max=9.48, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((128,), dtype=float32, min=-2.9, max=0.0, mean=-0.187),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((128,), dtype=float32, min=0.055, max=1.0, mean=0.88),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.445),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((128,), dtype=float32, min=-2.933, max=2.497, mean=0.034),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((128,), dtype=int64, min=59582472.0, max=270966814.0, mean=117382878.016),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((128,), dtype=object, head={'step': 636, 'net_worth': 64988.94}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((128, 25, 3), dtype=float32, min=-10.123, max=10.933, mean=0.095),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((128, 25, 3), dtype=float32, min=-10.113, max=10.931, mean=0.097),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.07),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((128,), dtype=int64, min=779.0, max=799.0, mean=789.32),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.07),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((128,), dtype=float32, min=-1.06, max=1.027, mean=-0.103)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:56:44,776\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 8.23974609375e-05,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 9.640000000000001e-06,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.1980890929698944,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': -6.16256379259994e-09,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': -0.03430459648370743,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': 0.07265744358301163,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.646, max=0.646, mean=0.646),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.21788588166236877},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 140000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-56-52\n",
            "  done: false\n",
            "  episode_len_mean: 1371.24\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4722132475200209\n",
            "  episode_reward_mean: -0.5002784215573466\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 100\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 8.23974609375e-05\n",
            "          cur_lr: 9.640000000000001e-06\n",
            "          entropy: 0.18031854229047894\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.004200732895697001\n",
            "          policy_loss: -0.02692153217503801\n",
            "          total_loss: 0.07825799187412485\n",
            "          vf_explained_var: 0.6286167502403259\n",
            "          vf_loss: 0.21396473981440067\n",
            "    num_agent_steps_sampled: 140000\n",
            "    num_steps_sampled: 140000\n",
            "    num_steps_trained: 140000\n",
            "  iterations_since_restore: 35\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 29.183870967741942\n",
            "    ram_util_percent: 15.699999999999996\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04871775651062235\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.089591185952694\n",
            "    mean_inference_ms: 1.2942559483850191\n",
            "    mean_raw_obs_processing_ms: 0.1570751251056977\n",
            "  time_since_restore: 698.1103222370148\n",
            "  time_this_iter_s: 21.464468955993652\n",
            "  time_total_s: 698.1103222370148\n",
            "  timers:\n",
            "    learn_throughput: 510.78\n",
            "    learn_time_ms: 7831.163\n",
            "    sample_throughput: 300.387\n",
            "    sample_time_ms: 13316.151\n",
            "    update_time_ms: 1.726\n",
            "  timestamp: 1619351812\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 140000\n",
            "  training_iteration: 35\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:56:52,600\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=140101, mean_mean=15.981116543574263, mean_std=9.674790923010148), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.0/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">          698.11</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">-0.500278</td><td style=\"text-align: right;\">           -0.472213</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           1371.24</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 144000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-57-14\n",
            "  done: false\n",
            "  episode_len_mean: 1403.6\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4722132475200209\n",
            "  episode_reward_mean: -0.5000336835888483\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 101\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 4.119873046875e-05\n",
            "          cur_lr: 9.600000000000001e-06\n",
            "          entropy: 0.18331157509237528\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.003703549453348387\n",
            "          policy_loss: -0.02256552129983902\n",
            "          total_loss: 0.09195251928758807\n",
            "          vf_explained_var: 0.6003190279006958\n",
            "          vf_loss: 0.23270201869308949\n",
            "    num_agent_steps_sampled: 144000\n",
            "    num_steps_sampled: 144000\n",
            "    num_steps_trained: 144000\n",
            "  iterations_since_restore: 36\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.925000000000004\n",
            "    ram_util_percent: 15.7\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04871923305483394\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.0955677587317643\n",
            "    mean_inference_ms: 1.2943054511922534\n",
            "    mean_raw_obs_processing_ms: 0.15712768539750166\n",
            "  time_since_restore: 720.3721139431\n",
            "  time_this_iter_s: 22.261791706085205\n",
            "  time_total_s: 720.3721139431\n",
            "  timers:\n",
            "    learn_throughput: 510.487\n",
            "    learn_time_ms: 7835.651\n",
            "    sample_throughput: 296.816\n",
            "    sample_time_ms: 13476.358\n",
            "    update_time_ms: 1.727\n",
            "  timestamp: 1619351834\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 144000\n",
            "  training_iteration: 36\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.0/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         720.372</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\">-0.500034</td><td style=\"text-align: right;\">           -0.472213</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">            1403.6</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:57:14,902\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=144102, mean_mean=15.9968968969779, mean_std=9.653964113039615), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 148000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-57-36\n",
            "  done: false\n",
            "  episode_len_mean: 1440.27\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4722132475200209\n",
            "  episode_reward_mean: -0.4995093850771187\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 102\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 2.0599365234375e-05\n",
            "          cur_lr: 9.56e-06\n",
            "          entropy: 0.17666380759328604\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.003152188059175387\n",
            "          policy_loss: -0.02529037054046057\n",
            "          total_loss: 0.09761913781403564\n",
            "          vf_explained_var: 0.5572994947433472\n",
            "          vf_loss: 0.24935216223821044\n",
            "    num_agent_steps_sampled: 148000\n",
            "    num_steps_sampled: 148000\n",
            "    num_steps_trained: 148000\n",
            "  iterations_since_restore: 37\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.829032258064522\n",
            "    ram_util_percent: 15.716129032258063\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.048720538796231734\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.1016632350817133\n",
            "    mean_inference_ms: 1.294341708008716\n",
            "    mean_raw_obs_processing_ms: 0.15718014414189543\n",
            "  time_since_restore: 742.0087180137634\n",
            "  time_this_iter_s: 21.636604070663452\n",
            "  time_total_s: 742.0087180137634\n",
            "  timers:\n",
            "    learn_throughput: 510.025\n",
            "    learn_time_ms: 7842.757\n",
            "    sample_throughput: 296.655\n",
            "    sample_time_ms: 13483.681\n",
            "    update_time_ms: 1.736\n",
            "  timestamp: 1619351856\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 148000\n",
            "  training_iteration: 37\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.0/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         742.009</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">-0.499509</td><td style=\"text-align: right;\">           -0.472213</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           1440.27</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:57:36,576\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=148103, mean_mean=16.007880965026196, mean_std=9.63515768100789), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:57:36,609\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': { 'net_worth': 41000.23,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                             'step': 3431},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=-1.083, max=1.017, mean=0.219),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:57:36,613\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-0.999, max=1.001, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-0.127, max=-0.127, mean=-0.127),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.881, max=0.881, mean=0.881),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.177, max=-0.177, mean=-0.177)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:57:36,620\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=-5.05, max=60.897, mean=18.709)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:57:36,620\tINFO sampler.py:589 -- Info return from env: {0: {'agent0': {'net_worth': 41053.886148, 'step': 3432}}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:57:36,621\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=-5.05, max=60.897, mean=18.709)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:57:36,621\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=-1.084, max=1.015, mean=0.216)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:57:37,481\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-9.906, max=9.911, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-2.757, max=0.0, mean=-0.204),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((200,), dtype=float32, min=0.063, max=1.0, mean=0.871),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.49),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-2.025, max=2.178, mean=-0.026),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=1596089946.0, max=1596089946.0, mean=1596089946.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 3432, 'net_worth': 41053.886148}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-5.094, max=1.787, mean=-0.022),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((200, 25, 3), dtype=float32, min=-5.094, max=1.787, mean=-0.02),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.17),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=842.0, max=842.0, mean=842.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.17),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-1.765, max=1.025, mean=-0.144)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:57:37,483\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-9.906, max=9.911, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-2.757, max=0.0, mean=-0.204),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.063, max=1.0, mean=0.871),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.49),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-2.025, max=2.178, mean=-0.026),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=1596089946.0, max=1596089946.0, mean=1596089946.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 3432, 'net_worth': 41053.886148}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-5.094, max=1.787, mean=-0.022),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-5.094, max=1.787, mean=-0.02),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.17),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=842.0, max=842.0, mean=842.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.17),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-1.765, max=1.025, mean=-0.144)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:57:37,487\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:57:50,344\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:57:50,347\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-8.894, max=8.903, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((128,), dtype=float32, min=-2.899, max=0.0, mean=-0.136),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((128,), dtype=float32, min=0.055, max=1.0, mean=0.91),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.5),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((128,), dtype=float32, min=-2.657, max=2.76, mean=0.061),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((128,), dtype=int64, min=1529849998.0, max=1596089946.0, mean=1536577492.719),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((128,), dtype=object, head={'step': 60, 'net_worth': 59320.69}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((128, 25, 3), dtype=float32, min=-5.839, max=9.65, mean=0.051),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((128, 25, 3), dtype=float32, min=-5.838, max=9.648, mean=0.046),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.008),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((128,), dtype=int64, min=842.0, max=862.0, mean=851.523),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.008),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((128,), dtype=float32, min=-1.106, max=1.104, mean=-0.049)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:57:50,357\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 1.02996826171875e-05,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 9.52e-06,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.1677478402853012,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': 4.2109316034100175e-09,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': -0.06072165444493294,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': 0.06754766404628754,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.559, max=0.559, mean=0.559),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.25989359617233276},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 152000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-57-58\n",
            "  done: false\n",
            "  episode_len_mean: 1475.79\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4722132475200209\n",
            "  episode_reward_mean: -0.49914931592517675\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 103\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.02996826171875e-05\n",
            "          cur_lr: 9.52e-06\n",
            "          entropy: 0.18007018184289336\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0037225178311928175\n",
            "          policy_loss: -0.018939732370199636\n",
            "          total_loss: 0.10438051199889742\n",
            "          vf_explained_var: 0.5625643730163574\n",
            "          vf_loss: 0.2502418337389827\n",
            "    num_agent_steps_sampled: 152000\n",
            "    num_steps_sampled: 152000\n",
            "    num_steps_trained: 152000\n",
            "  iterations_since_restore: 38\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 29.012903225806454\n",
            "    ram_util_percent: 15.800000000000004\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.048721845036169525\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.1078665108686447\n",
            "    mean_inference_ms: 1.2943713276468036\n",
            "    mean_raw_obs_processing_ms: 0.15723306821824562\n",
            "  time_since_restore: 763.4686598777771\n",
            "  time_this_iter_s: 21.459941864013672\n",
            "  time_total_s: 763.4686598777771\n",
            "  timers:\n",
            "    learn_throughput: 510.527\n",
            "    learn_time_ms: 7835.046\n",
            "    sample_throughput: 295.883\n",
            "    sample_time_ms: 13518.878\n",
            "    update_time_ms: 1.716\n",
            "  timestamp: 1619351878\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 152000\n",
            "  training_iteration: 38\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:57:58,070\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=152104, mean_mean=16.01782878313572, mean_std=9.618752957924517), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.0/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         763.469</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\">-0.499149</td><td style=\"text-align: right;\">           -0.472213</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           1475.79</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 156000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-58-19\n",
            "  done: false\n",
            "  episode_len_mean: 1516.04\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.47194638710841286\n",
            "  episode_reward_mean: -0.4987894802352127\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 104\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 5.14984130859375e-06\n",
            "          cur_lr: 9.48e-06\n",
            "          entropy: 0.1670657522045076\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.003364230004081037\n",
            "          policy_loss: -0.026194626647338737\n",
            "          total_loss: 0.09534930950030684\n",
            "          vf_explained_var: 0.5641337633132935\n",
            "          vf_loss: 0.24642916396260262\n",
            "    num_agent_steps_sampled: 156000\n",
            "    num_steps_sampled: 156000\n",
            "    num_steps_trained: 156000\n",
            "  iterations_since_restore: 39\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.85806451612903\n",
            "    ram_util_percent: 15.800000000000004\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04872304765558564\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.1141791657600666\n",
            "    mean_inference_ms: 1.2943919635887102\n",
            "    mean_raw_obs_processing_ms: 0.15728596787215776\n",
            "  time_since_restore: 785.1494598388672\n",
            "  time_this_iter_s: 21.680799961090088\n",
            "  time_total_s: 785.1494598388672\n",
            "  timers:\n",
            "    learn_throughput: 510.345\n",
            "    learn_time_ms: 7837.83\n",
            "    sample_throughput: 295.591\n",
            "    sample_time_ms: 13532.232\n",
            "    update_time_ms: 1.717\n",
            "  timestamp: 1619351899\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 156000\n",
            "  training_iteration: 39\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.0/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         785.149</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">-0.498789</td><td style=\"text-align: right;\">           -0.471946</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           1516.04</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:58:19,788\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=156105, mean_mean=16.028573192978786, mean_std=9.601038862390945), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 160000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-58-41\n",
            "  done: false\n",
            "  episode_len_mean: 1554.61\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4707517833809657\n",
            "  episode_reward_mean: -0.49837434636393285\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 105\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 2.574920654296875e-06\n",
            "          cur_lr: 9.440000000000001e-06\n",
            "          entropy: 0.16764060407876968\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.003940324466384482\n",
            "          policy_loss: -0.022341036674333736\n",
            "          total_loss: 0.08667337111546658\n",
            "          vf_explained_var: 0.5968314409255981\n",
            "          vf_loss: 0.22138161165639758\n",
            "    num_agent_steps_sampled: 160000\n",
            "    num_steps_sampled: 160000\n",
            "    num_steps_trained: 160000\n",
            "  iterations_since_restore: 40\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 29.01\n",
            "    ram_util_percent: 15.800000000000004\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04872408667310312\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.1205936304250839\n",
            "    mean_inference_ms: 1.2944013937466685\n",
            "    mean_raw_obs_processing_ms: 0.1573389346848227\n",
            "  time_since_restore: 806.6354780197144\n",
            "  time_this_iter_s: 21.486018180847168\n",
            "  time_total_s: 806.6354780197144\n",
            "  timers:\n",
            "    learn_throughput: 511.217\n",
            "    learn_time_ms: 7824.467\n",
            "    sample_throughput: 294.808\n",
            "    sample_time_ms: 13568.175\n",
            "    update_time_ms: 1.759\n",
            "  timestamp: 1619351921\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 160000\n",
            "  training_iteration: 40\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:58:41,309\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=160106, mean_mean=16.035019148318487, mean_std=9.583835241112604), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.0/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         806.635</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">-0.498374</td><td style=\"text-align: right;\">           -0.470752</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           1554.61</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:58:41,347\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': { 'net_worth': 53290.53,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                             'step': 3232},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=-0.831, max=1.512, mean=0.272),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:58:41,351\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-2.907, max=2.92, mean=0.007),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-0.003, max=-0.003, mean=-0.003),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.997, max=0.997, mean=0.997),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.055, max=0.055, mean=0.055)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:58:41,358\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=-5.403, max=58.479, mean=19.74)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:58:41,358\tINFO sampler.py:589 -- Info return from env: {0: {'agent0': {'net_worth': 53290.53, 'step': 3233}}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:58:41,358\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=-5.403, max=58.479, mean=19.74)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:58:41,359\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=-0.832, max=1.512, mean=0.271)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:58:42,245\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-6.867, max=6.882, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-4.963, max=-0.0, mean=-0.248),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((200,), dtype=float32, min=0.007, max=1.0, mean=0.858),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.39),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-1.467, max=1.608, mean=-0.02),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=829050206.0, max=829050206.0, mean=829050206.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 3233, 'net_worth': 53290.53}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-3.353, max=3.514, mean=0.1),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((200, 25, 3), dtype=float32, min=-3.353, max=3.514, mean=0.1),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.125),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=905.0, max=905.0, mean=905.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.125),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-0.988, max=0.715, mean=-0.105)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:58:42,248\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-6.867, max=6.882, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-4.963, max=-0.0, mean=-0.248),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.007, max=1.0, mean=0.858),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.39),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-1.467, max=1.608, mean=-0.02),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=829050206.0, max=829050206.0, mean=829050206.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 3233, 'net_worth': 53290.53}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-3.353, max=3.514, mean=0.1),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-3.353, max=3.514, mean=0.1),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.125),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=905.0, max=905.0, mean=905.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.125),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-0.988, max=0.715, mean=-0.105)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:58:42,252\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:58:55,960\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:58:55,964\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-7.913, max=7.919, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((128,), dtype=float32, min=-4.955, max=0.0, mean=-0.314),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((128,), dtype=float32, min=0.007, max=1.0, mean=0.849),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.406),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((128,), dtype=float32, min=-3.708, max=2.896, mean=-0.006),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((128,), dtype=int64, min=829050206.0, max=956815515.0, mean=925872354.227),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((128,), dtype=object, head={'step': 3002, 'net_worth': 52068.788646}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((128, 25, 3), dtype=float32, min=-10.167, max=10.992, mean=0.058),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((128, 25, 3), dtype=float32, min=-10.159, max=10.99, mean=0.057),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.047),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((128,), dtype=int64, min=905.0, max=925.0, mean=915.109),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.047),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((128,), dtype=float32, min=-0.989, max=1.037, mean=-0.04)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:58:55,974\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 1.2874603271484376e-06,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 9.400000000000001e-06,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.1804846227169037,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': 4.648172957644192e-10,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': 0.0059906356036663055,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': 0.14574092626571655,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.495, max=0.495, mean=0.495),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.28311029076576233},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 164000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-59-03\n",
            "  done: false\n",
            "  episode_len_mean: 1593.49\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4707517833809657\n",
            "  episode_reward_mean: -0.4979301517043244\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 106\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.2874603271484376e-06\n",
            "          cur_lr: 9.400000000000001e-06\n",
            "          entropy: 0.16745587484911084\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.004659509133489337\n",
            "          policy_loss: -0.023171058477601036\n",
            "          total_loss: 0.09121035214047879\n",
            "          vf_explained_var: 0.5789772868156433\n",
            "          vf_loss: 0.23211193457245827\n",
            "    num_agent_steps_sampled: 164000\n",
            "    num_steps_sampled: 164000\n",
            "    num_steps_trained: 164000\n",
            "  iterations_since_restore: 41\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.987499999999997\n",
            "    ram_util_percent: 15.8\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04872508394250634\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.1271129219755394\n",
            "    mean_inference_ms: 1.2944462247343862\n",
            "    mean_raw_obs_processing_ms: 0.15739230080695144\n",
            "  time_since_restore: 829.02041888237\n",
            "  time_this_iter_s: 22.38494086265564\n",
            "  time_total_s: 829.02041888237\n",
            "  timers:\n",
            "    learn_throughput: 511.318\n",
            "    learn_time_ms: 7822.926\n",
            "    sample_throughput: 291.643\n",
            "    sample_time_ms: 13715.402\n",
            "    update_time_ms: 1.739\n",
            "  timestamp: 1619351943\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 164000\n",
            "  training_iteration: 41\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:59:03,733\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=164107, mean_mean=16.04236001860507, mean_std=9.56709227579796), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.0/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">          829.02</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">-0.49793</td><td style=\"text-align: right;\">           -0.470752</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           1593.49</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 168000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-59-25\n",
            "  done: false\n",
            "  episode_len_mean: 1629.88\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4707517833809657\n",
            "  episode_reward_mean: -0.4975867275060573\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 107\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 6.437301635742188e-07\n",
            "          cur_lr: 9.36e-06\n",
            "          entropy: 0.168603440746665\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0026479228654352482\n",
            "          policy_loss: -0.0285381481226068\n",
            "          total_loss: 0.08689777430845425\n",
            "          vf_explained_var: 0.5847318172454834\n",
            "          vf_loss: 0.23424391075968742\n",
            "    num_agent_steps_sampled: 168000\n",
            "    num_steps_sampled: 168000\n",
            "    num_steps_trained: 168000\n",
            "  iterations_since_restore: 42\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.816129032258065\n",
            "    ram_util_percent: 15.800000000000004\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.048725929069319295\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.1337199559108968\n",
            "    mean_inference_ms: 1.2944815718042069\n",
            "    mean_raw_obs_processing_ms: 0.1574458683259018\n",
            "  time_since_restore: 850.4857466220856\n",
            "  time_this_iter_s: 21.465327739715576\n",
            "  time_total_s: 850.4857466220856\n",
            "  timers:\n",
            "    learn_throughput: 511.125\n",
            "    learn_time_ms: 7825.872\n",
            "    sample_throughput: 290.558\n",
            "    sample_time_ms: 13766.619\n",
            "    update_time_ms: 1.738\n",
            "  timestamp: 1619351965\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 168000\n",
            "  training_iteration: 42\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:59:25,234\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=168108, mean_mean=16.052501577340024, mean_std=9.552826991449939), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.0/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         850.486</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\">-0.497587</td><td style=\"text-align: right;\">           -0.470752</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           1629.88</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 172000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_11-59-46\n",
            "  done: false\n",
            "  episode_len_mean: 1662.9\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4707517833809657\n",
            "  episode_reward_mean: -0.4973306249842942\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 108\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 3.218650817871094e-07\n",
            "          cur_lr: 9.32e-06\n",
            "          entropy: 0.17245064955204725\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.004492301122809295\n",
            "          policy_loss: -0.023669564514420927\n",
            "          total_loss: 0.09088024229276925\n",
            "          vf_explained_var: 0.5990070104598999\n",
            "          vf_loss: 0.23254862148314714\n",
            "    num_agent_steps_sampled: 172000\n",
            "    num_steps_sampled: 172000\n",
            "    num_steps_trained: 172000\n",
            "  iterations_since_restore: 43\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 30.446666666666665\n",
            "    ram_util_percent: 15.800000000000004\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04872651858471934\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.1403999880047697\n",
            "    mean_inference_ms: 1.2945040180877232\n",
            "    mean_raw_obs_processing_ms: 0.15749907710299552\n",
            "  time_since_restore: 871.7198650836945\n",
            "  time_this_iter_s: 21.234118461608887\n",
            "  time_total_s: 871.7198650836945\n",
            "  timers:\n",
            "    learn_throughput: 511.974\n",
            "    learn_time_ms: 7812.902\n",
            "    sample_throughput: 290.022\n",
            "    sample_time_ms: 13792.057\n",
            "    update_time_ms: 1.729\n",
            "  timestamp: 1619351986\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 172000\n",
            "  training_iteration: 43\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.0/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">          871.72</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">-0.497331</td><td style=\"text-align: right;\">           -0.470752</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">            1662.9</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:59:46,508\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=172109, mean_mean=16.06291316528394, mean_std=9.536254285928313), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:59:46,541\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': { 'net_worth': 39516.18,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                             'step': 3569},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=-1.364, max=0.781, mean=-0.096),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:59:46,545\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-5.119, max=5.117, mean=-0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-0.0, max=-0.0, mean=-0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.046, max=0.046, mean=0.046)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:59:46,554\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=-14.296, max=54.701, mean=14.54)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:59:46,554\tINFO sampler.py:589 -- Info return from env: {0: {'agent0': {'net_worth': 39516.18, 'step': 3570}}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:59:46,555\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=-14.296, max=54.701, mean=14.54)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:59:46,555\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=-1.686, max=0.781, mean=-0.136)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:59:47,425\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-10.517, max=10.523, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-3.588, max=0.0, mean=-0.171),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((200,), dtype=float32, min=0.028, max=1.0, mean=0.907),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.47),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-1.481, max=2.053, mean=0.044),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=1750833988.0, max=1750833988.0, mean=1750833988.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 3570, 'net_worth': 39516.18}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-12.031, max=16.168, mean=0.002),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((200, 25, 3), dtype=float32, min=-12.031, max=16.168, mean=0.003),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.07),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=968.0, max=968.0, mean=968.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.07),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-1.435, max=1.063, mean=-0.114)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:59:47,428\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-10.517, max=10.523, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-3.588, max=0.0, mean=-0.171),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.028, max=1.0, mean=0.907),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.47),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-1.481, max=2.053, mean=0.044),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=1750833988.0, max=1750833988.0, mean=1750833988.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 3570, 'net_worth': 39516.18}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-12.031, max=16.168, mean=0.002),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-12.031, max=16.168, mean=0.003),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.07),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=968.0, max=968.0, mean=968.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.07),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-1.435, max=1.063, mean=-0.114)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 11:59:47,431\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:59:59,953\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:59:59,957\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-7.685, max=7.677, mean=-0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((128,), dtype=float32, min=-1.861, max=0.0, mean=-0.084),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((128,), dtype=float32, min=0.155, max=1.0, mean=0.943),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.516),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((128,), dtype=float32, min=-3.291, max=3.388, mean=0.083),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((128,), dtype=int64, min=838371456.0, max=1750833988.0, mean=916786204.844),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((128,), dtype=object, head={'step': 1251, 'net_worth': 71040.5096697}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((128, 25, 3), dtype=float32, min=-12.028, max=16.168, mean=-0.034),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((128, 25, 3), dtype=float32, min=-12.024, max=16.165, mean=-0.028),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((128,), dtype=int64, min=968.0, max=988.0, mean=978.953),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((128,), dtype=float32, min=-1.184, max=1.151, mean=-0.051)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 11:59:59,966\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 1.609325408935547e-07,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 9.280000000000001e-06,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.11777201294898987,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': 3.8575176386501653e-10,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': -0.08326907455921173,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': 0.047129612416028976,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.561, max=0.561, mean=0.561),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.2631528377532959},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 176000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-00-07\n",
            "  done: false\n",
            "  episode_len_mean: 1700.53\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4707517833809657\n",
            "  episode_reward_mean: -0.4969086794284735\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 109\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.609325408935547e-07\n",
            "          cur_lr: 9.280000000000001e-06\n",
            "          entropy: 0.16302760550752282\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.002434147954772925\n",
            "          policy_loss: -0.022916635032743216\n",
            "          total_loss: 0.08921443577855825\n",
            "          vf_explained_var: 0.5995903611183167\n",
            "          vf_loss: 0.22752269636839628\n",
            "    num_agent_steps_sampled: 176000\n",
            "    num_steps_sampled: 176000\n",
            "    num_steps_trained: 176000\n",
            "  iterations_since_restore: 44\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.97333333333334\n",
            "    ram_util_percent: 15.800000000000004\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04872675405611768\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.1471510448510422\n",
            "    mean_inference_ms: 1.2945129354213256\n",
            "    mean_raw_obs_processing_ms: 0.1575518127215076\n",
            "  time_since_restore: 892.679811000824\n",
            "  time_this_iter_s: 20.959945917129517\n",
            "  time_total_s: 892.679811000824\n",
            "  timers:\n",
            "    learn_throughput: 513.451\n",
            "    learn_time_ms: 7790.418\n",
            "    sample_throughput: 289.751\n",
            "    sample_time_ms: 13804.943\n",
            "    update_time_ms: 1.719\n",
            "  timestamp: 1619352007\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 176000\n",
            "  training_iteration: 44\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:00:07,503\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=176110, mean_mean=16.07005433361606, mean_std=9.522550720216095), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.0/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">          892.68</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\">-0.496909</td><td style=\"text-align: right;\">           -0.470752</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           1700.53</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 180000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-00-28\n",
            "  done: false\n",
            "  episode_len_mean: 1741.3\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4707517833809657\n",
            "  episode_reward_mean: -0.49650700349203547\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 110\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 8.046627044677735e-08\n",
            "          cur_lr: 9.240000000000001e-06\n",
            "          entropy: 0.15354389883577824\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0029255697954795323\n",
            "          policy_loss: -0.022140530141768977\n",
            "          total_loss: 0.08883896219776943\n",
            "          vf_explained_var: 0.5925682187080383\n",
            "          vf_loss: 0.22502985876053572\n",
            "    num_agent_steps_sampled: 180000\n",
            "    num_steps_sampled: 180000\n",
            "    num_steps_trained: 180000\n",
            "  iterations_since_restore: 45\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.63666666666667\n",
            "    ram_util_percent: 15.800000000000004\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04872653133457301\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.1539744308235937\n",
            "    mean_inference_ms: 1.2945056029028392\n",
            "    mean_raw_obs_processing_ms: 0.1576036861726498\n",
            "  time_since_restore: 913.626523733139\n",
            "  time_this_iter_s: 20.946712732315063\n",
            "  time_total_s: 913.626523733139\n",
            "  timers:\n",
            "    learn_throughput: 515.481\n",
            "    learn_time_ms: 7759.745\n",
            "    sample_throughput: 290.194\n",
            "    sample_time_ms: 13783.895\n",
            "    update_time_ms: 1.692\n",
            "  timestamp: 1619352028\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 180000\n",
            "  training_iteration: 45\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:00:28,485\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=180111, mean_mean=16.075556368641674, mean_std=9.50883910962795), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.0/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         913.627</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">-0.496507</td><td style=\"text-align: right;\">           -0.470752</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">            1741.3</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 184000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-00-49\n",
            "  done: false\n",
            "  episode_len_mean: 1780.87\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4707517833809657\n",
            "  episode_reward_mean: -0.4961597339422664\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 111\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 4.0233135223388675e-08\n",
            "          cur_lr: 9.2e-06\n",
            "          entropy: 0.15625772322528064\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0029236952432256658\n",
            "          policy_loss: -0.02349664125358686\n",
            "          total_loss: 0.09005764324683696\n",
            "          vf_explained_var: 0.5800817012786865\n",
            "          vf_loss: 0.23023372003808618\n",
            "    num_agent_steps_sampled: 184000\n",
            "    num_steps_sampled: 184000\n",
            "    num_steps_trained: 184000\n",
            "  iterations_since_restore: 46\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.526666666666664\n",
            "    ram_util_percent: 15.866666666666664\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04872608914109671\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.1608731873070774\n",
            "    mean_inference_ms: 1.2944867451912774\n",
            "    mean_raw_obs_processing_ms: 0.15765520975495903\n",
            "  time_since_restore: 934.887622833252\n",
            "  time_this_iter_s: 21.261099100112915\n",
            "  time_total_s: 934.887622833252\n",
            "  timers:\n",
            "    learn_throughput: 517.26\n",
            "    learn_time_ms: 7733.05\n",
            "    sample_throughput: 291.744\n",
            "    sample_time_ms: 13710.645\n",
            "    update_time_ms: 1.66\n",
            "  timestamp: 1619352049\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 184000\n",
            "  training_iteration: 46\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:00:49,783\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=184112, mean_mean=16.082351219539827, mean_std=9.495510709890398), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.0/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         934.888</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\">-0.49616</td><td style=\"text-align: right;\">           -0.470752</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           1780.87</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:00:49,819\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': { 'net_worth': 56144.112764585996,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                             'step': 2950},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=-3.317, max=1.487, mean=0.092),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': 1,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': 0.002678571428571308,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:00:49,823\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-5.202, max=5.189, mean=-0.007),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-0.0, max=-0.0, mean=-0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.535, max=-0.535, mean=-0.535)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:00:49,831\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=-17.515, max=66.404, mean=16.814)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:00:49,832\tINFO sampler.py:589 -- Info return from env: {0: {'agent0': {'net_worth': 55976.88892026, 'step': 2951}}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:00:49,832\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=-17.515, max=66.404, mean=16.814)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:00:49,832\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=-3.318, max=1.488, mean=0.07)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:00:50,652\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-7.29, max=7.296, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-2.907, max=-0.0, mean=-0.174),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((200,), dtype=float32, min=0.055, max=1.0, mean=0.892),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.485),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-1.81, max=2.067, mean=-0.028),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=1076897978.0, max=1076897978.0, mean=1076897978.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 2951, 'net_worth': 55976.88892026}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.344, max=6.515, mean=0.103),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.344, max=6.515, mean=0.103),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.145),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=1031.0, max=1031.0, mean=1031.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.145),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-1.067, max=0.916, mean=-0.117)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:00:50,655\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-7.29, max=7.296, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-2.907, max=-0.0, mean=-0.174),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.055, max=1.0, mean=0.892),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.485),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-1.81, max=2.067, mean=-0.028),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=1076897978.0, max=1076897978.0, mean=1076897978.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 2951, 'net_worth': 55976.88892026}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.344, max=6.515, mean=0.103),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.344, max=6.515, mean=0.103),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.145),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=1031.0, max=1031.0, mean=1031.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.145),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-1.067, max=0.916, mean=-0.117)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:00:50,660\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:01:03,477\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:01:03,479\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-7.282, max=7.289, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((128,), dtype=float32, min=-2.883, max=-0.0, mean=-0.203),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((128,), dtype=float32, min=0.056, max=1.0, mean=0.883),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.438),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((128,), dtype=float32, min=-2.898, max=2.946, mean=-0.202),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((128,), dtype=int64, min=1076897978.0, max=1412443636.0, mean=1289235464.703),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((128,), dtype=object, head={'step': 4126, 'net_worth': 36906.061797555}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((128, 25, 3), dtype=float32, min=-10.267, max=8.467, mean=-0.017),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((128, 25, 3), dtype=float32, min=-10.264, max=8.465, mean=-0.018),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.133),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((128,), dtype=int64, min=1031.0, max=1051.0, mean=1040.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.133),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((128,), dtype=float32, min=-1.141, max=1.172, mean=-0.023)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:01:03,491\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 2.0116567611694337e-08,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 9.16e-06,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.17207790911197662,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': -1.3453724800882583e-09,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': 0.20206069946289062,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': 0.33038029074668884,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.551, max=0.551, mean=0.551),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.2600807845592499},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 188000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-01-11\n",
            "  done: false\n",
            "  episode_len_mean: 1820.25\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4707517833809657\n",
            "  episode_reward_mean: -0.4957800122996361\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 112\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 2.0116567611694337e-08\n",
            "          cur_lr: 9.16e-06\n",
            "          entropy: 0.15830970276147127\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0025772353146749083\n",
            "          policy_loss: -0.015737932757474482\n",
            "          total_loss: 0.10312536839046516\n",
            "          vf_explained_var: 0.5752325057983398\n",
            "          vf_loss: 0.24089279770851135\n",
            "    num_agent_steps_sampled: 188000\n",
            "    num_steps_sampled: 188000\n",
            "    num_steps_trained: 188000\n",
            "  iterations_since_restore: 47\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.880645161290325\n",
            "    ram_util_percent: 15.899999999999995\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04872546267300303\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.1678462268462986\n",
            "    mean_inference_ms: 1.2944567134085154\n",
            "    mean_raw_obs_processing_ms: 0.1577062243497999\n",
            "  time_since_restore: 956.1088571548462\n",
            "  time_this_iter_s: 21.22123432159424\n",
            "  time_total_s: 956.1088571548462\n",
            "  timers:\n",
            "    learn_throughput: 519.722\n",
            "    learn_time_ms: 7696.422\n",
            "    sample_throughput: 291.848\n",
            "    sample_time_ms: 13705.761\n",
            "    update_time_ms: 1.637\n",
            "  timestamp: 1619352071\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 188000\n",
            "  training_iteration: 47\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.0/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         956.109</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">-0.49578</td><td style=\"text-align: right;\">           -0.470752</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           1820.25</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:01:11,042\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=188113, mean_mean=16.09058716781393, mean_std=9.478215874578183), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 192000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-01-32\n",
            "  done: false\n",
            "  episode_len_mean: 1859.99\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.46776125857629947\n",
            "  episode_reward_mean: -0.4953579072717236\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 113\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.0058283805847169e-08\n",
            "          cur_lr: 9.12e-06\n",
            "          entropy: 0.15365764405578375\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.003231189191865269\n",
            "          policy_loss: -0.01373151084408164\n",
            "          total_loss: 0.10021894244709983\n",
            "          vf_explained_var: 0.5775837898254395\n",
            "          vf_loss: 0.23097406327724457\n",
            "    num_agent_steps_sampled: 192000\n",
            "    num_steps_sampled: 192000\n",
            "    num_steps_trained: 192000\n",
            "  iterations_since_restore: 48\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.71666666666666\n",
            "    ram_util_percent: 15.899999999999997\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04872443697375475\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.17488650689642\n",
            "    mean_inference_ms: 1.294413905439235\n",
            "    mean_raw_obs_processing_ms: 0.157756548895081\n",
            "  time_since_restore: 977.2494914531708\n",
            "  time_this_iter_s: 21.140634298324585\n",
            "  time_total_s: 977.2494914531708\n",
            "  timers:\n",
            "    learn_throughput: 520.511\n",
            "    learn_time_ms: 7684.751\n",
            "    sample_throughput: 292.281\n",
            "    sample_time_ms: 13685.475\n",
            "    update_time_ms: 1.64\n",
            "  timestamp: 1619352092\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 192000\n",
            "  training_iteration: 48\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         977.249</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\">-0.495358</td><td style=\"text-align: right;\">           -0.467761</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           1859.99</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:01:32,216\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=192114, mean_mean=16.09264301383487, mean_std=9.46476174947684), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 196000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-01-53\n",
            "  done: false\n",
            "  episode_len_mean: 1901.36\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.46776125857629947\n",
            "  episode_reward_mean: -0.4950950818024556\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 114\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 5.029141902923584e-09\n",
            "          cur_lr: 9.080000000000001e-06\n",
            "          entropy: 0.15058489609509706\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.003004074347700225\n",
            "          policy_loss: -0.02331571193644777\n",
            "          total_loss: 0.09650396159850061\n",
            "          vf_explained_var: 0.5563536882400513\n",
            "          vf_loss: 0.2426510462537408\n",
            "    num_agent_steps_sampled: 196000\n",
            "    num_steps_sampled: 196000\n",
            "    num_steps_trained: 196000\n",
            "  iterations_since_restore: 49\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.841935483870962\n",
            "    ram_util_percent: 15.899999999999995\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04872329360206216\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.1820106671317085\n",
            "    mean_inference_ms: 1.2943634197386036\n",
            "    mean_raw_obs_processing_ms: 0.15780672365454643\n",
            "  time_since_restore: 998.968512058258\n",
            "  time_this_iter_s: 21.71902060508728\n",
            "  time_total_s: 998.968512058258\n",
            "  timers:\n",
            "    learn_throughput: 521.796\n",
            "    learn_time_ms: 7665.834\n",
            "    sample_throughput: 291.795\n",
            "    sample_time_ms: 13708.257\n",
            "    update_time_ms: 1.646\n",
            "  timestamp: 1619352113\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 196000\n",
            "  training_iteration: 49\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         998.969</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">-0.495095</td><td style=\"text-align: right;\">           -0.467761</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           1901.36</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:01:53,971\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=196115, mean_mean=16.097423376861734, mean_std=9.447987488892283), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:01:54,004\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': { 'net_worth': 66200.2166160999,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                             'step': 2001},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=-1.344, max=1.861, mean=0.086),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': 1,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': 0.0002726809365711791,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:01:54,007\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-2.07, max=2.081, mean=0.005),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-0.016, max=-0.016, mean=-0.016),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.984, max=0.984, mean=0.984),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.795, max=-0.795, mean=-0.795)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:01:54,012\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=-10.701, max=54.608, mean=17.184)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:01:54,012\tINFO sampler.py:589 -- Info return from env: { 0: { 'agent0': { 'net_worth': 65712.99298847701,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                    'step': 2002}}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:01:54,012\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=-10.701, max=54.608, mean=17.184)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:01:54,013\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=-1.345, max=1.861, mean=0.031)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:01:55,501\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-7.919, max=7.909, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-4.295, max=0.0, mean=-0.177),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((200,), dtype=float32, min=0.014, max=1.0, mean=0.908),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.4),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-1.261, max=1.133, mean=0.029),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=997399408.0, max=997399408.0, mean=997399408.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 2002, 'net_worth': 65712.99298847701}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-5.553, max=8.328, mean=-0.146),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((200, 25, 3), dtype=float32, min=-5.553, max=8.328, mean=-0.143),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.05),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=1094.0, max=1094.0, mean=1094.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.05),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-1.254, max=1.115, mean=-0.079)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:01:55,503\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-7.919, max=7.909, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-4.295, max=0.0, mean=-0.177),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.014, max=1.0, mean=0.908),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.4),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-1.261, max=1.133, mean=0.029),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=997399408.0, max=997399408.0, mean=997399408.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 2002, 'net_worth': 65712.99298847701}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-5.553, max=8.328, mean=-0.146),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-5.553, max=8.328, mean=-0.143),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.05),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=1094.0, max=1094.0, mean=1094.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.05),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-1.254, max=1.115, mean=-0.079)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:01:55,508\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:02:08,930\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:02:08,933\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-13.144, max=13.149, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((128,), dtype=float32, min=-3.261, max=0.0, mean=-0.182),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((128,), dtype=float32, min=0.038, max=1.0, mean=0.891),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.5),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((128,), dtype=float32, min=-2.431, max=2.596, mean=0.057),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((128,), dtype=int64, min=997399408.0, max=1042987798.0, mean=1014851213.547),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((128,), dtype=object, head={'step': 324, 'net_worth': 65128.86}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((128, 25, 3), dtype=float32, min=-5.978, max=8.496, mean=-0.005),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((128, 25, 3), dtype=float32, min=-10.297, max=8.493, mean=-0.005),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.109),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((128,), dtype=int64, min=1094.0, max=1114.0, mean=1103.508),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.109),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((128,), dtype=float32, min=-1.232, max=1.057, mean=-0.14)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:02:08,943\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 2.514570951461792e-09,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 9.040000000000002e-06,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.16919763386249542,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': -4.97697127954666e-09,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': -0.05722539499402046,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': 0.06122405081987381,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.599, max=0.599, mean=0.599),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.24028287827968597},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 200000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-02-16\n",
            "  done: false\n",
            "  episode_len_mean: 1942.63\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.46776125857629947\n",
            "  episode_reward_mean: -0.49476412229509825\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 115\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 2.514570951461792e-09\n",
            "          cur_lr: 9.040000000000002e-06\n",
            "          entropy: 0.1524198658298701\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.003352242918481352\n",
            "          policy_loss: -0.01671355467988178\n",
            "          total_loss: 0.101104564324487\n",
            "          vf_explained_var: 0.5594030618667603\n",
            "          vf_loss: 0.23868463514372706\n",
            "    num_agent_steps_sampled: 200000\n",
            "    num_steps_sampled: 200000\n",
            "    num_steps_trained: 200000\n",
            "  iterations_since_restore: 50\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 29.040625\n",
            "    ram_util_percent: 15.9\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04872219141255729\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.189242542514012\n",
            "    mean_inference_ms: 1.2943104424660246\n",
            "    mean_raw_obs_processing_ms: 0.15785821565430658\n",
            "  time_since_restore: 1021.5845701694489\n",
            "  time_this_iter_s: 22.616058111190796\n",
            "  time_total_s: 1021.5845701694489\n",
            "  timers:\n",
            "    learn_throughput: 522.32\n",
            "    learn_time_ms: 7658.139\n",
            "    sample_throughput: 289.247\n",
            "    sample_time_ms: 13829.007\n",
            "    update_time_ms: 1.602\n",
            "  timestamp: 1619352136\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 200000\n",
            "  training_iteration: 50\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:02:16,622\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=200116, mean_mean=16.103779260830787, mean_std=9.429630486632778), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         1021.58</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\">-0.494764</td><td style=\"text-align: right;\">           -0.467761</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           1942.63</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 204000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-02-38\n",
            "  done: false\n",
            "  episode_len_mean: 1983.64\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.46776125857629947\n",
            "  episode_reward_mean: -0.4944042694744404\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 116\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.257285475730896e-09\n",
            "          cur_lr: 9.0e-06\n",
            "          entropy: 0.15199089096859097\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.002534698727686191\n",
            "          policy_loss: -0.01708618929842487\n",
            "          total_loss: 0.09978402010165155\n",
            "          vf_explained_var: 0.5644137263298035\n",
            "          vf_loss: 0.23678022995591164\n",
            "    num_agent_steps_sampled: 204000\n",
            "    num_steps_sampled: 204000\n",
            "    num_steps_trained: 204000\n",
            "  iterations_since_restore: 51\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.825\n",
            "    ram_util_percent: 15.9\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04872100412181882\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.1965579720733255\n",
            "    mean_inference_ms: 1.2942490156739817\n",
            "    mean_raw_obs_processing_ms: 0.1579096060578823\n",
            "  time_since_restore: 1043.493170261383\n",
            "  time_this_iter_s: 21.908600091934204\n",
            "  time_total_s: 1043.493170261383\n",
            "  timers:\n",
            "    learn_throughput: 522.315\n",
            "    learn_time_ms: 7658.21\n",
            "    sample_throughput: 290.247\n",
            "    sample_time_ms: 13781.348\n",
            "    update_time_ms: 1.604\n",
            "  timestamp: 1619352158\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 204000\n",
            "  training_iteration: 51\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         1043.49</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">-0.494404</td><td style=\"text-align: right;\">           -0.467761</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           1983.64</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:02:38,567\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=204117, mean_mean=16.105855920099465, mean_std=9.412967101913958), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 208000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-03-00\n",
            "  done: false\n",
            "  episode_len_mean: 2023.11\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4671570434883884\n",
            "  episode_reward_mean: -0.4940087074808764\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 117\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 6.28642737865448e-10\n",
            "          cur_lr: 8.96e-06\n",
            "          entropy: 0.1471276672091335\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0027855907210323494\n",
            "          policy_loss: -0.023602281173225492\n",
            "          total_loss: 0.08753579936455935\n",
            "          vf_explained_var: 0.5980697870254517\n",
            "          vf_loss: 0.2252187067642808\n",
            "    num_agent_steps_sampled: 208000\n",
            "    num_steps_sampled: 208000\n",
            "    num_steps_trained: 208000\n",
            "  iterations_since_restore: 52\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.98387096774194\n",
            "    ram_util_percent: 15.899999999999995\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.048719763039928504\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.2039459567504098\n",
            "    mean_inference_ms: 1.2941813627527017\n",
            "    mean_raw_obs_processing_ms: 0.15796125723570312\n",
            "  time_since_restore: 1065.1686046123505\n",
            "  time_this_iter_s: 21.675434350967407\n",
            "  time_total_s: 1065.1686046123505\n",
            "  timers:\n",
            "    learn_throughput: 523.273\n",
            "    learn_time_ms: 7644.189\n",
            "    sample_throughput: 289.511\n",
            "    sample_time_ms: 13816.394\n",
            "    update_time_ms: 1.588\n",
            "  timestamp: 1619352180\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 208000\n",
            "  training_iteration: 52\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:03:00,281\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=208118, mean_mean=16.109929719447262, mean_std=9.39888376063385), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         1065.17</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\">-0.494009</td><td style=\"text-align: right;\">           -0.467157</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           2023.11</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:03:00,317\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': { 'net_worth': 65413.00719788801,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                             'step': 943},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=-5.338, max=2.273, mean=-0.666),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': 1,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': 0.0034496852775280917,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:03:00,320\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-2.443, max=2.441, mean=-0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-0.008, max=-0.008, mean=-0.008),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.992, max=0.992, mean=0.992),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.819, max=-0.819, mean=-0.819)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:03:00,323\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=-51.592, max=40.787, mean=9.324)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:03:00,323\tINFO sampler.py:589 -- Info return from env: {0: {'agent0': {'net_worth': 65017.604668065, 'step': 944}}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:03:00,324\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=-51.592, max=40.787, mean=9.324)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:03:00,324\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=-3.534, max=2.274, mean=-0.525)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:03:00,897\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-9.368, max=9.359, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-4.631, max=0.0, mean=-0.152),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((200,), dtype=float32, min=0.01, max=1.0, mean=0.912),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.445),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-1.783, max=0.885, mean=-0.026),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=277257156.0, max=277257156.0, mean=277257156.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 944, 'net_worth': 65017.604668065}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.275, max=11.078, mean=0.091),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((200, 25, 3), dtype=float32, min=-5.338, max=11.078, mean=0.087),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.07),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=1157.0, max=1157.0, mean=1157.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.07),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-1.242, max=1.105, mean=-0.044)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:03:00,900\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-9.368, max=9.359, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-4.631, max=0.0, mean=-0.152),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.01, max=1.0, mean=0.912),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.445),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-1.783, max=0.885, mean=-0.026),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=277257156.0, max=277257156.0, mean=277257156.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 944, 'net_worth': 65017.604668065}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.275, max=11.078, mean=0.091),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-5.338, max=11.078, mean=0.087),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.07),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=1157.0, max=1157.0, mean=1157.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.07),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-1.242, max=1.105, mean=-0.044)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:03:00,905\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:03:14,806\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:03:14,809\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-8.708, max=8.712, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((128,), dtype=float32, min=-3.987, max=0.0, mean=-0.223),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((128,), dtype=float32, min=0.019, max=1.0, mean=0.885),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.398),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((128,), dtype=float32, min=-3.287, max=3.266, mean=-0.059),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((128,), dtype=int64, min=277257156.0, max=712124241.0, mean=335012940.727),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((128,), dtype=object, head={'step': 2820, 'net_worth': 67188.38}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((128, 25, 3), dtype=float32, min=-5.894, max=9.784, mean=0.006),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((128, 25, 3), dtype=float32, min=-5.909, max=11.078, mean=0.008),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.102),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((128,), dtype=int64, min=1157.0, max=1177.0, mean=1167.078),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.102),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((128,), dtype=float32, min=-1.163, max=1.105, mean=-0.09)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:03:14,820\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 3.14321368932724e-10,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 8.920000000000001e-06,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.15325422585010529,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': -3.7657532647727976e-10,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': 0.058542925864458084,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': 0.18465864658355713,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.487, max=0.487, mean=0.487),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.255296528339386},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 212000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-03-22\n",
            "  done: false\n",
            "  episode_len_mean: 2065.72\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4671570434883884\n",
            "  episode_reward_mean: -0.49355800967723146\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 118\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 3.14321368932724e-10\n",
            "          cur_lr: 8.920000000000001e-06\n",
            "          entropy: 0.1428791806101799\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.004222192059387453\n",
            "          policy_loss: -0.017345796193694696\n",
            "          total_loss: 0.0992089052888332\n",
            "          vf_explained_var: 0.5621764659881592\n",
            "          vf_loss: 0.23596698325127363\n",
            "    num_agent_steps_sampled: 212000\n",
            "    num_steps_sampled: 212000\n",
            "    num_steps_trained: 212000\n",
            "  iterations_since_restore: 53\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 29.025806451612898\n",
            "    ram_util_percent: 15.899999999999995\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.048718353754546445\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.211427223146511\n",
            "    mean_inference_ms: 1.2941061685085429\n",
            "    mean_raw_obs_processing_ms: 0.15801321118058212\n",
            "  time_since_restore: 1087.249549627304\n",
            "  time_this_iter_s: 22.080945014953613\n",
            "  time_total_s: 1087.249549627304\n",
            "  timers:\n",
            "    learn_throughput: 524.851\n",
            "    learn_time_ms: 7621.216\n",
            "    sample_throughput: 287.273\n",
            "    sample_time_ms: 13924.056\n",
            "    update_time_ms: 1.597\n",
            "  timestamp: 1619352202\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 212000\n",
            "  training_iteration: 53\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         1087.25</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">-0.493558</td><td style=\"text-align: right;\">           -0.467157</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           2065.72</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:03:22,400\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=212119, mean_mean=16.115385085440646, mean_std=9.385062097651282), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 216000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-03-44\n",
            "  done: false\n",
            "  episode_len_mean: 2065.72\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4671570434883884\n",
            "  episode_reward_mean: -0.49355800967723146\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 118\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.57160684466362e-10\n",
            "          cur_lr: 8.880000000000001e-06\n",
            "          entropy: 0.1465684473514557\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0036299716230132617\n",
            "          policy_loss: -0.020722023997223005\n",
            "          total_loss: 0.09041071333922446\n",
            "          vf_explained_var: 0.5900181531906128\n",
            "          vf_loss: 0.2251968397758901\n",
            "    num_agent_steps_sampled: 216000\n",
            "    num_steps_sampled: 216000\n",
            "    num_steps_trained: 216000\n",
            "  iterations_since_restore: 54\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.9625\n",
            "    ram_util_percent: 15.9\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.048718353754546445\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.211427223146511\n",
            "    mean_inference_ms: 1.2941061685085429\n",
            "    mean_raw_obs_processing_ms: 0.1580132111805821\n",
            "  time_since_restore: 1109.3657307624817\n",
            "  time_this_iter_s: 22.116181135177612\n",
            "  time_total_s: 1109.3657307624817\n",
            "  timers:\n",
            "    learn_throughput: 522.812\n",
            "    learn_time_ms: 7650.939\n",
            "    sample_throughput: 285.511\n",
            "    sample_time_ms: 14009.98\n",
            "    update_time_ms: 1.611\n",
            "  timestamp: 1619352224\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 216000\n",
            "  training_iteration: 54\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:03:44,553\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=216119, mean_mean=16.12220826541776, mean_std=9.369295611554683), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         1109.37</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\">-0.493558</td><td style=\"text-align: right;\">           -0.467157</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           2065.72</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 220000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-04-05\n",
            "  done: false\n",
            "  episode_len_mean: 2106.86\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4671570434883884\n",
            "  episode_reward_mean: -0.49313777742185955\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 119\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 7.8580342233181e-11\n",
            "          cur_lr: 8.84e-06\n",
            "          entropy: 0.1473676902242005\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0036951277579646558\n",
            "          policy_loss: -0.017436651192838326\n",
            "          total_loss: 0.08134042873280123\n",
            "          vf_explained_var: 0.6500146389007568\n",
            "          vf_loss: 0.20050150388851762\n",
            "    num_agent_steps_sampled: 220000\n",
            "    num_steps_sampled: 220000\n",
            "    num_steps_trained: 220000\n",
            "  iterations_since_restore: 55\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.946666666666665\n",
            "    ram_util_percent: 15.94\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04871669068675868\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.219034857087258\n",
            "    mean_inference_ms: 1.2940158512983462\n",
            "    mean_raw_obs_processing_ms: 0.15806438678957646\n",
            "  time_since_restore: 1130.6051094532013\n",
            "  time_this_iter_s: 21.239378690719604\n",
            "  time_total_s: 1130.6051094532013\n",
            "  timers:\n",
            "    learn_throughput: 521.722\n",
            "    learn_time_ms: 7666.916\n",
            "    sample_throughput: 285.242\n",
            "    sample_time_ms: 14023.174\n",
            "    update_time_ms: 1.627\n",
            "  timestamp: 1619352245\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 220000\n",
            "  training_iteration: 55\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:04:05,832\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=220120, mean_mean=16.126480963122248, mean_std=9.36165744205659), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         1130.61</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">-0.493138</td><td style=\"text-align: right;\">           -0.467157</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           2106.86</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:04:05,867\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': { 'net_worth': 39490.9,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                             'step': 3995},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=-1.187, max=0.316, mean=-0.199),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:04:05,870\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-6.946, max=6.936, mean=-0.005),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-0.0, max=-0.0, mean=-0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.089, max=0.089, mean=0.089)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:04:05,878\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=-5.564, max=48.058, mean=13.704)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:04:05,878\tINFO sampler.py:589 -- Info return from env: {0: {'agent0': {'net_worth': 39490.9, 'step': 3996}}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:04:05,878\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=-5.564, max=48.058, mean=13.704)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:04:05,879\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=-1.181, max=0.316, mean=-0.192)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:04:06,810\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-9.811, max=9.807, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-4.005, max=0.0, mean=-0.143),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((200,), dtype=float32, min=0.018, max=1.0, mean=0.915),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.335),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-1.502, max=1.336, mean=-0.068),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=1777475764.0, max=1777475764.0, mean=1777475764.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 3996, 'net_worth': 39490.9}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-2.072, max=3.422, mean=0.005),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((200, 25, 3), dtype=float32, min=-2.072, max=3.422, mean=0.004),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.13),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=1219.0, max=1219.0, mean=1219.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.13),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-0.903, max=0.64, mean=-0.062)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:04:06,812\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-9.811, max=9.807, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-4.005, max=0.0, mean=-0.143),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.018, max=1.0, mean=0.915),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.335),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-1.502, max=1.336, mean=-0.068),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=1777475764.0, max=1777475764.0, mean=1777475764.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 3996, 'net_worth': 39490.9}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-2.072, max=3.422, mean=0.005),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-2.072, max=3.422, mean=0.004),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.13),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=1219.0, max=1219.0, mean=1219.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.13),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-0.903, max=0.64, mean=-0.062)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:04:06,817\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:04:20,475\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:04:20,479\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-8.71, max=8.718, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((128,), dtype=float32, min=-5.544, max=0.0, mean=-0.183),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((128,), dtype=float32, min=0.004, max=1.0, mean=0.905),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.445),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((128,), dtype=float32, min=-4.208, max=2.536, mean=0.117),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((128,), dtype=int64, min=1329299978.0, max=1777475764.0, mean=1378319204.594),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((128,), dtype=object, head={'step': 1459, 'net_worth': 67127.84}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((128, 25, 3), dtype=float32, min=-10.325, max=9.808, mean=0.053),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((128, 25, 3), dtype=float32, min=-10.324, max=9.805, mean=0.047),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=0.062),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((128,), dtype=int64, min=1219.0, max=1239.0, mean=1229.508),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=0.062),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((128,), dtype=float32, min=-1.09, max=1.183, mean=-0.008)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:04:20,489\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 3.92901711165905e-11,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 8.8e-06,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.13714538514614105,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': 4.384570484461392e-10,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': -0.11731962859630585,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': -0.008724726736545563,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.626, max=0.626, mean=0.626),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.2199327051639557},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 224000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-04-28\n",
            "  done: false\n",
            "  episode_len_mean: 2147.39\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4671570434883884\n",
            "  episode_reward_mean: -0.4927828468246671\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 120\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 3.92901711165905e-11\n",
            "          cur_lr: 8.8e-06\n",
            "          entropy: 0.1454922624398023\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.00360899735824205\n",
            "          policy_loss: -0.016894764528842643\n",
            "          total_loss: 0.08476754516595975\n",
            "          vf_explained_var: 0.6367388963699341\n",
            "          vf_loss: 0.20623446349054575\n",
            "    num_agent_steps_sampled: 224000\n",
            "    num_steps_sampled: 224000\n",
            "    num_steps_trained: 224000\n",
            "  iterations_since_restore: 56\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 29.1375\n",
            "    ram_util_percent: 15.996875\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.048715050581311384\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.2266918155972344\n",
            "    mean_inference_ms: 1.293959115487469\n",
            "    mean_raw_obs_processing_ms: 0.15811574097560166\n",
            "  time_since_restore: 1152.9603354930878\n",
            "  time_this_iter_s: 22.355226039886475\n",
            "  time_total_s: 1152.9603354930878\n",
            "  timers:\n",
            "    learn_throughput: 520.997\n",
            "    learn_time_ms: 7677.593\n",
            "    sample_throughput: 283.249\n",
            "    sample_time_ms: 14121.847\n",
            "    update_time_ms: 1.646\n",
            "  timestamp: 1619352268\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 224000\n",
            "  training_iteration: 56\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:04:28,224\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=224121, mean_mean=16.131195038204233, mean_std=9.352154318768742), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         1152.96</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\">-0.492783</td><td style=\"text-align: right;\">           -0.467157</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           2147.39</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 228000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-04-49\n",
            "  done: false\n",
            "  episode_len_mean: 2189.9\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4671570434883884\n",
            "  episode_reward_mean: -0.4923080365946554\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 121\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.964508555829525e-11\n",
            "          cur_lr: 8.76e-06\n",
            "          entropy: 0.1416155705228448\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0036547693598549813\n",
            "          policy_loss: -0.025590552133508027\n",
            "          total_loss: 0.07171357481274754\n",
            "          vf_explained_var: 0.6365674138069153\n",
            "          vf_loss: 0.1974405637010932\n",
            "    num_agent_steps_sampled: 228000\n",
            "    num_steps_sampled: 228000\n",
            "    num_steps_trained: 228000\n",
            "  iterations_since_restore: 57\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.919354838709676\n",
            "    ram_util_percent: 16.0\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04871336611611149\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.2343994256186013\n",
            "    mean_inference_ms: 1.2938961429063733\n",
            "    mean_raw_obs_processing_ms: 0.15816711851415055\n",
            "  time_since_restore: 1174.3777830600739\n",
            "  time_this_iter_s: 21.417447566986084\n",
            "  time_total_s: 1174.3777830600739\n",
            "  timers:\n",
            "    learn_throughput: 520.564\n",
            "    learn_time_ms: 7683.98\n",
            "    sample_throughput: 282.984\n",
            "    sample_time_ms: 14135.086\n",
            "    update_time_ms: 1.676\n",
            "  timestamp: 1619352289\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 228000\n",
            "  training_iteration: 57\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:04:49,681\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=228122, mean_mean=16.13378142296834, mean_std=9.346406244648122), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         1174.38</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">-0.492308</td><td style=\"text-align: right;\">           -0.467157</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">            2189.9</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 232000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-05-11\n",
            "  done: false\n",
            "  episode_len_mean: 2231.77\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4670292687255444\n",
            "  episode_reward_mean: -0.49186320351424734\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 122\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 9.822542779147626e-12\n",
            "          cur_lr: 8.720000000000001e-06\n",
            "          entropy: 0.14360425504855812\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0035488843714119866\n",
            "          policy_loss: -0.01427240518387407\n",
            "          total_loss: 0.09352772435522638\n",
            "          vf_explained_var: 0.62073814868927\n",
            "          vf_loss: 0.21847234340384603\n",
            "    num_agent_steps_sampled: 232000\n",
            "    num_steps_sampled: 232000\n",
            "    num_steps_trained: 232000\n",
            "  iterations_since_restore: 58\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.935483870967747\n",
            "    ram_util_percent: 16.0\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.048711663885471816\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.2421601299624898\n",
            "    mean_inference_ms: 1.293828608131377\n",
            "    mean_raw_obs_processing_ms: 0.15821850149967379\n",
            "  time_since_restore: 1195.9907507896423\n",
            "  time_this_iter_s: 21.61296772956848\n",
            "  time_total_s: 1195.9907507896423\n",
            "  timers:\n",
            "    learn_throughput: 520.012\n",
            "    learn_time_ms: 7692.129\n",
            "    sample_throughput: 282.203\n",
            "    sample_time_ms: 14174.171\n",
            "    update_time_ms: 1.703\n",
            "  timestamp: 1619352311\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 232000\n",
            "  training_iteration: 58\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:05:11,333\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=232123, mean_mean=16.136766520683803, mean_std=9.335550339292032), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         1195.99</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\">-0.491863</td><td style=\"text-align: right;\">           -0.467029</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           2231.77</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:05:11,369\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': { 'net_worth': 62225.48233396601,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                             'step': 2702},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=-1.204, max=1.297, mean=0.288),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': 1,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': 0.0017811012916384428,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:05:11,373\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-0.6, max=0.592, mean=-0.004),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-1.457, max=-1.457, mean=-1.457),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.233, max=0.233, mean=0.233),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.479, max=-0.479, mean=-0.479)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:05:11,380\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=-4.869, max=64.166, mean=19.449)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:05:11,380\tINFO sampler.py:589 -- Info return from env: {0: {'agent0': {'net_worth': 62038.81, 'step': 2703}}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:05:11,380\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=-4.869, max=64.166, mean=19.449)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:05:11,381\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=-1.205, max=1.293, mean=0.285)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:05:12,157\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-11.609, max=11.608, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-4.101, max=0.0, mean=-0.152),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((200,), dtype=float32, min=0.017, max=1.0, mean=0.919),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.32),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-1.591, max=1.361, mean=-0.031),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=1118227834.0, max=1118227834.0, mean=1118227834.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 2703, 'net_worth': 62038.81}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-6.028, max=4.906, mean=-0.193),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((200, 25, 3), dtype=float32, min=-6.028, max=4.906, mean=-0.193),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.115),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=1282.0, max=1282.0, mean=1282.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.115),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-1.076, max=0.986, mean=-0.084)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:05:12,159\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-11.609, max=11.608, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-4.101, max=0.0, mean=-0.152),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.017, max=1.0, mean=0.919),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.32),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-1.591, max=1.361, mean=-0.031),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=1118227834.0, max=1118227834.0, mean=1118227834.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 2703, 'net_worth': 62038.81}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-6.028, max=4.906, mean=-0.193),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-6.028, max=4.906, mean=-0.193),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.115),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=1282.0, max=1282.0, mean=1282.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.115),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-1.076, max=0.986, mean=-0.084)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:05:12,164\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:05:25,292\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:05:25,294\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-10.138, max=10.146, mean=-0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((128,), dtype=float32, min=-3.516, max=0.0, mean=-0.128),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((128,), dtype=float32, min=0.03, max=1.0, mean=0.915),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.406),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((128,), dtype=float32, min=-4.492, max=2.849, mean=0.035),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((128,), dtype=int64, min=1118227834.0, max=1124602013.0, mean=1121514520.047),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((128,), dtype=object, head={'step': 4161, 'net_worth': 40310.3}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((128, 25, 3), dtype=float32, min=-10.342, max=11.073, mean=-0.002),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((128, 25, 3), dtype=float32, min=-10.339, max=11.07, mean=-0.007),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=0.047),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((128,), dtype=int64, min=1282.0, max=1302.0, mean=1291.273),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=0.047),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((128,), dtype=float32, min=-1.117, max=1.177, mean=0.034)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:05:25,304\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 4.911271389573813e-12,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 8.680000000000002e-06,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.16635164618492126,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': 3.4562446238339817e-09,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': -0.03467255085706711,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': 0.08286258578300476,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.506, max=0.506, mean=0.506),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.23839734494686127},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 236000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-05-33\n",
            "  done: false\n",
            "  episode_len_mean: 2271.79\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4670292687255444\n",
            "  episode_reward_mean: -0.4916980505770009\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 123\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 4.911271389573813e-12\n",
            "          cur_lr: 8.680000000000002e-06\n",
            "          entropy: 0.13951950403861701\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0033807925283326767\n",
            "          policy_loss: -0.019438983435975388\n",
            "          total_loss: 0.080726125248475\n",
            "          vf_explained_var: 0.626551628112793\n",
            "          vf_loss: 0.2031206046231091\n",
            "    num_agent_steps_sampled: 236000\n",
            "    num_steps_sampled: 236000\n",
            "    num_steps_trained: 236000\n",
            "  iterations_since_restore: 59\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 29.032258064516128\n",
            "    ram_util_percent: 16.0\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04870990530424582\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.249973845506196\n",
            "    mean_inference_ms: 1.2937549783110978\n",
            "    mean_raw_obs_processing_ms: 0.15826992060037884\n",
            "  time_since_restore: 1217.724274635315\n",
            "  time_this_iter_s: 21.733523845672607\n",
            "  time_total_s: 1217.724274635315\n",
            "  timers:\n",
            "    learn_throughput: 519.255\n",
            "    learn_time_ms: 7703.343\n",
            "    sample_throughput: 282.398\n",
            "    sample_time_ms: 14164.414\n",
            "    update_time_ms: 1.706\n",
            "  timestamp: 1619352333\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 236000\n",
            "  training_iteration: 59\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:05:33,104\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=236124, mean_mean=16.137489288792214, mean_std=9.325469169401892), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         1217.72</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">-0.491698</td><td style=\"text-align: right;\">           -0.467029</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           2271.79</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 240000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-05-55\n",
            "  done: false\n",
            "  episode_len_mean: 2313.03\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4670292687255444\n",
            "  episode_reward_mean: -0.49135377712837053\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 124\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 2.4556356947869064e-12\n",
            "          cur_lr: 8.64e-06\n",
            "          entropy: 0.1400114931166172\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0029326197054615477\n",
            "          policy_loss: -0.02294024938601069\n",
            "          total_loss: 0.0821608372789342\n",
            "          vf_explained_var: 0.6022675037384033\n",
            "          vf_loss: 0.2130024046637118\n",
            "    num_agent_steps_sampled: 240000\n",
            "    num_steps_sampled: 240000\n",
            "    num_steps_trained: 240000\n",
            "  iterations_since_restore: 60\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.993548387096773\n",
            "    ram_util_percent: 16.0\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.048708195704142636\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.2578478876853452\n",
            "    mean_inference_ms: 1.2936777964629198\n",
            "    mean_raw_obs_processing_ms: 0.1583216739059738\n",
            "  time_since_restore: 1239.717229604721\n",
            "  time_this_iter_s: 21.992954969406128\n",
            "  time_total_s: 1239.717229604721\n",
            "  timers:\n",
            "    learn_throughput: 518.617\n",
            "    learn_time_ms: 7712.819\n",
            "    sample_throughput: 283.837\n",
            "    sample_time_ms: 14092.619\n",
            "    update_time_ms: 1.731\n",
            "  timestamp: 1619352355\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 240000\n",
            "  training_iteration: 60\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:05:55,137\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=240125, mean_mean=16.142115344815544, mean_std=9.312749001016988), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         1239.72</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\">-0.491354</td><td style=\"text-align: right;\">           -0.467029</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           2313.03</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 244000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-06-17\n",
            "  done: false\n",
            "  episode_len_mean: 2355.49\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4670292687255444\n",
            "  episode_reward_mean: -0.49106814086178796\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 125\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.2278178473934532e-12\n",
            "          cur_lr: 8.6e-06\n",
            "          entropy: 0.1405585682950914\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.002532709466322558\n",
            "          policy_loss: -0.017392116540577263\n",
            "          total_loss: 0.08543775443104096\n",
            "          vf_explained_var: 0.6028314828872681\n",
            "          vf_loss: 0.20847090892493725\n",
            "    num_agent_steps_sampled: 244000\n",
            "    num_steps_sampled: 244000\n",
            "    num_steps_trained: 244000\n",
            "  iterations_since_restore: 61\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.909375\n",
            "    ram_util_percent: 16.0\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04870646135790577\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.2657911491775053\n",
            "    mean_inference_ms: 1.293597249965761\n",
            "    mean_raw_obs_processing_ms: 0.15837360114505744\n",
            "  time_since_restore: 1262.027316570282\n",
            "  time_this_iter_s: 22.310086965560913\n",
            "  time_total_s: 1262.027316570282\n",
            "  timers:\n",
            "    learn_throughput: 518.14\n",
            "    learn_time_ms: 7719.917\n",
            "    sample_throughput: 283.172\n",
            "    sample_time_ms: 14125.678\n",
            "    update_time_ms: 1.739\n",
            "  timestamp: 1619352377\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 244000\n",
            "  training_iteration: 61\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:06:17,489\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=244126, mean_mean=16.14144746219209, mean_std=9.299172211183755), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         1262.03</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">-0.491068</td><td style=\"text-align: right;\">           -0.467029</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           2355.49</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:06:17,527\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': { 'net_worth': 71711.888962802,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                             'step': 1347},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=-3.575, max=0.705, mean=-0.293),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': 1,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': -0.022452137504171854,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:06:17,530\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-2.4, max=2.408, mean=0.004),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-0.008, max=-0.008, mean=-0.008),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.992, max=0.992, mean=0.992),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.555, max=0.555, mean=0.555)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:06:17,535\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=-26.109, max=55.726, mean=12.605)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:06:17,535\tINFO sampler.py:589 -- Info return from env: { 0: { 'agent0': { 'net_worth': 72004.576923279,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                    'step': 1348}}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:06:17,536\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=-26.109, max=55.726, mean=12.605)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:06:17,536\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=-3.579, max=0.705, mean=-0.317)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:06:18,157\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-7.802, max=7.814, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-5.356, max=0.0, mean=-0.105),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((200,), dtype=float32, min=0.005, max=1.0, mean=0.945),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.35),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-1.345, max=1.249, mean=0.045),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=1065593389.0, max=1065593389.0, mean=1065593389.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 1348, 'net_worth': 72004.576923279}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-3.6, max=9.853, mean=0.193),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((200, 25, 3), dtype=float32, min=-3.6, max=9.851, mean=0.185),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=0.03),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=1345.0, max=1345.0, mean=1345.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=0.03),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-1.195, max=1.159, mean=-0.015)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:06:18,160\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-7.802, max=7.814, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-5.356, max=0.0, mean=-0.105),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.005, max=1.0, mean=0.945),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.35),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-1.345, max=1.249, mean=0.045),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=1065593389.0, max=1065593389.0, mean=1065593389.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 1348, 'net_worth': 72004.576923279}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-3.6, max=9.853, mean=0.193),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-3.6, max=9.851, mean=0.185),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=0.03),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=1345.0, max=1345.0, mean=1345.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=0.03),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-1.195, max=1.159, mean=-0.015)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:06:18,165\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:06:32,013\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:06:32,016\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-8.214, max=8.212, mean=-0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((128,), dtype=float32, min=-5.229, max=0.0, mean=-0.191),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((128,), dtype=float32, min=0.005, max=1.0, mean=0.902),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.469),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((128,), dtype=float32, min=-4.523, max=2.874, mean=-0.041),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((128,), dtype=int64, min=117150118.0, max=1065593389.0, mean=791434005.977),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((128,), dtype=object, head={'step': 3364, 'net_worth': 56352.09}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((128, 25, 3), dtype=float32, min=-12.147, max=16.319, mean=0.031),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((128, 25, 3), dtype=float32, min=-12.142, max=16.316, mean=0.027),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.016),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((128,), dtype=int64, min=1345.0, max=1365.0, mean=1356.086),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.016),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((128,), dtype=float32, min=-1.191, max=1.18, mean=-0.006)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:06:32,026\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 6.139089236967266e-13,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 8.560000000000001e-06,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.13605575263500214,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': -3.2104334746207996e-09,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': 0.041004449129104614,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': 0.1846550703048706,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.484, max=0.484, mean=0.484),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.2900223731994629},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 248000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-06-39\n",
            "  done: false\n",
            "  episode_len_mean: 2397.03\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.46692169871988376\n",
            "  episode_reward_mean: -0.4906355071265684\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 126\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 6.139089236967266e-13\n",
            "          cur_lr: 8.560000000000001e-06\n",
            "          entropy: 0.14100880548357964\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0031164652500592638\n",
            "          policy_loss: -0.021574786747805774\n",
            "          total_loss: 0.0816531361779198\n",
            "          vf_explained_var: 0.610784113407135\n",
            "          vf_loss: 0.20927602518349886\n",
            "    num_agent_steps_sampled: 248000\n",
            "    num_steps_sampled: 248000\n",
            "    num_steps_trained: 248000\n",
            "  iterations_since_restore: 62\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 29.140625\n",
            "    ram_util_percent: 16.0\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04870476283163025\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.2738017486804647\n",
            "    mean_inference_ms: 1.2935134267638886\n",
            "    mean_raw_obs_processing_ms: 0.15842591961775362\n",
            "  time_since_restore: 1284.3641839027405\n",
            "  time_this_iter_s: 22.336867332458496\n",
            "  time_total_s: 1284.3641839027405\n",
            "  timers:\n",
            "    learn_throughput: 516.795\n",
            "    learn_time_ms: 7740.013\n",
            "    sample_throughput: 282.253\n",
            "    sample_time_ms: 14171.678\n",
            "    update_time_ms: 1.759\n",
            "  timestamp: 1619352399\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 248000\n",
            "  training_iteration: 62\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         1284.36</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\">-0.490636</td><td style=\"text-align: right;\">           -0.466922</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           2397.03</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:06:39,865\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=248127, mean_mean=16.142918471214063, mean_std=9.289228834386797), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 252000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-07-02\n",
            "  done: false\n",
            "  episode_len_mean: 2438.16\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.46692169871988376\n",
            "  episode_reward_mean: -0.49021919454974244\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 127\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 3.069544618483633e-13\n",
            "          cur_lr: 8.520000000000001e-06\n",
            "          entropy: 0.13412068434990942\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0036986586201237515\n",
            "          policy_loss: -0.015320650942157954\n",
            "          total_loss: 0.0858285526628606\n",
            "          vf_explained_var: 0.6314723491668701\n",
            "          vf_loss: 0.2049808194860816\n",
            "    num_agent_steps_sampled: 252000\n",
            "    num_steps_sampled: 252000\n",
            "    num_steps_trained: 252000\n",
            "  iterations_since_restore: 63\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 29.00625\n",
            "    ram_util_percent: 16.0\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04870301524871588\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.2818794024690052\n",
            "    mean_inference_ms: 1.293424519763251\n",
            "    mean_raw_obs_processing_ms: 0.15847851861348947\n",
            "  time_since_restore: 1306.679652929306\n",
            "  time_this_iter_s: 22.31546902656555\n",
            "  time_total_s: 1306.679652929306\n",
            "  timers:\n",
            "    learn_throughput: 515.15\n",
            "    learn_time_ms: 7764.722\n",
            "    sample_throughput: 282.28\n",
            "    sample_time_ms: 14170.327\n",
            "    update_time_ms: 1.755\n",
            "  timestamp: 1619352422\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 252000\n",
            "  training_iteration: 63\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:07:02,217\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=252128, mean_mean=16.146398142645122, mean_std=9.280035928493675), (n=0, mean_mean=0.0, mean_std=0.0))}"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         1306.68</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">-0.490219</td><td style=\"text-align: right;\">           -0.466922</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           2438.16</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 256000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-07-25\n",
            "  done: false\n",
            "  episode_len_mean: 2479.58\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.46692169871988376\n",
            "  episode_reward_mean: -0.4898695315726744\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 128\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.5347723092418165e-13\n",
            "          cur_lr: 8.48e-06\n",
            "          entropy: 0.1336190130095929\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.003330187508254312\n",
            "          policy_loss: -0.018733303586486727\n",
            "          total_loss: 0.08233491532155313\n",
            "          vf_explained_var: 0.6187800765037537\n",
            "          vf_loss: 0.2048088158480823\n",
            "    num_agent_steps_sampled: 256000\n",
            "    num_steps_sampled: 256000\n",
            "    num_steps_trained: 256000\n",
            "  iterations_since_restore: 64\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.881818181818183\n",
            "    ram_util_percent: 16.0\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04870127877801688\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.290024737974123\n",
            "    mean_inference_ms: 1.2933653468196002\n",
            "    mean_raw_obs_processing_ms: 0.15853144671657435\n",
            "  time_since_restore: 1329.931412935257\n",
            "  time_this_iter_s: 23.251760005950928\n",
            "  time_total_s: 1329.931412935257\n",
            "  timers:\n",
            "    learn_throughput: 515.352\n",
            "    learn_time_ms: 7761.685\n",
            "    sample_throughput: 279.977\n",
            "    sample_time_ms: 14286.891\n",
            "    update_time_ms: 1.764\n",
            "  timestamp: 1619352445\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 256000\n",
            "  training_iteration: 64\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         1329.93</td><td style=\"text-align: right;\">256000</td><td style=\"text-align: right;\">-0.48987</td><td style=\"text-align: right;\">           -0.466922</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           2479.58</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:07:25,507\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=256129, mean_mean=16.148807245730826, mean_std=9.271325052856342), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:07:25,545\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': { 'net_worth': 60212.87664556399,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                             'step': 48},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=-3.659, max=2.562, mean=0.077),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': 1,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': -0.001724096645195461,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:07:25,547\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-3.461, max=3.442, mean=-0.009),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-0.001, max=-0.001, mean=-0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.999, max=0.999, mean=0.999),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.896, max=-0.896, mean=-0.896)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:07:25,550\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=-17.443, max=49.927, mean=17.891)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:07:25,550\tINFO sampler.py:589 -- Info return from env: {0: {'agent0': {'net_worth': 60032.24, 'step': 49}}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:07:25,551\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=-17.443, max=49.927, mean=17.891)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:07:25,551\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=-3.66, max=2.563, mean=0.088)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:07:26,032\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-8.17, max=8.175, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-2.954, max=0.0, mean=-0.169),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((200,), dtype=float32, min=0.052, max=1.0, mean=0.9),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.385),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-1.996, max=0.931, mean=-0.035),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=1852402573.0, max=1852402573.0, mean=1852402573.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 49, 'net_worth': 60032.24}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-3.667, max=2.865, mean=-0.11),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((200, 25, 3), dtype=float32, min=-3.667, max=2.865, mean=-0.111),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.03),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=1408.0, max=1408.0, mean=1408.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.03),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-1.145, max=1.315, mean=0.005)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:07:26,034\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-8.17, max=8.175, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-2.954, max=0.0, mean=-0.169),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.052, max=1.0, mean=0.9),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.385),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-1.996, max=0.931, mean=-0.035),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=1852402573.0, max=1852402573.0, mean=1852402573.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 49, 'net_worth': 60032.24}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-3.667, max=2.865, mean=-0.11),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-3.667, max=2.865, mean=-0.111),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.03),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=1408.0, max=1408.0, mean=1408.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.03),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-1.145, max=1.315, mean=0.005)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:07:26,039\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:07:39,322\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:07:39,325\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-7.982, max=7.987, mean=-0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((128,), dtype=float32, min=-0.908, max=0.0, mean=-0.056),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((128,), dtype=float32, min=0.403, max=1.0, mean=0.955),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.398),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((128,), dtype=float32, min=-3.685, max=2.494, mean=0.019),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((128,), dtype=int64, min=1852402573.0, max=1852402573.0, mean=1852402573.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((128,), dtype=object, head={'step': 2707, 'net_worth': 68959.654784436}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((128, 25, 3), dtype=float32, min=-10.367, max=9.817, mean=-0.025),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((128, 25, 3), dtype=float32, min=-10.365, max=9.815, mean=-0.023),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.125),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((128,), dtype=int64, min=1408.0, max=1427.0, mean=1418.586),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.125),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((128,), dtype=float32, min=-1.116, max=1.012, mean=-0.138)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:07:39,335\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 7.673861546209083e-14,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 8.44e-06,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.11002685129642487,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': -1.0246046233675088e-09,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': -0.019344724714756012,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': 0.06967801600694656,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.616, max=0.616, mean=0.616),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.18024598062038422},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 260000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-07-47\n",
            "  done: false\n",
            "  episode_len_mean: 2479.58\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.46692169871988376\n",
            "  episode_reward_mean: -0.4898695315726744\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 128\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 7.673861546209083e-14\n",
            "          cur_lr: 8.44e-06\n",
            "          entropy: 0.13643409800715744\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.003533759583660867\n",
            "          policy_loss: -0.019761574716540053\n",
            "          total_loss: 0.069319091970101\n",
            "          vf_explained_var: 0.6738130450248718\n",
            "          vf_loss: 0.18089001602493227\n",
            "    num_agent_steps_sampled: 260000\n",
            "    num_steps_sampled: 260000\n",
            "    num_steps_trained: 260000\n",
            "  iterations_since_restore: 65\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.98387096774194\n",
            "    ram_util_percent: 16.096774193548395\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04870127877801688\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.290024737974123\n",
            "    mean_inference_ms: 1.2933653468196002\n",
            "    mean_raw_obs_processing_ms: 0.15853144671657432\n",
            "  time_since_restore: 1351.4610669612885\n",
            "  time_this_iter_s: 21.529654026031494\n",
            "  time_total_s: 1351.4610669612885\n",
            "  timers:\n",
            "    learn_throughput: 514.919\n",
            "    learn_time_ms: 7768.215\n",
            "    sample_throughput: 279.536\n",
            "    sample_time_ms: 14309.407\n",
            "    update_time_ms: 1.782\n",
            "  timestamp: 1619352467\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 260000\n",
            "  training_iteration: 65\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:07:47,076\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=260129, mean_mean=16.153627373125126, mean_std=9.262996383345367), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         1351.46</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\">-0.48987</td><td style=\"text-align: right;\">           -0.466922</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           2479.58</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 264000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-08-08\n",
            "  done: false\n",
            "  episode_len_mean: 2519.35\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.46692169871988376\n",
            "  episode_reward_mean: -0.4894797402747967\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 129\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 3.836930773104541e-14\n",
            "          cur_lr: 8.400000000000001e-06\n",
            "          entropy: 0.13662584451958537\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.002947175882582087\n",
            "          policy_loss: -0.027522842516191304\n",
            "          total_loss: 0.0673175605625147\n",
            "          vf_explained_var: 0.6523270010948181\n",
            "          vf_loss: 0.192413333337754\n",
            "    num_agent_steps_sampled: 264000\n",
            "    num_steps_sampled: 264000\n",
            "    num_steps_trained: 264000\n",
            "  iterations_since_restore: 66\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.90333333333333\n",
            "    ram_util_percent: 16.07666666666667\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04870122357493467\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.298054149979654\n",
            "    mean_inference_ms: 1.2933063226410713\n",
            "    mean_raw_obs_processing_ms: 0.15858648668274986\n",
            "  time_since_restore: 1372.7965013980865\n",
            "  time_this_iter_s: 21.335434436798096\n",
            "  time_total_s: 1372.7965013980865\n",
            "  timers:\n",
            "    learn_throughput: 515.257\n",
            "    learn_time_ms: 7763.111\n",
            "    sample_throughput: 281.442\n",
            "    sample_time_ms: 14212.545\n",
            "    update_time_ms: 1.786\n",
            "  timestamp: 1619352488\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 264000\n",
            "  training_iteration: 66\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:08:08,447\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=264130, mean_mean=16.15714224762002, mean_std=9.257749431323578), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">          1372.8</td><td style=\"text-align: right;\">264000</td><td style=\"text-align: right;\">-0.48948</td><td style=\"text-align: right;\">           -0.466922</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           2519.35</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 268000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-08-29\n",
            "  done: false\n",
            "  episode_len_mean: 2559.6\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.46692169871988376\n",
            "  episode_reward_mean: -0.48907908254563126\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 130\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.9184653865522706e-14\n",
            "          cur_lr: 8.360000000000001e-06\n",
            "          entropy: 0.1302912337705493\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0030582447107008193\n",
            "          policy_loss: -0.0202570358524099\n",
            "          total_loss: 0.07238697254797444\n",
            "          vf_explained_var: 0.6498972773551941\n",
            "          vf_loss: 0.18789383443072438\n",
            "    num_agent_steps_sampled: 268000\n",
            "    num_steps_sampled: 268000\n",
            "    num_steps_trained: 268000\n",
            "  iterations_since_restore: 67\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.700000000000003\n",
            "    ram_util_percent: 16.100000000000005\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04870114642371793\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.306118749982734\n",
            "    mean_inference_ms: 1.2932420536074496\n",
            "    mean_raw_obs_processing_ms: 0.15864154463493133\n",
            "  time_since_restore: 1394.222263097763\n",
            "  time_this_iter_s: 21.425761699676514\n",
            "  time_total_s: 1394.222263097763\n",
            "  timers:\n",
            "    learn_throughput: 515.242\n",
            "    learn_time_ms: 7763.338\n",
            "    sample_throughput: 281.429\n",
            "    sample_time_ms: 14213.162\n",
            "    update_time_ms: 1.768\n",
            "  timestamp: 1619352509\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 268000\n",
            "  training_iteration: 67\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         1394.22</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\">-0.489079</td><td style=\"text-align: right;\">           -0.466922</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">            2559.6</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:08:29,912\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=268131, mean_mean=16.15836198725638, mean_std=9.253551939259824), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:08:29,946\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': { 'net_worth': 67853.4,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                             'step': 3009},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=-2.291, max=0.662, mean=-0.169),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:08:29,950\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-1.39, max=1.384, mean=-0.003),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-2.834, max=-2.834, mean=-2.834),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.059, max=0.059, mean=0.059),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.052, max=0.052, mean=0.052)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:08:29,959\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=-31.611, max=56.065, mean=14.018)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:08:29,960\tINFO sampler.py:589 -- Info return from env: { 0: { 'agent0': { 'net_worth': 67637.303150217,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                    'step': 3010}}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:08:29,960\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=-31.611, max=56.065, mean=14.018)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:08:29,960\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=-2.296, max=0.662, mean=-0.225)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:08:30,801\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-8.642, max=8.644, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-2.834, max=0.0, mean=-0.121),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((200,), dtype=float32, min=0.059, max=1.0, mean=0.922),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.525),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-1.642, max=1.321, mean=-0.02),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=775271977.0, max=775271977.0, mean=775271977.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 3010, 'net_worth': 67637.303150217}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.404, max=6.597, mean=0.015),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.404, max=6.597, mean=0.017),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.07),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=1470.0, max=1470.0, mean=1470.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.07),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-1.011, max=0.875, mean=-0.05)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:08:30,803\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-8.642, max=8.644, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-2.834, max=0.0, mean=-0.121),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.059, max=1.0, mean=0.922),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.525),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-1.642, max=1.321, mean=-0.02),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=775271977.0, max=775271977.0, mean=775271977.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 3010, 'net_worth': 67637.303150217}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.404, max=6.597, mean=0.015),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.404, max=6.597, mean=0.017),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.07),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=1470.0, max=1470.0, mean=1470.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.07),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-1.011, max=0.875, mean=-0.05)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:08:30,807\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:08:44,145\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:08:44,148\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 32,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((32, 2), dtype=float32, min=-9.063, max=9.07, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((32,), dtype=float32, min=-1.668, max=0.0, mean=-0.118),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((32,), dtype=float32, min=0.189, max=1.0, mean=0.924),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((32,), dtype=int64, min=0.0, max=1.0, mean=0.5),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((32,), dtype=float32, min=-3.207, max=0.865, mean=-0.233),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((32,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((32,), dtype=int64, min=775271977.0, max=1698582841.0, mean=1381194731.5),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((32,), dtype=object, head={'step': 4517, 'net_worth': 40212.834103626}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((32, 25, 3), dtype=float32, min=-4.384, max=11.113, mean=-0.021),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((32, 25, 3), dtype=float32, min=-4.382, max=11.11, mean=-0.027),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((32,), dtype=float32, min=-1.0, max=1.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((32,), dtype=int64, min=1470.0, max=1490.0, mean=1480.344),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((32,), dtype=float32, min=-1.0, max=1.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((32,), dtype=float32, min=-0.831, max=1.18, mean=0.102)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:08:44,156\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 9.592326932761353e-15,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 8.32e-06,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.11956211924552917,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': -1.4392700364851407e-08,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': 0.23336127400398254,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': 0.3275589942932129,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.679, max=0.679, mean=0.679),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.19078664481639862},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 272000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-08-51\n",
            "  done: false\n",
            "  episode_len_mean: 2601.08\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.46539707390548246\n",
            "  episode_reward_mean: -0.48858434294598296\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 131\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 9.592326932761353e-15\n",
            "          cur_lr: 8.32e-06\n",
            "          entropy: 0.1296213718596846\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0031835908848734107\n",
            "          policy_loss: -0.021091433125548065\n",
            "          total_loss: 0.07835510905715637\n",
            "          vf_explained_var: 0.6248527765274048\n",
            "          vf_loss: 0.2014855076558888\n",
            "    num_agent_steps_sampled: 272000\n",
            "    num_steps_sampled: 272000\n",
            "    num_steps_trained: 272000\n",
            "  iterations_since_restore: 68\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 29.187096774193545\n",
            "    ram_util_percent: 16.100000000000005\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.048701167222575725\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.3142287309070286\n",
            "    mean_inference_ms: 1.2931756768207368\n",
            "    mean_raw_obs_processing_ms: 0.15869694629069733\n",
            "  time_since_restore: 1416.179104089737\n",
            "  time_this_iter_s: 21.956840991973877\n",
            "  time_total_s: 1416.179104089737\n",
            "  timers:\n",
            "    learn_throughput: 514.786\n",
            "    learn_time_ms: 7770.218\n",
            "    sample_throughput: 280.885\n",
            "    sample_time_ms: 14240.686\n",
            "    update_time_ms: 1.75\n",
            "  timestamp: 1619352531\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 272000\n",
            "  training_iteration: 68\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         1416.18</td><td style=\"text-align: right;\">272000</td><td style=\"text-align: right;\">-0.488584</td><td style=\"text-align: right;\">           -0.465397</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           2601.08</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:08:51,905\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=272132, mean_mean=16.156714985980845, mean_std=9.247757746343707), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 276000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-09-13\n",
            "  done: false\n",
            "  episode_len_mean: 2642.13\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.46539707390548246\n",
            "  episode_reward_mean: -0.4882413087715872\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 132\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 4.7961634663806766e-15\n",
            "          cur_lr: 8.28e-06\n",
            "          entropy: 0.12862767092883587\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0025751535140443593\n",
            "          policy_loss: -0.014513626752886921\n",
            "          total_loss: 0.08987177105154842\n",
            "          vf_explained_var: 0.6096848845481873\n",
            "          vf_loss: 0.21134335407987237\n",
            "    num_agent_steps_sampled: 276000\n",
            "    num_steps_sampled: 276000\n",
            "    num_steps_trained: 276000\n",
            "  iterations_since_restore: 69\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.85483870967742\n",
            "    ram_util_percent: 16.100000000000005\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04870106271693565\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.3223825462153973\n",
            "    mean_inference_ms: 1.2931031863920583\n",
            "    mean_raw_obs_processing_ms: 0.15875212466267813\n",
            "  time_since_restore: 1437.8960025310516\n",
            "  time_this_iter_s: 21.716898441314697\n",
            "  time_total_s: 1437.8960025310516\n",
            "  timers:\n",
            "    learn_throughput: 515.765\n",
            "    learn_time_ms: 7755.47\n",
            "    sample_throughput: 280.628\n",
            "    sample_time_ms: 14253.735\n",
            "    update_time_ms: 1.754\n",
            "  timestamp: 1619352553\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 276000\n",
            "  training_iteration: 69\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:09:13,657\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=276133, mean_mean=16.15950114502837, mean_std=9.239180338944202), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">          1437.9</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\">-0.488241</td><td style=\"text-align: right;\">           -0.465397</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           2642.13</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 280000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-09-35\n",
            "  done: false\n",
            "  episode_len_mean: 2682.2\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.46539707390548246\n",
            "  episode_reward_mean: -0.48781518236079324\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 133\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 2.3980817331903383e-15\n",
            "          cur_lr: 8.24e-06\n",
            "          entropy: 0.13332790601998568\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0032916801792453043\n",
            "          policy_loss: -0.023353235970716923\n",
            "          total_loss: 0.07720492975204252\n",
            "          vf_explained_var: 0.6223318576812744\n",
            "          vf_loss: 0.20378289418295026\n",
            "    num_agent_steps_sampled: 280000\n",
            "    num_steps_sampled: 280000\n",
            "    num_steps_trained: 280000\n",
            "  iterations_since_restore: 70\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.796875\n",
            "    ram_util_percent: 16.1\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04870089901196249\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.330588668083614\n",
            "    mean_inference_ms: 1.2930260467680632\n",
            "    mean_raw_obs_processing_ms: 0.15880728061113403\n",
            "  time_since_restore: 1459.900260925293\n",
            "  time_this_iter_s: 22.004258394241333\n",
            "  time_total_s: 1459.900260925293\n",
            "  timers:\n",
            "    learn_throughput: 516.79\n",
            "    learn_time_ms: 7740.091\n",
            "    sample_throughput: 280.304\n",
            "    sample_time_ms: 14270.202\n",
            "    update_time_ms: 1.726\n",
            "  timestamp: 1619352575\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 280000\n",
            "  training_iteration: 70\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:09:35,696\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=280134, mean_mean=16.157039753416168, mean_std=9.229020695964072), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">          1459.9</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\">-0.487815</td><td style=\"text-align: right;\">           -0.465397</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">            2682.2</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:09:35,732\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': { 'net_worth': 74802.08,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                             'step': 1298},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=-0.752, max=4.376, mean=0.351),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:09:35,735\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-2.376, max=2.368, mean=-0.004),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-0.009, max=-0.009, mean=-0.009),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.991, max=0.991, mean=0.991),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.443, max=-0.443, mean=-0.443)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:09:35,738\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=-7.067, max=69.588, mean=20.403)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:09:35,738\tINFO sampler.py:589 -- Info return from env: {0: {'agent0': {'net_worth': 74802.08, 'step': 1299}}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:09:35,739\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=-7.067, max=69.588, mean=20.403)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:09:35,739\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=-0.752, max=4.377, mean=0.379)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:09:36,368\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-8.116, max=8.127, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-5.269, max=0.0, mean=-0.085),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((200,), dtype=float32, min=0.005, max=1.0, mean=0.963),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.35),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-1.127, max=1.191, mean=0.033),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=1877439023.0, max=1877439023.0, mean=1877439023.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 1299, 'net_worth': 74802.08}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-3.608, max=4.391, mean=-0.022),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((200, 25, 3), dtype=float32, min=-3.608, max=4.391, mean=-0.024),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.015),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=1533.0, max=1533.0, mean=1533.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.015),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-1.086, max=1.015, mean=-0.048)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:09:36,371\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-8.116, max=8.127, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-5.269, max=0.0, mean=-0.085),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.005, max=1.0, mean=0.963),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.35),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-1.127, max=1.191, mean=0.033),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=1877439023.0, max=1877439023.0, mean=1877439023.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 1299, 'net_worth': 74802.08}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-3.608, max=4.391, mean=-0.022),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-3.608, max=4.391, mean=-0.024),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.015),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=1533.0, max=1533.0, mean=1533.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.015),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-1.086, max=1.015, mean=-0.048)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:09:36,374\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:09:51,393\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:09:51,397\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-15.283, max=15.286, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((128,), dtype=float32, min=-5.269, max=0.0, mean=-0.136),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((128,), dtype=float32, min=0.005, max=1.0, mean=0.933),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.422),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((128,), dtype=float32, min=-2.368, max=2.566, mean=0.079),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((128,), dtype=int64, min=164378258.0, max=1877439023.0, mean=1475940406.203),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((128,), dtype=object, head={'step': 3698, 'net_worth': 59728.82}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((128, 25, 3), dtype=float32, min=-12.217, max=9.856, mean=-0.057),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((128, 25, 3), dtype=float32, min=-12.217, max=16.407, mean=-0.053),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=0.039),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((128,), dtype=int64, min=1533.0, max=1553.0, mean=1543.766),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=0.039),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((128,), dtype=float32, min=-0.981, max=1.048, mean=-0.002)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:09:51,406\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 1.1990408665951691e-15,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 8.200000000000001e-06,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.1184004545211792,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': 2.7538091806889042e-09,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': -0.0793175920844078,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': 0.016409724950790405,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.632, max=0.632, mean=0.632),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.19382265210151672},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 284000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-09-59\n",
            "  done: false\n",
            "  episode_len_mean: 2723.45\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.46539707390548246\n",
            "  episode_reward_mean: -0.48758056718643067\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 134\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.1990408665951691e-15\n",
            "          cur_lr: 8.200000000000001e-06\n",
            "          entropy: 0.12720614718273282\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.002545445047871908\n",
            "          policy_loss: -0.022750691568944603\n",
            "          total_loss: 0.07238514313939959\n",
            "          vf_explained_var: 0.6449360847473145\n",
            "          vf_loss: 0.19281579414382577\n",
            "    num_agent_steps_sampled: 284000\n",
            "    num_steps_sampled: 284000\n",
            "    num_steps_trained: 284000\n",
            "  iterations_since_restore: 71\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 29.9\n",
            "    ram_util_percent: 16.1\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.048700696370141905\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.338852699255601\n",
            "    mean_inference_ms: 1.2929822129708715\n",
            "    mean_raw_obs_processing_ms: 0.1588628820077161\n",
            "  time_since_restore: 1483.2228045463562\n",
            "  time_this_iter_s: 23.322543621063232\n",
            "  time_total_s: 1483.2228045463562\n",
            "  timers:\n",
            "    learn_throughput: 518.002\n",
            "    learn_time_ms: 7721.972\n",
            "    sample_throughput: 277.98\n",
            "    sample_time_ms: 14389.532\n",
            "    update_time_ms: 1.764\n",
            "  timestamp: 1619352599\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 284000\n",
            "  training_iteration: 71\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:09:59,056\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=284135, mean_mean=16.156597554954768, mean_std=9.221912275099632), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         1483.22</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\">-0.487581</td><td style=\"text-align: right;\">           -0.465397</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           2723.45</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 288000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-10-21\n",
            "  done: false\n",
            "  episode_len_mean: 2764.99\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.46539707390548246\n",
            "  episode_reward_mean: -0.4872382890653973\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 135\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 5.995204332975846e-16\n",
            "          cur_lr: 8.16e-06\n",
            "          entropy: 0.12620119517669082\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.003159416632115608\n",
            "          policy_loss: -0.022733608493581414\n",
            "          total_loss: 0.07946775192976929\n",
            "          vf_explained_var: 0.6150750517845154\n",
            "          vf_loss: 0.2069267500191927\n",
            "    num_agent_steps_sampled: 288000\n",
            "    num_steps_sampled: 288000\n",
            "    num_steps_trained: 288000\n",
            "  iterations_since_restore: 72\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.753125\n",
            "    ram_util_percent: 16.1\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.048700436932083466\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.3471837074258475\n",
            "    mean_inference_ms: 1.2929328218986362\n",
            "    mean_raw_obs_processing_ms: 0.15891867070166452\n",
            "  time_since_restore: 1505.8465802669525\n",
            "  time_this_iter_s: 22.623775720596313\n",
            "  time_total_s: 1505.8465802669525\n",
            "  timers:\n",
            "    learn_throughput: 518.627\n",
            "    learn_time_ms: 7712.671\n",
            "    sample_throughput: 277.247\n",
            "    sample_time_ms: 14427.578\n",
            "    update_time_ms: 1.753\n",
            "  timestamp: 1619352621\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 288000\n",
            "  training_iteration: 72\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         1505.85</td><td style=\"text-align: right;\">288000</td><td style=\"text-align: right;\">-0.487238</td><td style=\"text-align: right;\">           -0.465397</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           2764.99</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:10:21,720\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=288136, mean_mean=16.158825737822713, mean_std=9.214806997426892), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 292000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-10-43\n",
            "  done: false\n",
            "  episode_len_mean: 2764.99\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.46539707390548246\n",
            "  episode_reward_mean: -0.4872382890653973\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 135\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 2.997602166487923e-16\n",
            "          cur_lr: 8.12e-06\n",
            "          entropy: 0.1279839335475117\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0027636448430712335\n",
            "          policy_loss: -0.011387156308046542\n",
            "          total_loss: 0.07247945756535046\n",
            "          vf_explained_var: 0.691619336605072\n",
            "          vf_loss: 0.17029290809296072\n",
            "    num_agent_steps_sampled: 292000\n",
            "    num_steps_sampled: 292000\n",
            "    num_steps_trained: 292000\n",
            "  iterations_since_restore: 73\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.774193548387096\n",
            "    ram_util_percent: 16.100000000000005\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04870043693208345\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.3471837074258475\n",
            "    mean_inference_ms: 1.2929328218986358\n",
            "    mean_raw_obs_processing_ms: 0.15891867070166454\n",
            "  time_since_restore: 1527.2309625148773\n",
            "  time_this_iter_s: 21.384382247924805\n",
            "  time_total_s: 1527.2309625148773\n",
            "  timers:\n",
            "    learn_throughput: 519.55\n",
            "    learn_time_ms: 7698.971\n",
            "    sample_throughput: 278.78\n",
            "    sample_time_ms: 14348.253\n",
            "    update_time_ms: 1.762\n",
            "  timestamp: 1619352643\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 292000\n",
            "  training_iteration: 73\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         1527.23</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\">-0.487238</td><td style=\"text-align: right;\">           -0.465397</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           2764.99</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:10:43,141\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=292136, mean_mean=16.16294091342817, mean_std=9.207501923790948), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:10:43,175\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': { 'net_worth': 50266.117321245,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                             'step': 4129},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=-0.401, max=0.934, mean=0.134),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': 1,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': -9.360228389576619e-05,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:10:43,179\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-5.241, max=5.24, mean=-0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-0.0, max=-0.0, mean=-0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.135, max=-0.135, mean=-0.135)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:10:43,187\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=-4.487, max=60.029, mean=17.517)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:10:43,188\tINFO sampler.py:589 -- Info return from env: { 0: { 'agent0': { 'net_worth': 50258.274881535,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                    'step': 4130}}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:10:43,189\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=-4.487, max=60.029, mean=17.517)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:10:43,189\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=-0.401, max=0.932, mean=0.123)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:10:44,144\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-10.029, max=10.028, mean=-0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-2.187, max=0.0, mean=-0.097),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((200,), dtype=float32, min=0.112, max=1.0, mean=0.937),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.525),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-1.398, max=1.723, mean=0.077),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=1360018888.0, max=1360018888.0, mean=1360018888.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 4130, 'net_worth': 50258.274881535}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-1.59, max=2.338, mean=0.055),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((200, 25, 3), dtype=float32, min=-1.59, max=2.338, mean=0.056),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=1595.0, max=1595.0, mean=1595.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-0.872, max=1.049, mean=-0.077)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:10:44,146\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-10.029, max=10.028, mean=-0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-2.187, max=0.0, mean=-0.097),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.112, max=1.0, mean=0.937),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.525),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-1.398, max=1.723, mean=0.077),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=1360018888.0, max=1360018888.0, mean=1360018888.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 4130, 'net_worth': 50258.274881535}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-1.59, max=2.338, mean=0.055),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-1.59, max=2.338, mean=0.056),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=1595.0, max=1595.0, mean=1595.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-0.872, max=1.049, mean=-0.077)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:10:44,151\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:10:56,851\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:10:56,855\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-11.099, max=11.099, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((128,), dtype=float32, min=-4.56, max=0.0, mean=-0.137),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((128,), dtype=float32, min=0.01, max=1.0, mean=0.927),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.484),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((128,), dtype=float32, min=-3.141, max=3.798, mean=0.052),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((128,), dtype=int64, min=600982809.0, max=1360018888.0, mean=689932349.508),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((128,), dtype=object, head={'step': 2659, 'net_worth': 75286.835263728}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((128, 25, 3), dtype=float32, min=-6.035, max=11.103, mean=0.01),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((128, 25, 3), dtype=float32, min=-6.034, max=11.098, mean=0.009),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=0.008),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((128,), dtype=int64, min=1595.0, max=1615.0, mean=1604.727),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=0.008),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((128,), dtype=float32, min=-1.15, max=1.186, mean=-0.026)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:10:56,865\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 1.4988010832439614e-16,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 8.08e-06,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.1250113844871521,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': 2.761887163416077e-09,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': -0.05213741585612297,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': 0.041516728699207306,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.678, max=0.678, mean=0.678),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.18980856239795685},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 296000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-11-04\n",
            "  done: false\n",
            "  episode_len_mean: 2805.96\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4650068137471718\n",
            "  episode_reward_mean: -0.4868217742240889\n",
            "  episode_reward_min: -0.5356917374091527\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 136\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.4988010832439614e-16\n",
            "          cur_lr: 8.08e-06\n",
            "          entropy: 0.13072631182149053\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.003089962250669487\n",
            "          policy_loss: -0.01914928064798005\n",
            "          total_loss: 0.0664350418956019\n",
            "          vf_explained_var: 0.69329434633255\n",
            "          vf_loss: 0.1737831726204604\n",
            "    num_agent_steps_sampled: 296000\n",
            "    num_steps_sampled: 296000\n",
            "    num_steps_trained: 296000\n",
            "  iterations_since_restore: 74\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 29.066666666666663\n",
            "    ram_util_percent: 16.100000000000005\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.048699993018411956\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.3555649134566021\n",
            "    mean_inference_ms: 1.2928716613472182\n",
            "    mean_raw_obs_processing_ms: 0.15897378424013428\n",
            "  time_since_restore: 1548.5448467731476\n",
            "  time_this_iter_s: 21.313884258270264\n",
            "  time_total_s: 1548.5448467731476\n",
            "  timers:\n",
            "    learn_throughput: 520.744\n",
            "    learn_time_ms: 7681.314\n",
            "    sample_throughput: 282.244\n",
            "    sample_time_ms: 14172.132\n",
            "    update_time_ms: 1.753\n",
            "  timestamp: 1619352664\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 296000\n",
            "  training_iteration: 74\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         1548.54</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\">-0.486822</td><td style=\"text-align: right;\">           -0.465007</td><td style=\"text-align: right;\">           -0.535692</td><td style=\"text-align: right;\">           2805.96</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:11:04,491\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=296137, mean_mean=16.164670458102403, mean_std=9.205566910398932), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 300000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-11-25\n",
            "  done: false\n",
            "  episode_len_mean: 2846.09\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4650068137471718\n",
            "  episode_reward_mean: -0.4861885428167574\n",
            "  episode_reward_min: -0.5320461344046604\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 137\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 7.494005416219807e-17\n",
            "          cur_lr: 8.040000000000001e-06\n",
            "          entropy: 0.12276938324794173\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.002721507800742984\n",
            "          policy_loss: -0.019281476095784456\n",
            "          total_loss: 0.06233943279949017\n",
            "          vf_explained_var: 0.6784851551055908\n",
            "          vf_loss: 0.16569720418192446\n",
            "    num_agent_steps_sampled: 300000\n",
            "    num_steps_sampled: 300000\n",
            "    num_steps_trained: 300000\n",
            "  iterations_since_restore: 75\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.909677419354836\n",
            "    ram_util_percent: 16.100000000000005\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04869760297964785\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.363497266474125\n",
            "    mean_inference_ms: 1.292849873928775\n",
            "    mean_raw_obs_processing_ms: 0.1590136448344848\n",
            "  time_since_restore: 1569.8582091331482\n",
            "  time_this_iter_s: 21.31336236000061\n",
            "  time_total_s: 1569.8582091331482\n",
            "  timers:\n",
            "    learn_throughput: 521.417\n",
            "    learn_time_ms: 7671.405\n",
            "    sample_throughput: 282.478\n",
            "    sample_time_ms: 14160.403\n",
            "    update_time_ms: 1.745\n",
            "  timestamp: 1619352685\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 300000\n",
            "  training_iteration: 75\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:11:25,841\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=300138, mean_mean=16.163167074573586, mean_std=9.20458684595255), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         1569.86</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">-0.486189</td><td style=\"text-align: right;\">           -0.465007</td><td style=\"text-align: right;\">           -0.532046</td><td style=\"text-align: right;\">           2846.09</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 304000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-11-47\n",
            "  done: false\n",
            "  episode_len_mean: 2886.31\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4650068137471718\n",
            "  episode_reward_mean: -0.4858805819019502\n",
            "  episode_reward_min: -0.5320461344046604\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 138\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 3.7470027081099036e-17\n",
            "          cur_lr: 8.000000000000001e-06\n",
            "          entropy: 0.12459456687793136\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0037065468459331896\n",
            "          policy_loss: -0.025096761586610228\n",
            "          total_loss: 0.0823139333515428\n",
            "          vf_explained_var: 0.5936158895492554\n",
            "          vf_loss: 0.2173132924363017\n",
            "    num_agent_steps_sampled: 304000\n",
            "    num_steps_sampled: 304000\n",
            "    num_steps_trained: 304000\n",
            "  iterations_since_restore: 76\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.832258064516125\n",
            "    ram_util_percent: 16.13225806451613\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04869516183357267\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.371471433020926\n",
            "    mean_inference_ms: 1.2928226655925343\n",
            "    mean_raw_obs_processing_ms: 0.1590534652760973\n",
            "  time_since_restore: 1591.7848703861237\n",
            "  time_this_iter_s: 21.926661252975464\n",
            "  time_total_s: 1591.7848703861237\n",
            "  timers:\n",
            "    learn_throughput: 521.522\n",
            "    learn_time_ms: 7669.859\n",
            "    sample_throughput: 281.272\n",
            "    sample_time_ms: 14221.123\n",
            "    update_time_ms: 1.742\n",
            "  timestamp: 1619352707\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 304000\n",
            "  training_iteration: 76\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         1591.78</td><td style=\"text-align: right;\">304000</td><td style=\"text-align: right;\">-0.485881</td><td style=\"text-align: right;\">           -0.465007</td><td style=\"text-align: right;\">           -0.532046</td><td style=\"text-align: right;\">           2886.31</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:11:47,805\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=304139, mean_mean=16.161833050239228, mean_std=9.199926706716779), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:11:47,839\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': { 'net_worth': 75970.905864282,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                             'step': 2120},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=-5.219, max=1.546, mean=-0.561),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': 1,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': 0.0016525950651220533,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:11:47,841\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-0.527, max=0.511, mean=-0.008),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-0.303, max=-0.303, mean=-0.303),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.738, max=0.738, mean=0.738),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-1.095, max=-1.095, mean=-1.095)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:11:47,847\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=-46.105, max=49.856, mean=10.057)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:11:47,847\tINFO sampler.py:589 -- Info return from env: {0: {'agent0': {'net_worth': 75742.99, 'step': 2121}}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:11:47,848\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=-46.105, max=49.856, mean=10.057)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:11:47,848\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=-5.221, max=1.547, mean=-0.551)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:11:48,576\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-8.666, max=8.658, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-9.948, max=0.0, mean=-0.243),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((200,), dtype=float32, min=0.0, max=1.0, mean=0.889),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.49),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-1.862, max=1.503, mean=-0.01),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=1355531731.0, max=1355531731.0, mean=1355531731.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 2121, 'net_worth': 75742.99}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-5.229, max=8.394, mean=0.087),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((200, 25, 3), dtype=float32, min=-5.229, max=8.394, mean=0.084),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.07),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=1658.0, max=1658.0, mean=1658.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.07),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-1.205, max=1.213, mean=-0.06)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:11:48,579\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-8.666, max=8.658, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-9.948, max=0.0, mean=-0.243),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.0, max=1.0, mean=0.889),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.49),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-1.862, max=1.503, mean=-0.01),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=1355531731.0, max=1355531731.0, mean=1355531731.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 2121, 'net_worth': 75742.99}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-5.229, max=8.394, mean=0.087),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-5.229, max=8.394, mean=0.084),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.07),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=1658.0, max=1658.0, mean=1658.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.07),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-1.205, max=1.213, mean=-0.06)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:11:48,583\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:12:02,130\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:12:02,134\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-14.702, max=14.708, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((128,), dtype=float32, min=-9.948, max=0.0, mean=-0.238),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((128,), dtype=float32, min=0.0, max=1.0, mean=0.908),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.461),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((128,), dtype=float32, min=-3.904, max=2.489, mean=-0.021),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((128,), dtype=int64, min=1355531731.0, max=1451366778.0, mean=1393716007.539),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((128,), dtype=object, head={'step': 3129, 'net_worth': 59997.257382870004}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((128, 25, 3), dtype=float32, min=-12.161, max=16.336, mean=-0.01),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((128, 25, 3), dtype=float32, min=-12.158, max=16.332, mean=-0.006),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.039),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((128,), dtype=int64, min=1658.0, max=1678.0, mean=1668.32),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.039),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((128,), dtype=float32, min=-1.222, max=1.195, mean=-0.036)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:12:02,144\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 1.8735013540549518e-17,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 7.960000000000002e-06,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.12312512844800949,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': -1.6763157528743022e-09,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': 0.020514007657766342,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': 0.12904077768325806,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.614, max=0.614, mean=0.614),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.2195160835981369},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 308000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-12-09\n",
            "  done: false\n",
            "  episode_len_mean: 2925.42\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4650068137471718\n",
            "  episode_reward_mean: -0.48554215900235725\n",
            "  episode_reward_min: -0.5320461344046604\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 139\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.8735013540549518e-17\n",
            "          cur_lr: 7.960000000000002e-06\n",
            "          entropy: 0.12444752990268171\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0026740013472590363\n",
            "          policy_loss: -0.0201467540464364\n",
            "          total_loss: 0.0695829447649885\n",
            "          vf_explained_var: 0.6520583629608154\n",
            "          vf_loss: 0.18194833910092711\n",
            "    num_agent_steps_sampled: 308000\n",
            "    num_steps_sampled: 308000\n",
            "    num_steps_trained: 308000\n",
            "  iterations_since_restore: 77\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.990625\n",
            "    ram_util_percent: 16.196875\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.048692685816498375\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.3794869730774206\n",
            "    mean_inference_ms: 1.292790925656893\n",
            "    mean_raw_obs_processing_ms: 0.159093279816091\n",
            "  time_since_restore: 1613.8307609558105\n",
            "  time_this_iter_s: 22.04589056968689\n",
            "  time_total_s: 1613.8307609558105\n",
            "  timers:\n",
            "    learn_throughput: 520.689\n",
            "    learn_time_ms: 7682.131\n",
            "    sample_throughput: 280.295\n",
            "    sample_time_ms: 14270.669\n",
            "    update_time_ms: 1.751\n",
            "  timestamp: 1619352729\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 308000\n",
            "  training_iteration: 77\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:12:09,887\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=308140, mean_mean=16.162731258251483, mean_std=9.190465579940534), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         1613.83</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\">-0.485542</td><td style=\"text-align: right;\">           -0.465007</td><td style=\"text-align: right;\">           -0.532046</td><td style=\"text-align: right;\">           2925.42</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 312000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-12-32\n",
            "  done: false\n",
            "  episode_len_mean: 2963.62\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4650068137471718\n",
            "  episode_reward_mean: -0.48516169255273733\n",
            "  episode_reward_min: -0.5320461344046604\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 140\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 9.367506770274759e-18\n",
            "          cur_lr: 7.92e-06\n",
            "          entropy: 0.1282599614933133\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.002399767512542894\n",
            "          policy_loss: -0.016586425073910505\n",
            "          total_loss: 0.07385092508047819\n",
            "          vf_explained_var: 0.6566630601882935\n",
            "          vf_loss: 0.18343990063294768\n",
            "    num_agent_steps_sampled: 312000\n",
            "    num_steps_sampled: 312000\n",
            "    num_steps_trained: 312000\n",
            "  iterations_since_restore: 78\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.829032258064515\n",
            "    ram_util_percent: 16.199999999999996\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04869010333436462\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.3875508270285224\n",
            "    mean_inference_ms: 1.2927535919821347\n",
            "    mean_raw_obs_processing_ms: 0.15913314259366912\n",
            "  time_since_restore: 1635.9921333789825\n",
            "  time_this_iter_s: 22.161372423171997\n",
            "  time_total_s: 1635.9921333789825\n",
            "  timers:\n",
            "    learn_throughput: 521.365\n",
            "    learn_time_ms: 7672.168\n",
            "    sample_throughput: 279.699\n",
            "    sample_time_ms: 14301.091\n",
            "    update_time_ms: 1.756\n",
            "  timestamp: 1619352752\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 312000\n",
            "  training_iteration: 78\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:12:32,089\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=312141, mean_mean=16.161232372272497, mean_std=9.184589901031325), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         1635.99</td><td style=\"text-align: right;\">312000</td><td style=\"text-align: right;\">-0.485162</td><td style=\"text-align: right;\">           -0.465007</td><td style=\"text-align: right;\">           -0.532046</td><td style=\"text-align: right;\">           2963.62</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 316000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-12-55\n",
            "  done: false\n",
            "  episode_len_mean: 3006.28\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.45944705644585626\n",
            "  episode_reward_mean: -0.4844357017731493\n",
            "  episode_reward_min: -0.5086480455477201\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 141\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 4.6837533851373795e-18\n",
            "          cur_lr: 7.88e-06\n",
            "          entropy: 0.12830882403068244\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.004269230408681324\n",
            "          policy_loss: -0.019837968953652307\n",
            "          total_loss: 0.09072497765737353\n",
            "          vf_explained_var: 0.5770723223686218\n",
            "          vf_loss: 0.22369207139126956\n",
            "    num_agent_steps_sampled: 316000\n",
            "    num_steps_sampled: 316000\n",
            "    num_steps_trained: 316000\n",
            "  iterations_since_restore: 79\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.950000000000003\n",
            "    ram_util_percent: 16.200000000000003\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04868755041538579\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.3956948658784503\n",
            "    mean_inference_ms: 1.2927140801410635\n",
            "    mean_raw_obs_processing_ms: 0.1591733259268154\n",
            "  time_since_restore: 1659.2983350753784\n",
            "  time_this_iter_s: 23.306201696395874\n",
            "  time_total_s: 1659.2983350753784\n",
            "  timers:\n",
            "    learn_throughput: 521.509\n",
            "    learn_time_ms: 7670.055\n",
            "    sample_throughput: 276.584\n",
            "    sample_time_ms: 14462.171\n",
            "    update_time_ms: 1.759\n",
            "  timestamp: 1619352775\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 316000\n",
            "  training_iteration: 79\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:12:55,435\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=316142, mean_mean=16.162764150007497, mean_std=9.180024745323703), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">          1659.3</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\">-0.484436</td><td style=\"text-align: right;\">           -0.459447</td><td style=\"text-align: right;\">           -0.508648</td><td style=\"text-align: right;\">           3006.28</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:12:55,470\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': { 'net_worth': 60010.51,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                             'step': 52},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=-3.649, max=2.555, mean=0.188),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:12:55,473\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-0.056, max=0.06, mean=0.002),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-0.637, max=-0.637, mean=-0.637),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.529, max=0.529, mean=0.529),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.03, max=0.03, mean=0.03)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:12:55,475\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=-0.752, max=49.927, mean=19.031)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:12:55,475\tINFO sampler.py:589 -- Info return from env: {0: {'agent0': {'net_worth': 59870.865089, 'step': 53}}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:12:55,476\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=-0.752, max=49.927, mean=19.031)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:12:55,476\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=-3.65, max=1.718, mean=0.153)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:12:55,948\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-8.669, max=8.676, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-3.257, max=0.0, mean=-0.145),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((200,), dtype=float32, min=0.038, max=1.0, mean=0.917),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.38),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-1.678, max=1.339, mean=-0.028),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=958357928.0, max=958357928.0, mean=958357928.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 53, 'net_worth': 59870.865089}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-3.653, max=2.854, mean=-0.113),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((200, 25, 3), dtype=float32, min=-3.653, max=2.854, mean=-0.112),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.025),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=1721.0, max=1721.0, mean=1721.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.025),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-1.162, max=1.389, mean=0.003)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:12:55,950\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-8.669, max=8.676, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-3.257, max=0.0, mean=-0.145),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.038, max=1.0, mean=0.917),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.38),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-1.678, max=1.339, mean=-0.028),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=958357928.0, max=958357928.0, mean=958357928.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 53, 'net_worth': 59870.865089}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-3.653, max=2.854, mean=-0.113),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-3.653, max=2.854, mean=-0.112),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.025),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=1721.0, max=1721.0, mean=1721.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.025),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-1.162, max=1.389, mean=0.003)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:12:55,954\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:13:10,041\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:13:10,044\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-9.774, max=9.772, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((128,), dtype=float32, min=-2.828, max=0.0, mean=-0.114),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((128,), dtype=float32, min=0.059, max=1.0, mean=0.926),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.484),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((128,), dtype=float32, min=-4.308, max=3.782, mean=-0.113),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((128,), dtype=int64, min=958357928.0, max=958357928.0, mean=958357928.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((128,), dtype=object, head={'step': 1495, 'net_worth': 70851.253571196}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((128, 25, 3), dtype=float32, min=-12.136, max=16.302, mean=0.04),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((128, 25, 3), dtype=float32, min=-12.133, max=16.299, mean=0.042),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.062),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((128,), dtype=int64, min=1721.0, max=1740.0, mean=1731.102),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.062),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((128,), dtype=float32, min=-1.183, max=1.152, mean=-0.029)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:13:10,054\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 2.3418766925686897e-18,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 7.840000000000001e-06,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.13864745199680328,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': 1.7047148137550039e-09,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': 0.1125645563006401,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': 0.20606791973114014,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.68, max=0.68, mean=0.68),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.18977968394756317},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 320000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-13-17\n",
            "  done: false\n",
            "  episode_len_mean: 3006.28\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.45944705644585626\n",
            "  episode_reward_mean: -0.48443570177314926\n",
            "  episode_reward_min: -0.5086480455477201\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 141\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 2.3418766925686897e-18\n",
            "          cur_lr: 7.840000000000001e-06\n",
            "          entropy: 0.12639344669878483\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.00298833491615369\n",
            "          policy_loss: -0.019857921433867887\n",
            "          total_loss: 0.06002778670517728\n",
            "          vf_explained_var: 0.7096250057220459\n",
            "          vf_loss: 0.1622992802876979\n",
            "    num_agent_steps_sampled: 320000\n",
            "    num_steps_sampled: 320000\n",
            "    num_steps_trained: 320000\n",
            "  iterations_since_restore: 80\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.990322580645163\n",
            "    ram_util_percent: 16.199999999999996\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04868755041538579\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.3956948658784503\n",
            "    mean_inference_ms: 1.2927140801410635\n",
            "    mean_raw_obs_processing_ms: 0.1591733259268154\n",
            "  time_since_restore: 1681.5309867858887\n",
            "  time_this_iter_s: 22.232651710510254\n",
            "  time_total_s: 1681.5309867858887\n",
            "  timers:\n",
            "    learn_throughput: 521.305\n",
            "    learn_time_ms: 7673.054\n",
            "    sample_throughput: 276.202\n",
            "    sample_time_ms: 14482.138\n",
            "    update_time_ms: 1.748\n",
            "  timestamp: 1619352797\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 320000\n",
            "  training_iteration: 80\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:13:17,704\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=320142, mean_mean=16.1664494638047, mean_std=9.174290330385675), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         1681.53</td><td style=\"text-align: right;\">320000</td><td style=\"text-align: right;\">-0.484436</td><td style=\"text-align: right;\">           -0.459447</td><td style=\"text-align: right;\">           -0.508648</td><td style=\"text-align: right;\">           3006.28</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 324000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-13-38\n",
            "  done: false\n",
            "  episode_len_mean: 3045.82\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.45944705644585626\n",
            "  episode_reward_mean: -0.48411657604631597\n",
            "  episode_reward_min: -0.5086480455477201\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 142\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.1709383462843449e-18\n",
            "          cur_lr: 7.8e-06\n",
            "          entropy: 0.12766428827308118\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.002893395478167804\n",
            "          policy_loss: -0.021575558261247352\n",
            "          total_loss: 0.055675584415439516\n",
            "          vf_explained_var: 0.7161086797714233\n",
            "          vf_loss: 0.1570555737707764\n",
            "    num_agent_steps_sampled: 324000\n",
            "    num_steps_sampled: 324000\n",
            "    num_steps_trained: 324000\n",
            "  iterations_since_restore: 81\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.787096774193543\n",
            "    ram_util_percent: 16.199999999999996\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.048684722494882135\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.4038719152858723\n",
            "    mean_inference_ms: 1.2926939042338272\n",
            "    mean_raw_obs_processing_ms: 0.1592127672693206\n",
            "  time_since_restore: 1702.6705529689789\n",
            "  time_this_iter_s: 21.13956618309021\n",
            "  time_total_s: 1702.6705529689789\n",
            "  timers:\n",
            "    learn_throughput: 521.665\n",
            "    learn_time_ms: 7667.761\n",
            "    sample_throughput: 280.325\n",
            "    sample_time_ms: 14269.158\n",
            "    update_time_ms: 1.697\n",
            "  timestamp: 1619352818\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 324000\n",
            "  training_iteration: 81\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:13:38,881\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=324143, mean_mean=16.16796536673939, mean_std=9.172999173105087), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         1702.67</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\">-0.484117</td><td style=\"text-align: right;\">           -0.459447</td><td style=\"text-align: right;\">           -0.508648</td><td style=\"text-align: right;\">           3045.82</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 328000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-14-00\n",
            "  done: false\n",
            "  episode_len_mean: 3086.1\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.45944705644585626\n",
            "  episode_reward_mean: -0.48378198186929067\n",
            "  episode_reward_min: -0.5086480455477201\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 143\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 5.854691731421724e-19\n",
            "          cur_lr: 7.76e-06\n",
            "          entropy: 0.12157008377835155\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0037634472209902015\n",
            "          policy_loss: -0.01728813670342788\n",
            "          total_loss: 0.07241077936487272\n",
            "          vf_explained_var: 0.6632978320121765\n",
            "          vf_loss: 0.1818292266689241\n",
            "    num_agent_steps_sampled: 328000\n",
            "    num_steps_sampled: 328000\n",
            "    num_steps_trained: 328000\n",
            "  iterations_since_restore: 82\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.906666666666673\n",
            "    ram_util_percent: 16.199999999999996\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04868145760066603\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.411864335247499\n",
            "    mean_inference_ms: 1.2925905036231913\n",
            "    mean_raw_obs_processing_ms: 0.15925219280528516\n",
            "  time_since_restore: 1724.2255566120148\n",
            "  time_this_iter_s: 21.55500364303589\n",
            "  time_total_s: 1724.2255566120148\n",
            "  timers:\n",
            "    learn_throughput: 523.023\n",
            "    learn_time_ms: 7647.854\n",
            "    sample_throughput: 282.043\n",
            "    sample_time_ms: 14182.245\n",
            "    update_time_ms: 1.694\n",
            "  timestamp: 1619352840\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 328000\n",
            "  training_iteration: 82\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         1724.23</td><td style=\"text-align: right;\">328000</td><td style=\"text-align: right;\">-0.483782</td><td style=\"text-align: right;\">           -0.459447</td><td style=\"text-align: right;\">           -0.508648</td><td style=\"text-align: right;\">            3086.1</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:14:00,472\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=328144, mean_mean=16.168336128121037, mean_std=9.17140110371626), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:14:00,507\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': { 'net_worth': 77822.92673777281,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                             'step': 2708},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=-0.615, max=1.293, mean=0.333),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': 1,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': -0.0019301831372919853,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:14:00,509\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-2.085, max=2.089, mean=0.002),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-0.015, max=-0.015, mean=-0.015),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.985, max=0.985, mean=0.985),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.34, max=-0.34, mean=-0.34)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:14:00,517\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=-4.71, max=64.166, mean=19.975)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:14:00,517\tINFO sampler.py:589 -- Info return from env: { 0: { 'agent0': { 'net_worth': 77797.174471424,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                    'step': 2709}}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:14:00,517\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=-4.71, max=64.166, mean=19.975)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:14:00,518\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=-0.616, max=1.291, mean=0.34)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:14:01,298\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-12.702, max=12.703, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-4.793, max=0.0, mean=-0.112),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((200,), dtype=float32, min=0.008, max=1.0, mean=0.939),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.315),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-0.981, max=1.116, mean=-0.017),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=912703264.0, max=912703264.0, mean=912703264.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 2709, 'net_worth': 77797.174471424}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-6.021, max=4.9, mean=-0.197),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((200, 25, 3), dtype=float32, min=-6.021, max=4.9, mean=-0.197),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.1),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=1783.0, max=1783.0, mean=1783.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.1),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-1.087, max=1.079, mean=-0.083)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:14:01,301\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-12.702, max=12.703, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-4.793, max=0.0, mean=-0.112),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.008, max=1.0, mean=0.939),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.315),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-0.981, max=1.116, mean=-0.017),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=912703264.0, max=912703264.0, mean=912703264.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 2709, 'net_worth': 77797.174471424}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-6.021, max=4.9, mean=-0.197),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-6.021, max=4.9, mean=-0.197),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.1),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=1783.0, max=1783.0, mean=1783.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.1),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-1.087, max=1.079, mean=-0.083)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:14:01,305\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:14:15,030\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:14:15,033\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-9.813, max=9.813, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((128,), dtype=float32, min=-2.952, max=0.0, mean=-0.068),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((128,), dtype=float32, min=0.052, max=1.0, mean=0.956),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.406),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((128,), dtype=float32, min=-3.273, max=2.951, mean=0.028),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((128,), dtype=int64, min=233592921.0, max=912703264.0, mean=578453642.055),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((128,), dtype=object, head={'step': 301, 'net_worth': 64128.89392476001}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((128, 25, 3), dtype=float32, min=-12.14, max=16.311, mean=-0.007),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((128, 25, 3), dtype=float32, min=-12.136, max=16.304, mean=-0.005),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.07),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((128,), dtype=int64, min=1783.0, max=1803.0, mean=1793.297),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.07),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((128,), dtype=float32, min=-1.409, max=1.103, mean=-0.066)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:14:15,043\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 2.927345865710862e-19,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 7.72e-06,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.09676849842071533,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': -1.7184107470313847e-09,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': -0.02836233377456665,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': 0.07503657788038254,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.609, max=0.609, mean=0.609),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.2087331861257553},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 332000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-14-22\n",
            "  done: false\n",
            "  episode_len_mean: 3126.78\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.456938839542523\n",
            "  episode_reward_mean: -0.4833153931504207\n",
            "  episode_reward_min: -0.5086480455477201\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 144\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 2.927345865710862e-19\n",
            "          cur_lr: 7.72e-06\n",
            "          entropy: 0.1208685461897403\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0034575157405924983\n",
            "          policy_loss: -0.026434065774083138\n",
            "          total_loss: 0.07773083719075657\n",
            "          vf_explained_var: 0.6024261713027954\n",
            "          vf_loss: 0.21074717165902257\n",
            "    num_agent_steps_sampled: 332000\n",
            "    num_steps_sampled: 332000\n",
            "    num_steps_trained: 332000\n",
            "  iterations_since_restore: 83\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.934375\n",
            "    ram_util_percent: 16.2\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04867807716366622\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.4199011646028716\n",
            "    mean_inference_ms: 1.2924813537086646\n",
            "    mean_raw_obs_processing_ms: 0.15929145546660678\n",
            "  time_since_restore: 1746.4406130313873\n",
            "  time_this_iter_s: 22.21505641937256\n",
            "  time_total_s: 1746.4406130313873\n",
            "  timers:\n",
            "    learn_throughput: 523.1\n",
            "    learn_time_ms: 7646.725\n",
            "    sample_throughput: 280.378\n",
            "    sample_time_ms: 14266.431\n",
            "    update_time_ms: 1.694\n",
            "  timestamp: 1619352862\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 332000\n",
            "  training_iteration: 83\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         1746.44</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\">-0.483315</td><td style=\"text-align: right;\">           -0.456939</td><td style=\"text-align: right;\">           -0.508648</td><td style=\"text-align: right;\">           3126.78</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:14:22,724\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=332145, mean_mean=16.167690520477084, mean_std=9.167306311458919), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 336000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-14-45\n",
            "  done: false\n",
            "  episode_len_mean: 3165.27\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.456938839542523\n",
            "  episode_reward_mean: -0.4833819436757237\n",
            "  episode_reward_min: -0.5144373548234609\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 145\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.463672932855431e-19\n",
            "          cur_lr: 7.680000000000001e-06\n",
            "          entropy: 0.12703685136511922\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0038683002057950944\n",
            "          policy_loss: -0.013387312006670982\n",
            "          total_loss: 0.07162508554756641\n",
            "          vf_explained_var: 0.6780017614364624\n",
            "          vf_loss: 0.17256553377956152\n",
            "    num_agent_steps_sampled: 336000\n",
            "    num_steps_sampled: 336000\n",
            "    num_steps_trained: 336000\n",
            "  iterations_since_restore: 84\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.90625\n",
            "    ram_util_percent: 16.296875\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04867463618104715\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.4279789587445515\n",
            "    mean_inference_ms: 1.292367212825558\n",
            "    mean_raw_obs_processing_ms: 0.15933069561128038\n",
            "  time_since_restore: 1768.6932775974274\n",
            "  time_this_iter_s: 22.25266456604004\n",
            "  time_total_s: 1768.6932775974274\n",
            "  timers:\n",
            "    learn_throughput: 522.104\n",
            "    learn_time_ms: 7661.306\n",
            "    sample_throughput: 278.83\n",
            "    sample_time_ms: 14345.677\n",
            "    update_time_ms: 1.687\n",
            "  timestamp: 1619352885\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 336000\n",
            "  training_iteration: 84\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.1/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         1768.69</td><td style=\"text-align: right;\">336000</td><td style=\"text-align: right;\">-0.483382</td><td style=\"text-align: right;\">           -0.456939</td><td style=\"text-align: right;\">           -0.514437</td><td style=\"text-align: right;\">           3165.27</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:14:45,012\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=336146, mean_mean=16.16434057218042, mean_std=9.160289428606026), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 340000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-15-07\n",
            "  done: false\n",
            "  episode_len_mean: 3204.14\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.456938839542523\n",
            "  episode_reward_mean: -0.4829110640663966\n",
            "  episode_reward_min: -0.5144373548234609\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 146\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 7.318364664277155e-20\n",
            "          cur_lr: 7.640000000000001e-06\n",
            "          entropy: 0.12551230425015092\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.003397361098905094\n",
            "          policy_loss: -0.020462890446651727\n",
            "          total_loss: 0.07359033724060282\n",
            "          vf_explained_var: 0.6471982002258301\n",
            "          vf_loss: 0.19061670452356339\n",
            "    num_agent_steps_sampled: 340000\n",
            "    num_steps_sampled: 340000\n",
            "    num_steps_trained: 340000\n",
            "  iterations_since_restore: 85\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.715625\n",
            "    ram_util_percent: 16.3\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.0486711529770276\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.4361152455237178\n",
            "    mean_inference_ms: 1.292248397073284\n",
            "    mean_raw_obs_processing_ms: 0.15937001407992385\n",
            "  time_since_restore: 1791.3073315620422\n",
            "  time_this_iter_s: 22.614053964614868\n",
            "  time_total_s: 1791.3073315620422\n",
            "  timers:\n",
            "    learn_throughput: 523.122\n",
            "    learn_time_ms: 7646.394\n",
            "    sample_throughput: 276.041\n",
            "    sample_time_ms: 14490.623\n",
            "    update_time_ms: 1.684\n",
            "  timestamp: 1619352907\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 340000\n",
            "  training_iteration: 85\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.2/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         1791.31</td><td style=\"text-align: right;\">340000</td><td style=\"text-align: right;\">-0.482911</td><td style=\"text-align: right;\">           -0.456939</td><td style=\"text-align: right;\">           -0.514437</td><td style=\"text-align: right;\">           3204.14</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:15:07,661\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=340147, mean_mean=16.165079405171692, mean_std=9.15555893676676), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:15:07,699\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': { 'net_worth': 68261.29,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                             'step': 370},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=-2.551, max=2.625, mean=0.383),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:15:07,701\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-8.394, max=8.394, mean=-0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.154, max=-0.154, mean=-0.154)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:15:07,703\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=-32.897, max=68.508, mean=20.282)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:15:07,703\tINFO sampler.py:589 -- Info return from env: {0: {'agent0': {'net_worth': 68261.29, 'step': 371}}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:15:07,704\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=-32.897, max=68.508, mean=20.282)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:15:07,705\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=-2.552, max=2.626, mean=0.304)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:15:08,220\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-9.182, max=9.184, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-2.199, max=0.0, mean=-0.107),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((200,), dtype=float32, min=0.111, max=1.0, mean=0.933),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.455),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-1.639, max=1.031, mean=-0.02),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=745316802.0, max=745316802.0, mean=745316802.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 371, 'net_worth': 68261.29}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.166, max=3.503, mean=0.133),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.166, max=3.503, mean=0.134),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.055),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=1846.0, max=1846.0, mean=1846.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.055),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-1.141, max=1.251, mean=-0.035)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:15:08,223\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-9.182, max=9.184, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-2.199, max=0.0, mean=-0.107),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.111, max=1.0, mean=0.933),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.455),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-1.639, max=1.031, mean=-0.02),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=745316802.0, max=745316802.0, mean=745316802.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 371, 'net_worth': 68261.29}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.166, max=3.503, mean=0.133),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.166, max=3.503, mean=0.134),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.055),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=1846.0, max=1846.0, mean=1846.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.055),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-1.141, max=1.251, mean=-0.035)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:15:08,228\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:15:21,960\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:15:21,963\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-10.376, max=10.375, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((128,), dtype=float32, min=-6.231, max=0.0, mean=-0.174),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((128,), dtype=float32, min=0.002, max=1.0, mean=0.917),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.516),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((128,), dtype=float32, min=-2.312, max=1.985, mean=-0.056),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((128,), dtype=int64, min=745316802.0, max=745316802.0, mean=745316802.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((128,), dtype=object, head={'step': 1307, 'net_worth': 75724.63}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((128, 25, 3), dtype=float32, min=-5.992, max=9.761, mean=0.058),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((128, 25, 3), dtype=float32, min=-12.115, max=9.757, mean=0.058),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.086),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((128,), dtype=int64, min=1846.0, max=1865.0, mean=1856.023),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.086),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((128,), dtype=float32, min=-1.16, max=1.04, mean=-0.065)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:15:21,974\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 3.659182332138578e-20,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 7.600000000000001e-06,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.11807765066623688,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': -9.867653361084194e-10,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': 0.05648045614361763,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': 0.12954175472259521,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.757, max=0.757, mean=0.757),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.14848412573337555},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 344000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-15-29\n",
            "  done: false\n",
            "  episode_len_mean: 3204.14\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.456938839542523\n",
            "  episode_reward_mean: -0.4829110640663966\n",
            "  episode_reward_min: -0.5144373548234609\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 146\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 3.659182332138578e-20\n",
            "          cur_lr: 7.600000000000001e-06\n",
            "          entropy: 0.12339249718934298\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0036173573025735095\n",
            "          policy_loss: -0.021875960752367973\n",
            "          total_loss: 0.06299566099187359\n",
            "          vf_explained_var: 0.6926574110984802\n",
            "          vf_loss: 0.17221109056845307\n",
            "    num_agent_steps_sampled: 344000\n",
            "    num_steps_sampled: 344000\n",
            "    num_steps_trained: 344000\n",
            "  iterations_since_restore: 86\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 29.074193548387097\n",
            "    ram_util_percent: 16.300000000000004\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.0486711529770276\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.4361152455237178\n",
            "    mean_inference_ms: 1.2922483970732843\n",
            "    mean_raw_obs_processing_ms: 0.15937001407992385\n",
            "  time_since_restore: 1813.134693622589\n",
            "  time_this_iter_s: 21.827362060546875\n",
            "  time_total_s: 1813.134693622589\n",
            "  timers:\n",
            "    learn_throughput: 523.906\n",
            "    learn_time_ms: 7634.961\n",
            "    sample_throughput: 276.011\n",
            "    sample_time_ms: 14492.162\n",
            "    update_time_ms: 1.684\n",
            "  timestamp: 1619352929\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 344000\n",
            "  training_iteration: 86\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.2/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         1813.13</td><td style=\"text-align: right;\">344000</td><td style=\"text-align: right;\">-0.482911</td><td style=\"text-align: right;\">           -0.456939</td><td style=\"text-align: right;\">           -0.514437</td><td style=\"text-align: right;\">           3204.14</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:15:29,528\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=344147, mean_mean=16.169568358406174, mean_std=9.147575986088787), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 348000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-15-50\n",
            "  done: false\n",
            "  episode_len_mean: 3243.72\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4562224048973671\n",
            "  episode_reward_mean: -0.4824355628692045\n",
            "  episode_reward_min: -0.5144373548234609\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 147\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.829591166069289e-20\n",
            "          cur_lr: 7.5600000000000005e-06\n",
            "          entropy: 0.1335021248087287\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0032432745865662582\n",
            "          policy_loss: -0.01583653298439458\n",
            "          total_loss: 0.06178147016908042\n",
            "          vf_explained_var: 0.7204910516738892\n",
            "          vf_loss: 0.1579060449730605\n",
            "    num_agent_steps_sampled: 348000\n",
            "    num_steps_sampled: 348000\n",
            "    num_steps_trained: 348000\n",
            "  iterations_since_restore: 87\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.74838709677419\n",
            "    ram_util_percent: 16.300000000000004\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.0486648601379309\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.444000925957009\n",
            "    mean_inference_ms: 1.2921158777466326\n",
            "    mean_raw_obs_processing_ms: 0.15939813547180282\n",
            "  time_since_restore: 1834.4341847896576\n",
            "  time_this_iter_s: 21.29949116706848\n",
            "  time_total_s: 1834.4341847896576\n",
            "  timers:\n",
            "    learn_throughput: 525.067\n",
            "    learn_time_ms: 7618.068\n",
            "    sample_throughput: 277.114\n",
            "    sample_time_ms: 14434.517\n",
            "    update_time_ms: 1.701\n",
            "  timestamp: 1619352950\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 348000\n",
            "  training_iteration: 87\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:15:50,863\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=348148, mean_mean=16.170050638073636, mean_std=9.149933941961041), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.2/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         1834.43</td><td style=\"text-align: right;\">348000</td><td style=\"text-align: right;\">-0.482436</td><td style=\"text-align: right;\">           -0.456222</td><td style=\"text-align: right;\">           -0.514437</td><td style=\"text-align: right;\">           3243.72</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 352000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-16-12\n",
            "  done: false\n",
            "  episode_len_mean: 3283.03\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4562224048973671\n",
            "  episode_reward_mean: -0.48217580368704305\n",
            "  episode_reward_min: -0.5144373548234609\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 148\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 9.147955830346444e-21\n",
            "          cur_lr: 7.520000000000001e-06\n",
            "          entropy: 0.12228187243454158\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0027465862476674374\n",
            "          policy_loss: -0.023625498462934047\n",
            "          total_loss: 0.05997274295077659\n",
            "          vf_explained_var: 0.6871459484100342\n",
            "          vf_loss: 0.16964212525635958\n",
            "    num_agent_steps_sampled: 352000\n",
            "    num_steps_sampled: 352000\n",
            "    num_steps_trained: 352000\n",
            "  iterations_since_restore: 88\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.848387096774193\n",
            "    ram_util_percent: 16.300000000000004\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.048658516290227896\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.4519117777287829\n",
            "    mean_inference_ms: 1.2919780714940783\n",
            "    mean_raw_obs_processing_ms: 0.15942598497269883\n",
            "  time_since_restore: 1855.9508037567139\n",
            "  time_this_iter_s: 21.516618967056274\n",
            "  time_total_s: 1855.9508037567139\n",
            "  timers:\n",
            "    learn_throughput: 526.084\n",
            "    learn_time_ms: 7603.351\n",
            "    sample_throughput: 278.072\n",
            "    sample_time_ms: 14384.761\n",
            "    update_time_ms: 1.708\n",
            "  timestamp: 1619352972\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 352000\n",
            "  training_iteration: 88\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:16:12,418\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=352149, mean_mean=16.16982126389161, mean_std=9.14948617460836), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.2/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         1855.95</td><td style=\"text-align: right;\">352000</td><td style=\"text-align: right;\">-0.482176</td><td style=\"text-align: right;\">           -0.456222</td><td style=\"text-align: right;\">           -0.514437</td><td style=\"text-align: right;\">           3283.03</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:16:12,454\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': { 'net_worth': 77318.864856,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                             'step': 2632},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=-1.252, max=1.999, mean=0.531),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': 1,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': 0.0010566709304264954,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:16:12,457\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-1.377, max=1.373, mean=-0.002),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-0.062, max=-0.062, mean=-0.062),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.94, max=0.94, mean=0.94),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.642, max=-0.642, mean=-0.642)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:16:12,463\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=-2.472, max=72.207, mean=22.338)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:16:12,464\tINFO sampler.py:589 -- Info return from env: { 0: { 'agent0': { 'net_worth': 77290.944154802,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                    'step': 2633}}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:16:12,464\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=-2.472, max=72.207, mean=22.338)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:16:12,464\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=-1.252, max=1.994, mean=0.535)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:16:13,259\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-12.275, max=12.277, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-1.842, max=0.0, mean=-0.076),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((200,), dtype=float32, min=0.158, max=1.0, mean=0.944),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.525),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-1.13, max=1.478, mean=0.043),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=12066785.0, max=12066785.0, mean=12066785.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 2633, 'net_worth': 77290.944154802}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-5.989, max=4.869, mean=-0.109),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((200, 25, 3), dtype=float32, min=-5.989, max=4.867, mean=-0.108),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.045),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=1908.0, max=1908.0, mean=1908.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.045),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-1.177, max=1.154, mean=-0.088)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:16:13,261\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-12.275, max=12.277, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-1.842, max=0.0, mean=-0.076),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.158, max=1.0, mean=0.944),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.525),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-1.13, max=1.478, mean=0.043),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=12066785.0, max=12066785.0, mean=12066785.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 2633, 'net_worth': 77290.944154802}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-5.989, max=4.869, mean=-0.109),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-5.989, max=4.867, mean=-0.108),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.045),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=1908.0, max=1908.0, mean=1908.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.045),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-1.177, max=1.154, mean=-0.088)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:16:13,266\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:16:28,067\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:16:28,070\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-10.722, max=10.736, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((128,), dtype=float32, min=-0.802, max=0.0, mean=-0.064),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((128,), dtype=float32, min=0.448, max=1.0, mean=0.947),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.359),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((128,), dtype=float32, min=-4.371, max=2.544, mean=0.066),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((128,), dtype=int64, min=12066785.0, max=1360899568.0, mean=675945420.383),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((128,), dtype=object, head={'step': 2904, 'net_worth': 73924.37}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((128, 25, 3), dtype=float32, min=-10.353, max=8.655, mean=0.033),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((128, 25, 3), dtype=float32, min=-10.35, max=8.651, mean=0.033),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.016),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((128,), dtype=int64, min=1908.0, max=1928.0, mean=1918.555),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.016),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((128,), dtype=float32, min=-1.024, max=1.141, mean=-0.048)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:16:28,080\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 4.573977915173222e-21,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 7.48e-06,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.12478412687778473,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': 4.2737424710281857e-10,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': -0.06572629511356354,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': 0.018771959468722343,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.659, max=0.659, mean=0.659),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.17149221897125244},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 356000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-16-35\n",
            "  done: false\n",
            "  episode_len_mean: 3321.98\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4562224048973671\n",
            "  episode_reward_mean: -0.4818411976384733\n",
            "  episode_reward_min: -0.5144373548234609\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 149\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 4.573977915173222e-21\n",
            "          cur_lr: 7.48e-06\n",
            "          entropy: 0.12281950446777046\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0032372820132877678\n",
            "          policy_loss: -0.02040401892736554\n",
            "          total_loss: 0.0677489023655653\n",
            "          vf_explained_var: 0.665752649307251\n",
            "          vf_loss: 0.17876223335042596\n",
            "    num_agent_steps_sampled: 356000\n",
            "    num_steps_sampled: 356000\n",
            "    num_steps_trained: 356000\n",
            "  iterations_since_restore: 89\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.957575757575757\n",
            "    ram_util_percent: 16.3\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04865211059900651\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.4598593232752517\n",
            "    mean_inference_ms: 1.291867866606109\n",
            "    mean_raw_obs_processing_ms: 0.15945386984245202\n",
            "  time_since_restore: 1879.1268000602722\n",
            "  time_this_iter_s: 23.17599630355835\n",
            "  time_total_s: 1879.1268000602722\n",
            "  timers:\n",
            "    learn_throughput: 526.633\n",
            "    learn_time_ms: 7595.428\n",
            "    sample_throughput: 278.171\n",
            "    sample_time_ms: 14379.668\n",
            "    update_time_ms: 1.685\n",
            "  timestamp: 1619352995\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 356000\n",
            "  training_iteration: 89\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:16:35,632\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=356150, mean_mean=16.17019267218996, mean_std=9.145867496473773), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.2/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         1879.13</td><td style=\"text-align: right;\">356000</td><td style=\"text-align: right;\">-0.481841</td><td style=\"text-align: right;\">           -0.456222</td><td style=\"text-align: right;\">           -0.514437</td><td style=\"text-align: right;\">           3321.98</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 360000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-16-58\n",
            "  done: false\n",
            "  episode_len_mean: 3360.95\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4562224048973671\n",
            "  episode_reward_mean: -0.4814049106114162\n",
            "  episode_reward_min: -0.5144373548234609\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 150\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 2.286988957586611e-21\n",
            "          cur_lr: 7.440000000000001e-06\n",
            "          entropy: 0.12859400385059416\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0026388444457552396\n",
            "          policy_loss: -0.020575276343151927\n",
            "          total_loss: 0.06924792425706983\n",
            "          vf_explained_var: 0.6642929911613464\n",
            "          vf_loss: 0.18221828574314713\n",
            "    num_agent_steps_sampled: 360000\n",
            "    num_steps_sampled: 360000\n",
            "    num_steps_trained: 360000\n",
            "  iterations_since_restore: 90\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.856249999999996\n",
            "    ram_util_percent: 16.3\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.048645598529868925\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.4678545998235555\n",
            "    mean_inference_ms: 1.2917524349766267\n",
            "    mean_raw_obs_processing_ms: 0.15948167502668056\n",
            "  time_since_restore: 1901.5239152908325\n",
            "  time_this_iter_s: 22.397115230560303\n",
            "  time_total_s: 1901.5239152908325\n",
            "  timers:\n",
            "    learn_throughput: 527.596\n",
            "    learn_time_ms: 7581.557\n",
            "    sample_throughput: 277.589\n",
            "    sample_time_ms: 14409.812\n",
            "    update_time_ms: 1.728\n",
            "  timestamp: 1619353018\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 360000\n",
            "  training_iteration: 90\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:16:58,066\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=360151, mean_mean=16.166936458136814, mean_std=9.139309097996382), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.2/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         1901.52</td><td style=\"text-align: right;\">360000</td><td style=\"text-align: right;\">-0.481405</td><td style=\"text-align: right;\">           -0.456222</td><td style=\"text-align: right;\">           -0.514437</td><td style=\"text-align: right;\">           3360.95</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:17:13,268\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((139, 2), dtype=float32, min=-9.205, max=9.211, mean=-0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((139,), dtype=float32, min=-4.629, max=0.0, mean=-0.192),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((139,), dtype=float32, min=0.01, max=1.0, mean=0.891),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((139,), dtype=int64, min=0.0, max=1.0, mean=0.59),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((139,), dtype=float32, min=-2.196, max=1.521, mean=-0.112),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((139,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((139,), dtype=bool, min=0.0, max=1.0, mean=0.007),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((139,), dtype=int64, min=1240689809.0, max=1240689809.0, mean=1240689809.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((139,), dtype=object, head={'step': 4732, 'net_worth': 40336.984047059996}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((139, 25, 3), dtype=float32, min=-6.233, max=6.94, mean=0.082),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((139, 25, 3), dtype=float32, min=-6.233, max=6.94, mean=0.091),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((139,), dtype=float32, min=-1.0, max=1.0, mean=-0.115),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((139,), dtype=int64, min=1969.0, max=1969.0, mean=1969.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((139,), dtype=float32, min=-1.0, max=1.0, mean=-0.115),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((139,), dtype=float32, min=-1.222, max=1.956, mean=-0.003)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:17:13,283\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': { 'net_worth': 35128.05293413,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                             'step': 4870},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=-4.203, max=0.022, mean=-1.334),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': None,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:17:13,285\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-4.163, max=4.167, mean=0.002),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-0.0, max=-0.0, mean=-0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=1.419, max=1.419, mean=1.419)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:17:13,286\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=0.0, max=100.0, mean=1.338)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:17:13,286\tINFO sampler.py:589 -- Info return from env: {0: {'agent0': {'net_worth': 58743.0, 'step': 1}}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:17:13,287\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=0.0, max=100.0, mean=1.338)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:17:13,287\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=-4.188, max=4.413, mean=-1.217)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:17:13,419\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-16.664, max=16.667, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-4.629, max=0.0, mean=-0.169),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.01, max=1.0, mean=0.904),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.49),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-2.196, max=1.521, mean=-0.076),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.005),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=1097239841.0, max=1240689809.0, mean=1196937568.76),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 4732, 'net_worth': 40336.984047059996}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-6.233, max=6.94, mean=-0.137),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-6.233, max=6.94, mean=-0.139),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.07),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=1969.0, max=1970.0, mean=1969.305),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.07),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-1.222, max=1.956, mean=0.006)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 364000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-17-20\n",
            "  done: false\n",
            "  episode_len_mean: 3399.55\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4562224048973671\n",
            "  episode_reward_mean: -0.48097231505137145\n",
            "  episode_reward_min: -0.5144373548234609\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 151\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.1434944787933055e-21\n",
            "          cur_lr: 7.400000000000001e-06\n",
            "          entropy: 0.12355700577609241\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.002532572965719737\n",
            "          policy_loss: -0.014796934090554714\n",
            "          total_loss: 0.07052095542894676\n",
            "          vf_explained_var: 0.6758089065551758\n",
            "          vf_loss: 0.17310692416504025\n",
            "    num_agent_steps_sampled: 364000\n",
            "    num_steps_sampled: 364000\n",
            "    num_steps_trained: 364000\n",
            "  iterations_since_restore: 91\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.868750000000002\n",
            "    ram_util_percent: 16.3\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04863900698962095\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.4759077666714786\n",
            "    mean_inference_ms: 1.291632674065942\n",
            "    mean_raw_obs_processing_ms: 0.15950966590536445\n",
            "  time_since_restore: 1924.3910193443298\n",
            "  time_this_iter_s: 22.867104053497314\n",
            "  time_total_s: 1924.3910193443298\n",
            "  timers:\n",
            "    learn_throughput: 528.029\n",
            "    learn_time_ms: 7575.338\n",
            "    sample_throughput: 274.182\n",
            "    sample_time_ms: 14588.829\n",
            "    update_time_ms: 1.731\n",
            "  timestamp: 1619353040\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 364000\n",
            "  training_iteration: 91\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:17:20,974\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=364152, mean_mean=16.16816097636165, mean_std=9.135977328964636), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.2/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         1924.39</td><td style=\"text-align: right;\">364000</td><td style=\"text-align: right;\">-0.480972</td><td style=\"text-align: right;\">           -0.456222</td><td style=\"text-align: right;\">           -0.514437</td><td style=\"text-align: right;\">           3399.55</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:17:21,008\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:17:34,523\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:17:34,526\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-8.625, max=8.631, mean=-0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((128,), dtype=float32, min=-2.726, max=0.0, mean=-0.11),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((128,), dtype=float32, min=0.065, max=1.0, mean=0.935),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.461),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((128,), dtype=float32, min=-5.476, max=4.074, mean=0.104),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((128,), dtype=int64, min=1097239841.0, max=1097239841.0, mean=1097239841.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((128,), dtype=object, head={'step': 2517, 'net_worth': 75536.45206514001}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((128, 25, 3), dtype=float32, min=-12.022, max=16.153, mean=0.016),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((128, 25, 3), dtype=float32, min=-12.019, max=16.147, mean=0.016),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=0.047),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((128,), dtype=int64, min=1971.0, max=1990.0, mean=1980.695),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=0.047),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((128,), dtype=float32, min=-1.067, max=1.213, mean=0.005)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:17:34,536\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 5.717472393966528e-22,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 7.360000000000001e-06,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.11926805973052979,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': 8.329639200610472e-09,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': -0.10396933555603027,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': 0.017055464908480644,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.554, max=0.554, mean=0.554),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.24443493783473969},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 368000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-17-42\n",
            "  done: false\n",
            "  episode_len_mean: 3399.55\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4562224048973671\n",
            "  episode_reward_mean: -0.4809723150513714\n",
            "  episode_reward_min: -0.5144373548234609\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 151\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 5.717472393966528e-22\n",
            "          cur_lr: 7.360000000000001e-06\n",
            "          entropy: 0.12353346263989806\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.002540987334214151\n",
            "          policy_loss: -0.01884176125167869\n",
            "          total_loss: 0.053723140590591356\n",
            "          vf_explained_var: 0.737580418586731\n",
            "          vf_loss: 0.1476004630094394\n",
            "    num_agent_steps_sampled: 368000\n",
            "    num_steps_sampled: 368000\n",
            "    num_steps_trained: 368000\n",
            "  iterations_since_restore: 92\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.912903225806453\n",
            "    ram_util_percent: 16.300000000000004\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04863900698962095\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.4759077666714786\n",
            "    mean_inference_ms: 1.2916326740659423\n",
            "    mean_raw_obs_processing_ms: 0.15950966590536447\n",
            "  time_since_restore: 1945.5639905929565\n",
            "  time_this_iter_s: 21.17297124862671\n",
            "  time_total_s: 1945.5639905929565\n",
            "  timers:\n",
            "    learn_throughput: 527.319\n",
            "    learn_time_ms: 7585.542\n",
            "    sample_throughput: 275.094\n",
            "    sample_time_ms: 14540.465\n",
            "    update_time_ms: 1.74\n",
            "  timestamp: 1619353062\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 368000\n",
            "  training_iteration: 92\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:17:42,183\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=368152, mean_mean=16.171195984540848, mean_std=9.131402046793081), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.2/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         1945.56</td><td style=\"text-align: right;\">368000</td><td style=\"text-align: right;\">-0.480972</td><td style=\"text-align: right;\">           -0.456222</td><td style=\"text-align: right;\">           -0.514437</td><td style=\"text-align: right;\">           3399.55</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 372000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-18-03\n",
            "  done: false\n",
            "  episode_len_mean: 3436.86\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4562224048973671\n",
            "  episode_reward_mean: -0.4805304616157135\n",
            "  episode_reward_min: -0.5144373548234609\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 152\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 2.858736196983264e-22\n",
            "          cur_lr: 7.320000000000001e-06\n",
            "          entropy: 0.12684136093594134\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.003183559758326737\n",
            "          policy_loss: -0.0233189458376728\n",
            "          total_loss: 0.049229731783270836\n",
            "          vf_explained_var: 0.73520427942276\n",
            "          vf_loss: 0.14763418398797512\n",
            "    num_agent_steps_sampled: 372000\n",
            "    num_steps_sampled: 372000\n",
            "    num_steps_trained: 372000\n",
            "  iterations_since_restore: 93\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.846666666666668\n",
            "    ram_util_percent: 16.300000000000004\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04863082190618575\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.4836725818905134\n",
            "    mean_inference_ms: 1.291510371785839\n",
            "    mean_raw_obs_processing_ms: 0.1595326037706156\n",
            "  time_since_restore: 1966.7231624126434\n",
            "  time_this_iter_s: 21.15917181968689\n",
            "  time_total_s: 1966.7231624126434\n",
            "  timers:\n",
            "    learn_throughput: 527.93\n",
            "    learn_time_ms: 7576.755\n",
            "    sample_throughput: 276.943\n",
            "    sample_time_ms: 14443.396\n",
            "    update_time_ms: 1.739\n",
            "  timestamp: 1619353083\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 372000\n",
            "  training_iteration: 93\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:18:03,380\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=372153, mean_mean=16.17265718831399, mean_std=9.132998528749956), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.2/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         1966.72</td><td style=\"text-align: right;\">372000</td><td style=\"text-align: right;\">-0.48053</td><td style=\"text-align: right;\">           -0.456222</td><td style=\"text-align: right;\">           -0.514437</td><td style=\"text-align: right;\">           3436.86</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 376000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-18-25\n",
            "  done: false\n",
            "  episode_len_mean: 3475.47\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4562224048973671\n",
            "  episode_reward_mean: -0.4801043094005136\n",
            "  episode_reward_min: -0.5144373548234609\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 153\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.429368098491632e-22\n",
            "          cur_lr: 7.280000000000001e-06\n",
            "          entropy: 0.11822745762765408\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0028458995002438314\n",
            "          policy_loss: -0.02797413778898772\n",
            "          total_loss: 0.05311323277419433\n",
            "          vf_explained_var: 0.693484902381897\n",
            "          vf_loss: 0.16453929035924375\n",
            "    num_agent_steps_sampled: 376000\n",
            "    num_steps_sampled: 376000\n",
            "    num_steps_trained: 376000\n",
            "  iterations_since_restore: 94\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.81612903225807\n",
            "    ram_util_percent: 16.303225806451614\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04862256303856994\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.4914643442060092\n",
            "    mean_inference_ms: 1.2913833577588185\n",
            "    mean_raw_obs_processing_ms: 0.15955539218779863\n",
            "  time_since_restore: 1988.453575372696\n",
            "  time_this_iter_s: 21.73041296005249\n",
            "  time_total_s: 1988.453575372696\n",
            "  timers:\n",
            "    learn_throughput: 529.81\n",
            "    learn_time_ms: 7549.873\n",
            "    sample_throughput: 277.429\n",
            "    sample_time_ms: 14418.083\n",
            "    update_time_ms: 1.739\n",
            "  timestamp: 1619353105\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 376000\n",
            "  training_iteration: 94\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:18:25,148\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=376154, mean_mean=16.170620329696543, mean_std=9.131626041751526), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.2/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         1988.45</td><td style=\"text-align: right;\">376000</td><td style=\"text-align: right;\">-0.480104</td><td style=\"text-align: right;\">           -0.456222</td><td style=\"text-align: right;\">           -0.514437</td><td style=\"text-align: right;\">           3475.47</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:18:25,184\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': { 'net_worth': 84028.64,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                             'step': 2398},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=-4.761, max=2.411, mean=-0.306),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': -0.0029999412028972605,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:18:25,186\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-1.193, max=1.201, mean=0.004),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-0.087, max=-0.087, mean=-0.087),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.916, max=0.916, mean=0.916),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.96, max=0.96, mean=0.96)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:18:25,194\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=-42.876, max=63.402, mean=12.162)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:18:25,194\tINFO sampler.py:589 -- Info return from env: { 0: { 'agent0': { 'net_worth': 84252.6945203375,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                    'step': 2399}}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:18:25,194\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=-42.876, max=63.402, mean=12.162)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:18:25,195\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=-4.763, max=1.79, mean=-0.344)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:18:25,944\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-9.276, max=9.273, mean=-0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-3.185, max=0.0, mean=-0.145),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((200,), dtype=float32, min=0.041, max=1.0, mean=0.906),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.595),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-1.613, max=1.605, mean=0.013),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=146120695.0, max=146120695.0, mean=146120695.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 2399, 'net_worth': 84252.6945203375}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.776, max=3.498, mean=0.205),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.776, max=3.498, mean=0.203),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.055),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=2033.0, max=2033.0, mean=2033.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.055),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-1.168, max=1.131, mean=-0.068)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:18:25,946\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-9.276, max=9.273, mean=-0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-3.185, max=0.0, mean=-0.145),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.041, max=1.0, mean=0.906),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.595),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-1.613, max=1.605, mean=0.013),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=146120695.0, max=146120695.0, mean=146120695.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 2399, 'net_worth': 84252.6945203375}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.776, max=3.498, mean=0.205),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-4.776, max=3.498, mean=0.203),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.055),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=2033.0, max=2033.0, mean=2033.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.055),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-1.168, max=1.131, mean=-0.068)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:18:25,950\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:18:40,244\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:18:40,247\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-8.765, max=8.764, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((128,), dtype=float32, min=-4.464, max=0.0, mean=-0.234),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((128,), dtype=float32, min=0.012, max=1.0, mean=0.883),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.375),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((128,), dtype=float32, min=-2.535, max=2.788, mean=-0.122),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((128,), dtype=int64, min=146120695.0, max=1386722901.0, mean=533808884.375),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((128,), dtype=object, head={'step': 281, 'net_worth': 65560.27}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((128, 25, 3), dtype=float32, min=-11.999, max=16.121, mean=-0.066),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((128, 25, 3), dtype=float32, min=-11.996, max=16.116, mean=-0.061),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.195),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((128,), dtype=int64, min=2033.0, max=2053.0, mean=2042.516),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.195),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((128,), dtype=float32, min=-1.222, max=1.654, mean=-0.135)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:18:40,257\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 7.14684049245816e-23,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 7.240000000000001e-06,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.14552822709083557,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': -7.083332809187937e-10,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': 0.12223900854587555,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': 0.18922968208789825,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.725, max=0.725, mean=0.725),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.1368919014930725},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 380000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-18-47\n",
            "  done: false\n",
            "  episode_len_mean: 3515.11\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.45445558973502875\n",
            "  episode_reward_mean: -0.47956986680671887\n",
            "  episode_reward_min: -0.5144373548234609\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 154\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 7.14684049245816e-23\n",
            "          cur_lr: 7.240000000000001e-06\n",
            "          entropy: 0.12439095694571733\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0035264198668301105\n",
            "          policy_loss: -0.020991377561585978\n",
            "          total_loss: 0.08165668116998859\n",
            "          vf_explained_var: 0.6192762851715088\n",
            "          vf_loss: 0.20778393652290106\n",
            "    num_agent_steps_sampled: 380000\n",
            "    num_steps_sampled: 380000\n",
            "    num_steps_trained: 380000\n",
            "  iterations_since_restore: 95\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 29.078125\n",
            "    ram_util_percent: 16.30625\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04861425872449525\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.4993029607925694\n",
            "    mean_inference_ms: 1.2912530477106725\n",
            "    mean_raw_obs_processing_ms: 0.15957830670196438\n",
            "  time_since_restore: 2011.071042060852\n",
            "  time_this_iter_s: 22.617466688156128\n",
            "  time_total_s: 2011.071042060852\n",
            "  timers:\n",
            "    learn_throughput: 529.438\n",
            "    learn_time_ms: 7555.183\n",
            "    sample_throughput: 277.524\n",
            "    sample_time_ms: 14413.168\n",
            "    update_time_ms: 1.726\n",
            "  timestamp: 1619353127\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 380000\n",
            "  training_iteration: 95\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.2/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         2011.07</td><td style=\"text-align: right;\">380000</td><td style=\"text-align: right;\">-0.47957</td><td style=\"text-align: right;\">           -0.454456</td><td style=\"text-align: right;\">           -0.514437</td><td style=\"text-align: right;\">           3515.11</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:18:47,803\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=380155, mean_mean=16.169120812257443, mean_std=9.124400639639397), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 384000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-19-10\n",
            "  done: false\n",
            "  episode_len_mean: 3554.99\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.45445558973502875\n",
            "  episode_reward_mean: -0.4790584024041668\n",
            "  episode_reward_min: -0.5144373548234609\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 155\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 3.57342024622908e-23\n",
            "          cur_lr: 7.2000000000000005e-06\n",
            "          entropy: 0.1251690466888249\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0037970449557178654\n",
            "          policy_loss: -0.025831731531070545\n",
            "          total_loss: 0.06678490340709686\n",
            "          vf_explained_var: 0.65619957447052\n",
            "          vf_loss: 0.1877366527915001\n",
            "    num_agent_steps_sampled: 384000\n",
            "    num_steps_sampled: 384000\n",
            "    num_steps_trained: 384000\n",
            "  iterations_since_restore: 96\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.87272727272727\n",
            "    ram_util_percent: 16.4\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04860421161611973\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.5070134303752385\n",
            "    mean_inference_ms: 1.2910158694461706\n",
            "    mean_raw_obs_processing_ms: 0.15959290208251875\n",
            "  time_since_restore: 2033.9030663967133\n",
            "  time_this_iter_s: 22.832024335861206\n",
            "  time_total_s: 2033.9030663967133\n",
            "  timers:\n",
            "    learn_throughput: 529.444\n",
            "    learn_time_ms: 7555.089\n",
            "    sample_throughput: 275.604\n",
            "    sample_time_ms: 14513.599\n",
            "    update_time_ms: 1.748\n",
            "  timestamp: 1619353150\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 384000\n",
            "  training_iteration: 96\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.2/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">          2033.9</td><td style=\"text-align: right;\">384000</td><td style=\"text-align: right;\">-0.479058</td><td style=\"text-align: right;\">           -0.454456</td><td style=\"text-align: right;\">           -0.514437</td><td style=\"text-align: right;\">           3554.99</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:19:10,670\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=384156, mean_mean=16.168673418270522, mean_std=9.120729666165776), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 388000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-19-32\n",
            "  done: false\n",
            "  episode_len_mean: 3554.99\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.45445558973502875\n",
            "  episode_reward_mean: -0.47905840240416686\n",
            "  episode_reward_min: -0.5144373548234609\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 155\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.78671012311454e-23\n",
            "          cur_lr: 7.160000000000001e-06\n",
            "          entropy: 0.12230481812730432\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0022432550795201678\n",
            "          policy_loss: -0.018157828380935825\n",
            "          total_loss: 0.06354395212838426\n",
            "          vf_explained_var: 0.7005819082260132\n",
            "          vf_loss: 0.16584965749643743\n",
            "    num_agent_steps_sampled: 388000\n",
            "    num_steps_sampled: 388000\n",
            "    num_steps_trained: 388000\n",
            "  iterations_since_restore: 97\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.777419354838706\n",
            "    ram_util_percent: 16.399999999999995\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04860421161611971\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.5070134303752383\n",
            "    mean_inference_ms: 1.2910158694461706\n",
            "    mean_raw_obs_processing_ms: 0.15959290208251875\n",
            "  time_since_restore: 2055.8480894565582\n",
            "  time_this_iter_s: 21.94502305984497\n",
            "  time_total_s: 2055.8480894565582\n",
            "  timers:\n",
            "    learn_throughput: 529.601\n",
            "    learn_time_ms: 7552.85\n",
            "    sample_throughput: 274.339\n",
            "    sample_time_ms: 14580.508\n",
            "    update_time_ms: 1.707\n",
            "  timestamp: 1619353172\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 388000\n",
            "  training_iteration: 97\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:19:32,654\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=388156, mean_mean=16.1720043547763, mean_std=9.114295616057085), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.2/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         2055.85</td><td style=\"text-align: right;\">388000</td><td style=\"text-align: right;\">-0.479058</td><td style=\"text-align: right;\">           -0.454456</td><td style=\"text-align: right;\">           -0.514437</td><td style=\"text-align: right;\">           3554.99</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:19:32,688\tINFO sampler.py:997 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'info': { 'net_worth': 47026.04,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                             'step': 4418},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'obs': np.ndarray((25, 3), dtype=float64, min=-4.888, max=3.015, mean=-1.503),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_action': 0,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'prev_reward': -0.002999953711620429,\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:19:32,691\tINFO sampler.py:1015 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-4.297, max=4.291, mean=-0.003),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-0.0, max=-0.0, mean=-0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.242, max=-0.242, mean=-0.242)})}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:19:32,700\tINFO sampler.py:588 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 3), dtype=float32, min=-74.851, max=43.272, mean=-1.048)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:19:32,700\tINFO sampler.py:589 -- Info return from env: {0: {'agent0': {'net_worth': 47026.04, 'step': 4419}}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:19:32,700\tINFO sampler.py:810 -- Preprocessed obs: np.ndarray((25, 3), dtype=float32, min=-74.851, max=43.272, mean=-1.048)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:19:32,701\tINFO sampler.py:814 -- Filtered obs: np.ndarray((25, 3), dtype=float64, min=-4.889, max=3.015, mean=-1.502)\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:19:33,695\tINFO simple_list_collector.py:685 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-11.801, max=11.817, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-2.65, max=0.0, mean=-0.09),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'action_prob': np.ndarray((200,), dtype=float32, min=0.071, max=1.0, mean=0.947),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.195),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-1.33, max=0.964, mean=-0.008),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=1812105144.0, max=1812105144.0, mean=1812105144.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 4419, 'net_worth': 47026.04}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-7.243, max=9.213, mean=-0.312),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'obs': np.ndarray((200, 25, 3), dtype=float32, min=-7.243, max=9.213, mean=-0.318),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.135),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=2095.0, max=2095.0, mean=2095.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.135),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-1.217, max=1.005, mean=-0.127)}}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:19:33,697\tINFO rollout_worker.py:742 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-11.801, max=11.817, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-2.65, max=0.0, mean=-0.09),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'action_prob': np.ndarray((200,), dtype=float32, min=0.071, max=1.0, mean=0.947),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.195),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-1.33, max=0.964, mean=-0.008),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=1812105144.0, max=1812105144.0, mean=1812105144.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'infos': np.ndarray((200,), dtype=object, head={'step': 4419, 'net_worth': 47026.04}),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'new_obs': np.ndarray((200, 25, 3), dtype=float32, min=-7.243, max=9.213, mean=-0.312),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'obs': np.ndarray((200, 25, 3), dtype=float32, min=-7.243, max=9.213, mean=-0.318),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.135),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=2095.0, max=2095.0, mean=2095.0),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.135),\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-1.217, max=1.005, mean=-0.127)}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m 2021-04-25 12:19:33,701\tINFO rollout_worker.py:704 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:19:47,673\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:19:47,676\tINFO rollout_worker.py:883 -- Training on concatenated sample batches:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'count': 128,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-9.428, max=9.443, mean=0.001),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_logp': np.ndarray((128,), dtype=float32, min=-1.9, max=0.0, mean=-0.065),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'action_prob': np.ndarray((128,), dtype=float32, min=0.15, max=1.0, mean=0.957),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.484),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'advantages': np.ndarray((128,), dtype=float32, min=-2.743, max=3.436, mean=0.223),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'eps_id': np.ndarray((128,), dtype=int64, min=1475384822.0, max=1812105144.0, mean=1509582979.703),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'infos': np.ndarray((128,), dtype=object, head={'step': 1302, 'net_worth': 70747.97}),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'new_obs': np.ndarray((128, 25, 3), dtype=float32, min=-10.261, max=9.632, mean=-0.013),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'obs': np.ndarray((128, 25, 3), dtype=float32, min=-10.258, max=9.629, mean=-0.008),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'rewards': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=0.109),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'unroll_id': np.ndarray((128,), dtype=int64, min=2095.0, max=2115.0, mean=2105.57),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=0.109),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                           'vf_preds': np.ndarray((128,), dtype=float32, min=-1.1, max=1.208, mean=0.024)}},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:19:47,686\tDEBUG rollout_worker.py:909 -- Training out:\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'learner_stats': { 'allreduce_latency': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_kl_coeff': 8.9335506155727e-24,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'cur_lr': 7.120000000000001e-06,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy': 0.08272892236709595,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'entropy_coeff': 0.01,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'kl': -1.6918256795506181e-09,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'policy_loss': -0.22307178378105164,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'total_loss': -0.14460867643356323,\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.733, max=0.733, mean=0.733),\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                                          'vf_loss': 0.15858078002929688},\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m                       'model': {}}}\n",
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 392000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-19-55\n",
            "  done: false\n",
            "  episode_len_mean: 3592.54\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.45445558973502875\n",
            "  episode_reward_mean: -0.47872113285590673\n",
            "  episode_reward_min: -0.5144373548234609\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 156\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 8.9335506155727e-24\n",
            "          cur_lr: 7.120000000000001e-06\n",
            "          entropy: 0.1238850790541619\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0020793292205780745\n",
            "          policy_loss: -0.024007245985558257\n",
            "          total_loss: 0.04585509918979369\n",
            "          vf_explained_var: 0.7522255182266235\n",
            "          vf_loss: 0.14220239059068263\n",
            "    num_agent_steps_sampled: 392000\n",
            "    num_steps_sampled: 392000\n",
            "    num_steps_trained: 392000\n",
            "  iterations_since_restore: 98\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 29.728125\n",
            "    ram_util_percent: 16.4\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04859407859527427\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.5147636918739726\n",
            "    mean_inference_ms: 1.2908029049587222\n",
            "    mean_raw_obs_processing_ms: 0.1596072349126093\n",
            "  time_since_restore: 2078.2987821102142\n",
            "  time_this_iter_s: 22.450692653656006\n",
            "  time_total_s: 2078.2987821102142\n",
            "  timers:\n",
            "    learn_throughput: 529.927\n",
            "    learn_time_ms: 7548.211\n",
            "    sample_throughput: 272.507\n",
            "    sample_time_ms: 14678.535\n",
            "    update_time_ms: 1.704\n",
            "  timestamp: 1619353195\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 392000\n",
            "  training_iteration: 98\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.2/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">          2078.3</td><td style=\"text-align: right;\">392000</td><td style=\"text-align: right;\">-0.478721</td><td style=\"text-align: right;\">           -0.454456</td><td style=\"text-align: right;\">           -0.514437</td><td style=\"text-align: right;\">           3592.54</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:19:55,140\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=392157, mean_mean=16.173048395126802, mean_std=9.116044967182365), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 396000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-20-16\n",
            "  done: false\n",
            "  episode_len_mean: 3631.59\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4541665655121454\n",
            "  episode_reward_mean: -0.4782425557854847\n",
            "  episode_reward_min: -0.5144373548234609\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 157\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 4.46677530778635e-24\n",
            "          cur_lr: 7.080000000000001e-06\n",
            "          entropy: 0.11795361689291894\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.003137676940241363\n",
            "          policy_loss: -0.026176609098911285\n",
            "          total_loss: 0.04879694082774222\n",
            "          vf_explained_var: 0.7179166674613953\n",
            "          vf_loss: 0.15230616996996105\n",
            "    num_agent_steps_sampled: 396000\n",
            "    num_steps_sampled: 396000\n",
            "    num_steps_trained: 396000\n",
            "  iterations_since_restore: 99\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.74193548387097\n",
            "    ram_util_percent: 16.399999999999995\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.04858380859484843\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.5225321098088826\n",
            "    mean_inference_ms: 1.2905825046508477\n",
            "    mean_raw_obs_processing_ms: 0.1596211734233816\n",
            "  time_since_restore: 2099.7469203472137\n",
            "  time_this_iter_s: 21.44813823699951\n",
            "  time_total_s: 2099.7469203472137\n",
            "  timers:\n",
            "    learn_throughput: 529.77\n",
            "    learn_time_ms: 7550.449\n",
            "    sample_throughput: 275.796\n",
            "    sample_time_ms: 14503.458\n",
            "    update_time_ms: 1.726\n",
            "  timestamp: 1619353216\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 396000\n",
            "  training_iteration: 99\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:20:16,624\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=396158, mean_mean=16.171446381893507, mean_std=9.116362029350288), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.2/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         2099.75</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">-0.478243</td><td style=\"text-align: right;\">           -0.454167</td><td style=\"text-align: right;\">           -0.514437</td><td style=\"text-align: right;\">           3631.59</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=1937)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_TradingEnv_ac7e4_00000:\n",
            "  agent_timesteps_total: 400000\n",
            "  custom_metrics: {}\n",
            "  date: 2021-04-25_12-20-38\n",
            "  done: true\n",
            "  episode_len_mean: 3668.19\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -0.4541665655121454\n",
            "  episode_reward_mean: -0.47779091273961777\n",
            "  episode_reward_min: -0.5144373548234609\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 158\n",
            "  experiment_id: 1d0d8d9306544d18a68c76bab8637070\n",
            "  hostname: 1203eb0d8193\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 2.233387653893175e-24\n",
            "          cur_lr: 7.04e-06\n",
            "          entropy: 0.11825826368294656\n",
            "          entropy_coeff: 0.01\n",
            "          kl: 0.0022050408624636475\n",
            "          policy_loss: -0.015952558489516377\n",
            "          total_loss: 0.06897426495561376\n",
            "          vf_explained_var: 0.6788142919540405\n",
            "          vf_loss: 0.17221880960278213\n",
            "    num_agent_steps_sampled: 400000\n",
            "    num_steps_sampled: 400000\n",
            "    num_steps_trained: 400000\n",
            "  iterations_since_restore: 100\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 28.77741935483871\n",
            "    ram_util_percent: 16.399999999999995\n",
            "  pid: 1909\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.048573480695865125\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.5303351434614496\n",
            "    mean_inference_ms: 1.290357434197812\n",
            "    mean_raw_obs_processing_ms: 0.15963506391764792\n",
            "  time_since_restore: 2121.9513263702393\n",
            "  time_this_iter_s: 22.204406023025513\n",
            "  time_total_s: 2121.9513263702393\n",
            "  timers:\n",
            "    learn_throughput: 529.62\n",
            "    learn_time_ms: 7552.58\n",
            "    sample_throughput: 276.202\n",
            "    sample_time_ms: 14482.149\n",
            "    update_time_ms: 1.684\n",
            "  timestamp: 1619353238\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 400000\n",
            "  training_iteration: 100\n",
            "  trial_id: ac7e4_00000\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1909)\u001b[0m 2021-04-25 12:20:38,867\tDEBUG trainer.py:594 -- synchronized filters: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=400159, mean_mean=16.170019054764754, mean_std=9.111639818812266), (n=0, mean_mean=0.0, mean_std=0.0))}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 4.2/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>RUNNING </td><td>172.28.0.2:1909</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         2121.95</td><td style=\"text-align: right;\">400000</td><td style=\"text-align: right;\">-0.477791</td><td style=\"text-align: right;\">           -0.454167</td><td style=\"text-align: right;\">           -0.514437</td><td style=\"text-align: right;\">           3668.19</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.9/25.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/1 GPUs, 0.0/15.02 GiB heap, 0.0/7.51 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_TradingEnv_ac7e4_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         2121.95</td><td style=\"text-align: right;\">400000</td><td style=\"text-align: right;\">-0.477791</td><td style=\"text-align: right;\">           -0.454167</td><td style=\"text-align: right;\">           -0.514437</td><td style=\"text-align: right;\">           3668.19</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2021-04-25 12:20:39,080\tINFO tune.py:549 -- Total run time: 2135.45 seconds (2135.28 seconds for the tuning loop).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qERd6hdGl5aP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8033f34c-2e66-46db-dd4d-9f650d39e8f2"
      },
      "source": [
        "!cat /root/ray_results/PPO/PPO_TradingEnv_04e4f_00000_0_2021-04-25_11-18-54/error.txt\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Failure # 1 (occurred at 2021-04-25_11-19-02)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n",
            "    results = self.trial_executor.fetch_result(trial)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n",
            "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/worker.py\", line 1481, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::PPO.train_buffered()\u001b[39m (pid=1059, ip=172.28.0.2)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/trainable.py\", line 232, in train\n",
            "    result = self.step()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/rllib/agents/trainer_template.py\", line 162, in step\n",
            "    res = next(self.train_exec_impl)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/util/iter.py\", line 756, in __next__\n",
            "    return next(self.built_iterator)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
            "    for item in it:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
            "    for item in it:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/util/iter.py\", line 843, in apply_filter\n",
            "    for item in it:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/util/iter.py\", line 843, in apply_filter\n",
            "    for item in it:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
            "    for item in it:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
            "    for item in it:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
            "    for item in it:\n",
            "  [Previous line repeated 1 more time]\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/util/iter.py\", line 876, in apply_flatten\n",
            "    for item in it:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/util/iter.py\", line 828, in add_wait_hooks\n",
            "    item = next(it)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
            "    for item in it:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
            "    for item in it:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
            "    for item in it:\n",
            "  [Previous line repeated 1 more time]\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/util/iter.py\", line 471, in base_iterator\n",
            "    yield ray.get(futures, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::RolloutWorker.par_iter_next()\u001b[39m (pid=1058, ip=172.28.0.2)\n",
            "  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
            "  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
            "    return method(__ray_actor, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/util/iter.py\", line 1152, in par_iter_next\n",
            "    return next(self.local_it)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 332, in gen_rollouts\n",
            "    yield self.sample()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 706, in sample\n",
            "    batches = [self.input_reader.next()]\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/rllib/evaluation/sampler.py\", line 96, in next\n",
            "    batches = [self.get_data()]\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/rllib/evaluation/sampler.py\", line 223, in get_data\n",
            "    item = next(self.rollout_provider)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/rllib/evaluation/sampler.py\", line 648, in _env_runner\n",
            "    base_env.send_actions(actions_to_send)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/rllib/env/base_env.py\", line 363, in send_actions\n",
            "    self.vector_env.vector_step(action_vector)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/rllib/env/vector_env.py\", line 167, in vector_step\n",
            "    obs, r, done, info = self.envs[i].step(actions[i])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensortrade/env/generic/environment.py\", line 124, in step\n",
            "    reward = self.reward_scheme.reward(self)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensortrade/env/default/rewards.py\", line 16, in reward\n",
            "    return self.get_reward(env.action_scheme.portfolio)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensortrade/env/default/rewards.py\", line 70, in get_reward\n",
            "    returns = np.array([x + 1 for x in returns[-self._window_size:]]).cumprod() -1\n",
            "TypeError: slice indices must be integers or None or have an __index__ method\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "\u001b[36mray::PPO.train_buffered()\u001b[39m (pid=1059, ip=172.28.0.2)\n",
            "  File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
            "  File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
            "  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
            "  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
            "    return method(__ray_actor, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/trainable.py\", line 173, in train_buffered\n",
            "    result = self.train()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/rllib/agents/trainer.py\", line 567, in train\n",
            "    self._try_recover()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/rllib/agents/trainer.py\", line 1264, in _try_recover\n",
            "    \"Not enough healthy workers remain to continue.\")\n",
            "RuntimeError: Not enough healthy workers remain to continue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59tMdFGiMalg"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sS3r2nXkkvXO"
      },
      "source": [
        "def create_eval_env(config):\n",
        "    p = Stream.source(list(data[\"close\"]), dtype=\"float\").rename(\"USD-BTC\")\n",
        "\n",
        "    # Define exchange\n",
        "    bitfinex = Exchange(\"bitfinex\", service=execute_order)(\n",
        "        p\n",
        "    )\n",
        "    portfolio = getPortfolio(bitfinex, 50000, 1)\n",
        "\n",
        "\n",
        "    feed = CPLogRSIMacdFeed()\n",
        "\n",
        "    for i in range(5):\n",
        "        print(feed.next())\n",
        "\n",
        "    reward_scheme = PBR(price=p)\n",
        "\n",
        "    action_scheme = BuySellHold(\n",
        "        cash=portfolio.wallets[0],\n",
        "        asset=portfolio.wallets[1]\n",
        "    ).attach(reward_scheme)\n",
        "\n",
        "    renderer_feed = DataFeed([\n",
        "        Stream.source(list(data[\"date\"])).rename(\"date\"),\n",
        "        Stream.source(list(data[\"open\"]), dtype=\"float\").rename(\"open\"),\n",
        "        Stream.source(list(data[\"high\"]), dtype=\"float\").rename(\"high\"),\n",
        "        Stream.source(list(data[\"low\"]), dtype=\"float\").rename(\"low\"),\n",
        "        Stream.source(list(data[\"close\"]), dtype=\"float\").rename(\"close\"),\n",
        "        Stream.source(list(data[\"volume\"]), dtype=\"float\").rename(\"volume\")\n",
        "    ])\n",
        "\n",
        "    environment = default.create(\n",
        "        feed=feed,\n",
        "        portfolio=portfolio,\n",
        "        action_scheme=BSH(\n",
        "            cash=portfolio.wallets[0],\n",
        "            asset=portfolio.wallets[1]\n",
        "        ),\n",
        "        reward_scheme=SimpleProfit(),\n",
        "        renderer_feed=renderer_feed,\n",
        "        # renderer=default.renderers.PlotlyTradingChart(),\n",
        "        renderer = PositionChangeChart(),\n",
        "        window_size=config[\"window_size\"],\n",
        "        max_allowed_loss=0.6\n",
        "    )\n",
        "    return environment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTmBmZL4MZRO",
        "outputId": "68c10fba-31b3-47cf-90ef-c5f3e6bec280"
      },
      "source": [
        "import ray.rllib.agents.ppo as ppo\n",
        "\n",
        "# Get checkpoint\n",
        "checkpoints = analysis.get_trial_checkpoints_paths(\n",
        "    trial=analysis.get_best_trial(\"episode_reward_mean\"),\n",
        "    metric=\"episode_reward_mean\"\n",
        ")\n",
        "checkpoint_path = checkpoints[0][0]\n",
        "\n",
        "# Restore agent\n",
        "agent = ppo.PPOTrainer(\n",
        "    env=\"TradingEnv\",\n",
        "    config={\n",
        "        \"env_config\": {\n",
        "            \"window_size\": 25\n",
        "        },\n",
        "        \"framework\": \"torch\",\n",
        "        \"log_level\": \"DEBUG\",\n",
        "        \"ignore_worker_failures\": True,\n",
        "        \"num_workers\": 1,\n",
        "        \"num_gpus\": num_gpus,\n",
        "        \"clip_rewards\": True,\n",
        "        \"lr\": 8e-6,\n",
        "        \"lr_schedule\": [\n",
        "            [0, 1e-1],\n",
        "            [int(1e2), 1e-2],\n",
        "            [int(1e3), 1e-3],\n",
        "            [int(1e4), 1e-4],\n",
        "            [int(1e5), 1e-5],\n",
        "            [int(1e6), 1e-6],\n",
        "            [int(1e7), 1e-7]\n",
        "        ],\n",
        "        \"gamma\": 0,\n",
        "        \"observation_filter\": \"MeanStdFilter\",\n",
        "        \"lambda\": 0.72,\n",
        "        \"vf_loss_coeff\": 0.5,\n",
        "        \"entropy_coeff\": 0.01\n",
        "    }\n",
        ")\n",
        "\n",
        "agent.restore(checkpoint_path)\n",
        "\n",
        "# Instantiate the environment\n",
        "env = create_eval_env({\n",
        "    \"window_size\": 25,\n",
        "})\n",
        "\n",
        "\n",
        "# Run until episode ends\n",
        "episode_reward = 0\n",
        "done = False\n",
        "obs = env.reset()\n",
        "i = 0\n",
        "\n",
        "rewards = []\n",
        "actions = []\n",
        "while not done:\n",
        "    action = agent.compute_action(obs)\n",
        "    actions.append(action)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    rewards.append(reward)\n",
        "    episode_reward += reward\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
            "\u001b[2m\u001b[36m(pid=2385)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=2385)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=2385)\u001b[0m non-resource variables are not supported in the long term\n",
            "\u001b[2m\u001b[36m(pid=2385)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning: divide by zero encountered in true_divide\n",
            "\u001b[2m\u001b[36m(pid=2385)\u001b[0m   return self.op(self.inputs[0].value, self.inputs[1].value)\n",
            "\u001b[2m\u001b[36m(pid=2385)\u001b[0m 2021-04-25 12:20:44,280\tDEBUG rollout_worker.py:1122 -- Creating policy for default_policy\n",
            "\u001b[2m\u001b[36m(pid=2385)\u001b[0m 2021-04-25 12:20:44,281\tDEBUG catalog.py:632 -- Created preprocessor <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f8646d0c590>: Box(-inf, inf, (25, 3), float32) -> (25, 3)\n",
            "2021-04-25 12:20:44,336\tDEBUG rollout_worker.py:1122 -- Creating policy for default_policy\n",
            "2021-04-25 12:20:44,338\tDEBUG catalog.py:632 -- Created preprocessor <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f964b5d6610>: Box(-inf, inf, (25, 3), float32) -> (25, 3)\n",
            "\u001b[2m\u001b[36m(pid=2385)\u001b[0m 2021-04-25 12:20:44,298\tINFO torch_policy.py:112 -- TorchPolicy running on CPU.\n",
            "\u001b[2m\u001b[36m(pid=2385)\u001b[0m 2021-04-25 12:20:44,330\tDEBUG rollout_worker.py:533 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
            "\u001b[2m\u001b[36m(pid=2385)\u001b[0m 2021-04-25 12:20:44,330\tDEBUG rollout_worker.py:680 -- Created rollout worker with env <ray.rllib.env.base_env._VectorEnvToBaseEnv object at 0x7f864919f890> (<TradingEnv instance>), policies {'default_policy': <ray.rllib.policy.policy_template.PPOTorchPolicy object at 0x7f864b3450d0>}\n",
            "2021-04-25 12:20:44,351\tINFO torch_policy.py:109 -- TorchPolicy running on GPU.\n",
            "2021-04-25 12:20:44,396\tINFO rollout_worker.py:1161 -- Built policy map: {'default_policy': <ray.rllib.policy.policy_template.PPOTorchPolicy object at 0x7f964b82d3d0>}\n",
            "2021-04-25 12:20:44,397\tINFO rollout_worker.py:1162 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f964b5d6610>}\n",
            "2021-04-25 12:20:44,398\tDEBUG rollout_worker.py:533 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
            "2021-04-25 12:20:44,400\tINFO rollout_worker.py:563 -- Built filter map: {'default_policy': MeanStdFilter((25, 3), True, True, None, (n=0, mean_mean=0.0, mean_std=0.0), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
            "2021-04-25 12:20:44,403\tDEBUG rollout_worker.py:680 -- Created rollout worker with env None (None), policies {'default_policy': <ray.rllib.policy.policy_template.PPOTorchPolicy object at 0x7f964b82d3d0>}\n",
            "2021-04-25 12:20:44,412\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
            "2021-04-25 12:20:44,433\tINFO trainable.py:378 -- Restored on 172.28.0.2 from checkpoint: /root/ray_results/PPO/PPO_TradingEnv_ac7e4_00000_0_2021-04-25_11-45-03/checkpoint_000100/checkpoint-100\n",
            "2021-04-25 12:20:44,435\tINFO trainable.py:385 -- Current state after restoring: {'_iteration': 100, '_timesteps_total': None, '_time_total': 2121.9513263702393, '_episodes_total': 158}\n",
            "/usr/local/lib/python3.7/dist-packages/tensortrade/feed/core/operators.py:171: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2385)\u001b[0m {'lr': nan, 'rsi': nan, 'macd': 0.0}\n",
            "\u001b[2m\u001b[36m(pid=2385)\u001b[0m {'lr': 0.00045761355334761333, 'rsi': 100.0, 'macd': 0.3802733214494462}\n",
            "\u001b[2m\u001b[36m(pid=2385)\u001b[0m {'lr': -0.0022099203461092287, 'rsi': 16.45021645021696, 'macd': -1.2850831900864756}\n",
            "\u001b[2m\u001b[36m(pid=2385)\u001b[0m {'lr': -0.001789832853125617, 'rsi': 9.61512851245181, 'macd': -3.5410157838872425}\n",
            "\u001b[2m\u001b[36m(pid=2385)\u001b[0m {'lr': 0.00872379407148216, 'rsi': 71.2075575333076, 'macd': 2.810321575123835}\n",
            "{'lr': nan, 'rsi': nan, 'macd': 0.0}\n",
            "{'lr': 0.00045761355334761333, 'rsi': 100.0, 'macd': 0.3802733214494462}\n",
            "{'lr': -0.0022099203461092287, 'rsi': 16.45021645021696, 'macd': -1.2850831900864756}\n",
            "{'lr': -0.001789832853125617, 'rsi': 9.61512851245181, 'macd': -3.5410157838872425}\n",
            "{'lr': 0.00872379407148216, 'rsi': 71.2075575333076, 'macd': 2.810321575123835}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "id": "4eSSmLSNsw-e",
        "outputId": "125a7444-1609-46d9-dcca-547673996b4e"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "plt.plot(range(len(rewards)), np.array(rewards).cumsum())\n",
        "plt.show()\n",
        "\n",
        "# Net worth plot\n",
        "\n",
        "d = env.action_scheme.portfolio.performance\n",
        "\n",
        "# lists = sorted(d.items()) # sorted by key, return+ a list of tuples\n",
        "keys = d[0].keys()\n",
        "x = range(len(d))\n",
        "c = {}\n",
        "\n",
        "for i in range(len(d)):\n",
        "    for key in keys:\n",
        "        if key != 'net_worth' and key != 'base_symbol':\n",
        "            if not key in c:\n",
        "                c[key] = []\n",
        "            c[key].append(d[i][key])\n",
        "\n",
        "for key in keys:\n",
        "    if key != 'net_worth' and key != 'base_symbol':\n",
        "        plt.plot(x, c[key], label=key)\n",
        "\n",
        "plt.set_title(\"Net Worth\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-d0b4d70141c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Net Worth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib.pyplot' has no attribute 'set_title'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}